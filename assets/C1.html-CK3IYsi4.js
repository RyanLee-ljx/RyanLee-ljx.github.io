import{_ as n}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as l,o as i,c as r,a as s,b as a,d as m,e as t}from"./app-7EW-I0QU.js";const o={},c=s("h1",{id:"before-reading",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#before-reading"},[s("span",null,"Before reading......")])],-1),p=s("p",null,[a("This blog is mainly a notebook of "),s("em",null,"Mathematical Foundations of Reinforcement Learning"),a(" by Shiyu Zhao from Westlake University WindyLab.")],-1),h={href:"https://github.com/MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning",target:"_blank",rel:"noopener noreferrer"},g=t('<h2 id="chapter-1-basic-concepts-of-reinforcement-learning" tabindex="-1"><a class="header-anchor" href="#chapter-1-basic-concepts-of-reinforcement-learning"><span>Chapter 1 Basic Concepts of Reinforcement Learning</span></a></h2><p>Reinforcement Learning (RL) can be described by the grid world example.</p><p>We place one agent in an environment, the goal of the agent is to find a good route to the target. Every cell/grid the agent placed can be seen as a state. Agent can take one action at each state according to a certain policy. The goal of RL is to find a good policy to guide the agent taking a sequence of acitons, travelling from the start place, moving from one state to another, and finally reach the target.</p><figure><img src="https://github.com/RyanLee-ljx/RyanLee-ljx.github.io/blob/image/RL/C1/ba3a55ea95b2571161575dba273af8b.png?raw=true" alt="grid world" tabindex="0" loading="lazy"><figcaption>grid world</figcaption></figure><h3 id="markov-decision-process" tabindex="-1"><a class="header-anchor" href="#markov-decision-process"><span>Markov decision process</span></a></h3><p>Reinforcement learning utilizes the MDP framework to model the interaction between a learning agent and its environment. Actually the above example is just one simple description of the Markov decision preocess. It reflects how the agent interacts with the environment.</p><p>The key concepts involves:</p><h4 id="state" tabindex="-1"><a class="header-anchor" href="#state"><span>State</span></a></h4><p><em>State</em> is the status of the agent with respect to the environment. Its set is the <em>state space</em> $ S = {s_i} $.</p><h4 id="action" tabindex="-1"><a class="header-anchor" href="#action"><span>Action</span></a></h4><p><em>Action</em> is what the agent do at a certain state. The agent will obtain a new state after taking one aciton. Similarly, its set is the <em>action space of a state</em> denoted as $ A(s_i) = {a_i} $</p><h4 id="policy" tabindex="-1"><a class="header-anchor" href="#policy"><span>Policy</span></a></h4><p><em>Policy</em> denoted as $ \\pi $, tells the agent what actions to take at a state. It gives the probability of each action to be taken at a certain state. In mathematical form ,we use <em>tabular representation</em> to display one policy. In programming, we use one array, matrix to represent a policy</p><figure><img src="https://github.com/RyanLee-ljx/RyanLee-ljx.github.io/blob/image/RL/C1/97cd3c24fba9676822a512a14a8ea73.png?raw=true" alt="tabular representation" tabindex="0" loading="lazy"><figcaption>tabular representation</figcaption></figure><h4 id="reward" tabindex="-1"><a class="header-anchor" href="#reward"><span>Reward</span></a></h4><p><em>Reward</em> guides the agent to our target. Agent wants more reward, which means the agent will minimize (the reward is negative) or maximum (the reward is postive) the reward in the process.</p><p><strong>Reward depends on the current state and action not the next state</strong>.</p><h4 id="probability-distribution" tabindex="-1"><a class="header-anchor" href="#probability-distribution"><span>Probability Distribution</span></a></h4><p>Involve two probability form:</p>',19),u=s("ul",null,[s("li",null,[a("State transition probability: at state "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"s")]),s("annotation",{encoding:"application/x-tex"},"s")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.4306em"}}),s("span",{class:"mord mathnormal"},"s")])])]),a(", taking action "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"a")]),s("annotation",{encoding:"application/x-tex"},"a")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.4306em"}}),s("span",{class:"mord mathnormal"},"a")])])]),a(", the probability to transit to state "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msup",null,[s("mi",null,"s"),s("mo",{mathvariant:"normal",lspace:"0em",rspace:"0em"},"′")])]),s("annotation",{encoding:"application/x-tex"},"s'")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.7519em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"s"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.7519em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"′")])])])])])])])])])])]),a(" is "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"p"),s("mo",{stretchy:"false"},"("),s("msup",null,[s("mi",null,"s"),s("mo",{mathvariant:"normal",lspace:"0em",rspace:"0em"},"′")]),s("mi",{mathvariant:"normal"},"∣"),s("mi",null,"s"),s("mo",{separator:"true"},","),s("mi",null,"a"),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"},"p(s'|s,a)")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.0019em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal"},"p"),s("span",{class:"mopen"},"("),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"s"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.7519em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"′")])])])])])])])]),s("span",{class:"mord"},"∣"),s("span",{class:"mord mathnormal"},"s"),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord mathnormal"},"a"),s("span",{class:"mclose"},")")])])])]),s("li",null,[a("Reward probability: at state "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"s")]),s("annotation",{encoding:"application/x-tex"},"s")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.4306em"}}),s("span",{class:"mord mathnormal"},"s")])])]),a(", taking action "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"a")]),s("annotation",{encoding:"application/x-tex"},"a")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.4306em"}}),s("span",{class:"mord mathnormal"},"a")])])]),a(", the probability to get reward "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"r")]),s("annotation",{encoding:"application/x-tex"},"r")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.4306em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"r")])])]),a(" is "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"p"),s("mo",{stretchy:"false"},"("),s("mi",null,"r"),s("mi",{mathvariant:"normal"},"∣"),s("mi",null,"s"),s("mo",{separator:"true"},","),s("mi",null,"a"),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"},"p(r|s, a)")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal"},"p"),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"r"),s("span",{class:"mord"},"∣"),s("span",{class:"mord mathnormal"},"s"),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord mathnormal"},"a"),s("span",{class:"mclose"},")")])])])])],-1),d=t('<h4 id="markov-property" tabindex="-1"><a class="header-anchor" href="#markov-property"><span>Markov Property</span></a></h4><p>Memoryless property: The state transiting to the next depends on current state and action rather than previous.</p><h3 id="other-concepts" tabindex="-1"><a class="header-anchor" href="#other-concepts"><span>Other concepts</span></a></h3><h4 id="trajectory-and-return" tabindex="-1"><a class="header-anchor" href="#trajectory-and-return"><span>Trajectory and Return</span></a></h4><p>A <em>trajectory</em> is a state-action-reward chain:</p><figure><img src="https://github.com/RyanLee-ljx/RyanLee-ljx.github.io/blob/image/RL/C1/77fc0d671ff46edc0f0f6c95381e7e0.png?raw=true" alt="chain" tabindex="0" loading="lazy"><figcaption>chain</figcaption></figure>',6),y=s("p",null,[a("The return of this trajectory is the sum of all the rewards collected along the trajectory. It can be finite, e.g., transit from target to target "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"s"),s("mn",null,"1")]),s("mo",null,"→"),s("msub",null,[s("mi",null,"s"),s("mn",null,"2")]),s("mo",null,"→"),s("msub",null,[s("mi",null,"s"),s("mn",null,"3")]),s("mo",null,"→"),s("msub",null,[s("mi",null,"s"),s("mn",null,"5")]),s("mo",null,"→"),s("msub",null,[s("mi",null,"s"),s("mn",null,"5")]),s("mo",null,"→"),s("msub",null,[s("mi",null,"s"),s("mn",null,"5")]),s("mi",{mathvariant:"normal"},"."),s("mi",{mathvariant:"normal"},"."),s("mi",{mathvariant:"normal"},".")]),s("annotation",{encoding:"application/x-tex"},"s_1 \\to s_2 \\to s_3 \\to s_5 \\to s_5 \\to s_5...")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.5806em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"s"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"1")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"→"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.5806em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"s"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"2")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"→"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.5806em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"s"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"3")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"→"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.5806em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"s"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"5")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"→"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.5806em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"s"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"5")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"→"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.5806em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"s"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"5")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mord"},"...")])])]),a(".")],-1),b=s("p",{class:"katex-block"},[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("semantics",null,[s("mrow",null,[s("mi",null,"r"),s("mi",null,"e"),s("mi",null,"t"),s("mi",null,"u"),s("mi",null,"r"),s("mi",null,"n"),s("mo",null,"="),s("mn",null,"0"),s("mo",null,"+"),s("mn",null,"0"),s("mo",null,"+"),s("mn",null,"0"),s("mo",null,"+"),s("mn",null,"1"),s("mo",null,"="),s("mn",null,"1")]),s("annotation",{encoding:"application/x-tex"}," return = 0 + 0 + 0 + 1 = 1 ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6151em"}}),s("span",{class:"mord mathnormal"},"re"),s("span",{class:"mord mathnormal"},"t"),s("span",{class:"mord mathnormal"},"u"),s("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"r"),s("span",{class:"mord mathnormal"},"n"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.7278em","vertical-align":"-0.0833em"}}),s("span",{class:"mord"},"0"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"+"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.7278em","vertical-align":"-0.0833em"}}),s("span",{class:"mord"},"0"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"+"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.7278em","vertical-align":"-0.0833em"}}),s("span",{class:"mord"},"0"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"+"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6444em"}}),s("span",{class:"mord"},"1"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6444em"}}),s("span",{class:"mord"},"1")])])])])],-1),f=s("h4",{id:"discounted-rate",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#discounted-rate"},[s("span",null,"Discounted Rate")])],-1),v=s("p",null,[s("em",null,"discount rate"),a(),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"γ"),s("mo",null,"∈"),s("mo",{stretchy:"false"},"["),s("mn",null,"0"),s("mo",{separator:"true"},","),s("mn",null,"1"),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"},"γ \\in [0, 1)")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.7335em","vertical-align":"-0.1944em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.05556em"}},"γ"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"∈"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mopen"},"["),s("span",{class:"mord"},"0"),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},"1"),s("span",{class:"mclose"},")")])])])],-1),x=s("p",null,"Roles: 1) the sum of the return becomes finite instead of infinite; 2) balance the far and near future rewards:",-1),w=s("ul",null,[s("li",null,[s("p",null,[a("If "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"γ")]),s("annotation",{encoding:"application/x-tex"},"γ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.625em","vertical-align":"-0.1944em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.05556em"}},"γ")])])]),a(" is close to 0, the value of the discounted return is dominated by the rewards obtained in the near future.")])]),s("li",null,[s("p",null,[a("If "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"γ")]),s("annotation",{encoding:"application/x-tex"},"γ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.625em","vertical-align":"-0.1944em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.05556em"}},"γ")])])]),a(" is close to 1, the value of the discounted return is dominated by the rewards obtained in the far future")])])],-1),k=s("h4",{id:"episode",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#episode"},[s("span",null,"Episode")])],-1),_=s("p",null,[a("When interacting with the environment following a policy, the agent may stop at some terminal states. The resulting trajectory is called an "),s("em",null,"episode"),a(" (or a "),s("em",null,"trial"),a("), e.g. "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"s"),s("mn",null,"1")]),s("mo",null,"→"),s("msub",null,[s("mi",null,"s"),s("mn",null,"2")]),s("mo",null,"→"),s("msub",null,[s("mi",null,"s"),s("mn",null,"3")]),s("mo",null,"→"),s("msub",null,[s("mi",null,"s"),s("mn",null,"5")])]),s("annotation",{encoding:"application/x-tex"},"s_1 \\to s_2 \\to s_3 \\to s_5")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.5806em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"s"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"1")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"→"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.5806em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"s"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"2")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"→"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.5806em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"s"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"3")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"→"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.5806em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"s"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"5")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])]),a(",")],-1),L=s("p",null,"An episode is usually assumed to be a finite trajectory. Tasks with episodes are called episodic tasks.",-1),M=s("p",null,"We can treat episodic and continuing tasks in a unified mathematical way by converting episodic tasks to continuing tasks:",-1),R=s("ul",null,[s("li",null,[s("p",null,"Option 1: Treat the target state as a special absorbing state. Once the agent reaches an absorbing state, it will never leave. The consequent rewards r = 0.")]),s("li",null,[s("p",null,"Option 2: Treat the target state as a normal state with a policy. The agent can still leave the target state and gain r = +1 when entering the target state.")])],-1),z=s("p",null,"This tutorial course considers option 2 so as to not distinguish the target state from the others and can treat it as a normal state.",-1);function T(j,C){const e=l("ExternalLinkIcon");return i(),r("div",null,[c,p,s("p",null,[a("You can find more about the book and related tutorial video at this "),s("a",h,[a("link"),m(e)]),a(".")]),g,u,d,y,b,f,v,x,w,k,_,L,M,R,z])}const I=n(o,[["render",T],["__file","C1.html.vue"]]),S=JSON.parse(`{"path":"/RL/C1.html","title":"Before reading......","lang":"zh-CN","frontmatter":{"icon":"RL","ReadingTime":true,"date":"2025-07-04T00:00:00.000Z","Word":true,"PageView":true,"category":"RL","description":"Before reading...... This blog is mainly a notebook of Mathematical Foundations of Reinforcement Learning by Shiyu Zhao from Westlake University WindyLab. You can find more abou...","head":[["meta",{"property":"og:url","content":"https://ryanlee-ljx.github.io/RL/C1.html"}],["meta",{"property":"og:site_name","content":"RyanLee's blog"}],["meta",{"property":"og:title","content":"Before reading......"}],["meta",{"property":"og:description","content":"Before reading...... This blog is mainly a notebook of Mathematical Foundations of Reinforcement Learning by Shiyu Zhao from Westlake University WindyLab. You can find more abou..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://github.com/RyanLee-ljx/RyanLee-ljx.github.io/blob/image/RL/C1/ba3a55ea95b2571161575dba273af8b.png?raw=true"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-07-04T08:46:06.000Z"}],["meta",{"property":"article:author","content":"RyanLee_ljx"}],["meta",{"property":"article:published_time","content":"2025-07-04T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-07-04T08:46:06.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Before reading......\\",\\"image\\":[\\"https://github.com/RyanLee-ljx/RyanLee-ljx.github.io/blob/image/RL/C1/ba3a55ea95b2571161575dba273af8b.png?raw=true\\",\\"https://github.com/RyanLee-ljx/RyanLee-ljx.github.io/blob/image/RL/C1/97cd3c24fba9676822a512a14a8ea73.png?raw=true\\",\\"https://github.com/RyanLee-ljx/RyanLee-ljx.github.io/blob/image/RL/C1/77fc0d671ff46edc0f0f6c95381e7e0.png?raw=true\\"],\\"datePublished\\":\\"2025-07-04T00:00:00.000Z\\",\\"dateModified\\":\\"2025-07-04T08:46:06.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"RyanLee_ljx\\",\\"email\\":\\"2284771024@qq.com\\"}]}"]]},"headers":[{"level":2,"title":"Chapter 1 Basic Concepts of Reinforcement Learning","slug":"chapter-1-basic-concepts-of-reinforcement-learning","link":"#chapter-1-basic-concepts-of-reinforcement-learning","children":[{"level":3,"title":"Markov decision process","slug":"markov-decision-process","link":"#markov-decision-process","children":[]},{"level":3,"title":"Other concepts","slug":"other-concepts","link":"#other-concepts","children":[]}]}],"git":{"createdTime":1751618766000,"updatedTime":1751618766000,"contributors":[{"name":"Flame","email":"2284771024@qq.com","commits":1}]},"readingTime":{"minutes":2.42,"words":725},"filePathRelative":"RL/C1.md","localizedDate":"2025年7月4日","excerpt":"\\n<p>This blog is mainly a notebook of <em>Mathematical Foundations of Reinforcement Learning</em> by Shiyu Zhao from Westlake University WindyLab.</p>\\n<p>You can find more about the book and related tutorial video at this <a href=\\"https://github.com/MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">link</a>.</p>","autoDesc":true}`);export{I as comp,S as data};
