import{_ as o}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r,o as l,c as s,a as e,b as t,d as a,e as i}from"./app-CVoSmys6.js";const c={},m=i('<h1 id="attention-mechanism" tabindex="-1"><a class="header-anchor" href="#attention-mechanism"><span>Attention Mechanism</span></a></h1><p>This article will introduce a powerful technique in machine learning called <em>Ateention Mechanism</em>.</p><p>The core method of <em>attention mechanism</em> is to pay more attention to what we want. It allows model to weigh the importance of different parts of input dynamically rather than treating them equally. The model learns to assign higher weights to the most relevant elements.</p><p>Before stepping into the main text, we should first know some preliminary knowledge.</p><h2 id="preliminaries" tabindex="-1"><a class="header-anchor" href="#preliminaries"><span>preliminaries</span></a></h2><ol><li>RNNs and LSTM network</li></ol><p>The content introduced below is mainly from this three blog/article.</p>',7),h={href:"https://en.wikipedia.org/wiki/Recurrent_neural_network",target:"_blank",rel:"noopener noreferrer"},p={href:"https://colah.github.io/posts/2015-08-Understanding-LSTMs/",target:"_blank",rel:"noopener noreferrer"},d={href:"https://blog.csdn.net/v_JULY_v/article/details/89894058?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169296729516800211518875%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169296729516800211518875&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-89894058-null-null.142%5Ev93%5EchatgptT3_2&utm_term=RNN&spm=1018.2226.3001.4187",target:"_blank",rel:"noopener noreferrer"},u=i('<p>RNNs, short for Recurrent Neural Networks, is a class of artificial nerural network used to process sequencial data. Unlike FFN(Forwardfeed Neural Network), RNNs process data across multiple times rather than in a single time, making them well-adapted for modelling and processing text, speech, and time series.</p><p>The following picture demonstrates the working flow of RNNs.</p><figure><img src="" alt="An unrolled recurrent neural network from [2]" tabindex="0" loading="lazy"><figcaption>An unrolled recurrent neural network from [2]</figcaption></figure><ol start="2"><li>Encoder and Decoder</li></ol><h2 id="basic-components" tabindex="-1"><a class="header-anchor" href="#basic-components"><span>Basic Components</span></a></h2><h2 id="working-flow" tabindex="-1"><a class="header-anchor" href="#working-flow"><span>Working Flow</span></a></h2>',6);function g(f,w){const n=r("ExternalLinkIcon");return l(),s("div",null,[m,e("p",null,[t("[1] "),e("a",h,[t("Recurrent neural network From Wikipedia"),a(n)])]),e("p",null,[t("[2] "),e("a",p,[t("Understanding LSTM Networks"),a(n)])]),e("p",null,[t("[3] "),e("a",d,[t("如何从RNN起步，一步一步通俗理解LSTM"),a(n)])]),u])}const y=o(c,[["render",g],["__file","attention.html.vue"]]),N=JSON.parse(`{"path":"/ML/attention.html","title":"Attention Mechanism","lang":"zh-CN","frontmatter":{"icon":"fangchahexiefangchafenxi-xuanzhong","ReadingTime":true,"Word":true,"PageView":true,"category":"ML","description":"Attention Mechanism This article will introduce a powerful technique in machine learning called Ateention Mechanism. The core method of attention mechanism is to pay more attent...","head":[["meta",{"property":"og:url","content":"https://ryanlee-ljx.github.io/ML/attention.html"}],["meta",{"property":"og:site_name","content":"RyanLee's blog"}],["meta",{"property":"og:title","content":"Attention Mechanism"}],["meta",{"property":"og:description","content":"Attention Mechanism This article will introduce a powerful technique in machine learning called Ateention Mechanism. The core method of attention mechanism is to pay more attent..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-01-30T14:31:27.000Z"}],["meta",{"property":"article:author","content":"RyanLee_ljx"}],["meta",{"property":"article:modified_time","content":"2025-01-30T14:31:27.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Attention Mechanism\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-01-30T14:31:27.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"RyanLee_ljx\\",\\"email\\":\\"2284771024@qq.com\\"}]}"]]},"headers":[{"level":2,"title":"preliminaries","slug":"preliminaries","link":"#preliminaries","children":[]},{"level":2,"title":"Basic Components","slug":"basic-components","link":"#basic-components","children":[]},{"level":2,"title":"Working Flow","slug":"working-flow","link":"#working-flow","children":[]}],"git":{"createdTime":1738247487000,"updatedTime":1738247487000,"contributors":[{"name":"Flame","email":"2284771024@qq.com","commits":1}]},"readingTime":{"minutes":0.83,"words":249},"filePathRelative":"ML/attention.md","localizedDate":"2025年1月30日","excerpt":"\\n<p>This article will introduce a powerful technique in machine learning called <em>Ateention Mechanism</em>.</p>\\n<p>The core method of <em>attention mechanism</em> is to pay more attention to what we want. It allows model to weigh the importance of different parts of input dynamically rather than treating them equally. The model learns to assign higher weights to the most relevant elements.</p>","autoDesc":true}`);export{y as comp,N as data};
