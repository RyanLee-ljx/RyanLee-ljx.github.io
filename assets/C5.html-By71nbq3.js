import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{o as l,c as n,a as s,b as a,e as t}from"./app-CakD6YAz.js";const i={},m=s("h1",{id:"chapter-5-monte-carlo-learning",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#chapter-5-monte-carlo-learning"},[s("span",null,"Chapter 5 Monte Carlo Learning")])],-1),r=s("p",null,"This chapter we will introduce a model-free approach for deriving optimal policy.",-1),p=s("p",null,[a("Here, model-free refers that we do not rely on a specific mathematical model to obtain state value or action value. Like, in the policy evaluation, we use "),s("strong",null,"BOE"),a(" to obtain state value, which is just "),s("em",null,"model-based"),a(". For model-free, we do not use that equation anymore. Instead, we leverage the "),s("em",null,"mean estimation"),a(" methods.")],-1),o=s("p",null,"Probably the following example can better illustrate",-1),c=s("div",{class:"hint-container tip"},[s("p",{class:"hint-container-title"},"Example Flip a coin"),s("p",null,[a("The result (either head or tail) is denoted as a random variable "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"X")]),s("annotation",{encoding:"application/x-tex"},"X")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6833em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.07847em"}},"X")])])]),a(".")]),s("ul",null,[s("li",null,[s("p",null,[a("if the result is head, then "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"X"),s("mo",null,"="),s("mo",null,"+"),s("mn",null,"1")]),s("annotation",{encoding:"application/x-tex"},"X = +1")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6833em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.07847em"}},"X"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.7278em","vertical-align":"-0.0833em"}}),s("span",{class:"mord"},"+"),s("span",{class:"mord"},"1")])])]),a(".")])]),s("li",null,[s("p",null,[a("if the result is tail, then "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"X"),s("mo",null,"="),s("mo",null,"−"),s("mn",null,"1")]),s("annotation",{encoding:"application/x-tex"},"X = −1")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6833em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.07847em"}},"X"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.7278em","vertical-align":"-0.0833em"}}),s("span",{class:"mord"},"−"),s("span",{class:"mord"},"1")])])]),a(".")])])]),s("p",null,[a("The aim is to compute "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",{mathvariant:"double-struck"},"E"),s("mo",{stretchy:"false"},"["),s("mi",null,"X"),s("mo",{stretchy:"false"},"]")]),s("annotation",{encoding:"application/x-tex"},"\\mathbb E[X]")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathbb"},"E"),s("span",{class:"mopen"},"["),s("span",{class:"mord mathnormal",style:{"margin-right":"0.07847em"}},"X"),s("span",{class:"mclose"},"]")])])]),a(".")]),s("p",null,"The model-based approach is to calculate the expectation through the definition."),s("p",{class:"katex-block"},[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("semantics",null,[s("mrow",null,[s("mi",{mathvariant:"double-struck"},"E"),s("mo",{stretchy:"false"},"["),s("mi",null,"X"),s("mo",{stretchy:"false"},"]"),s("mo",null,"="),s("munder",null,[s("mo",null,"∑"),s("mi",null,"x")]),s("mi",null,"p"),s("mo",{stretchy:"false"},"("),s("mi",null,"x"),s("mo",{stretchy:"false"},")"),s("mi",null,"x"),s("mo",null,"="),s("mn",null,"0.5"),s("mo",null,"×"),s("mn",null,"1"),s("mo",null,"+"),s("mn",null,"0.5"),s("mo",null,"×"),s("mo",{stretchy:"false"},"("),s("mo",null,"−"),s("mn",null,"1"),s("mo",{stretchy:"false"},")"),s("mo",null,"="),s("mn",null,"0.5")]),s("annotation",{encoding:"application/x-tex"}," \\mathbb E[X] = \\sum_{x} p(x)x = 0.5 \\times 1 + 0.5 \\times (-1) = 0.5 ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathbb"},"E"),s("span",{class:"mopen"},"["),s("span",{class:"mord mathnormal",style:{"margin-right":"0.07847em"}},"X"),s("span",{class:"mclose"},"]"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"2.3em","vertical-align":"-1.25em"}}),s("span",{class:"mop op-limits"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.05em"}},[s("span",{style:{top:"-1.9em","margin-left":"0em"}},[s("span",{class:"pstrut",style:{height:"3.05em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"x")])])]),s("span",{style:{top:"-3.05em"}},[s("span",{class:"pstrut",style:{height:"3.05em"}}),s("span",null,[s("span",{class:"mop op-symbol large-op"},"∑")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.25em"}},[s("span")])])])]),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord mathnormal"},"p"),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal"},"x"),s("span",{class:"mclose"},")"),s("span",{class:"mord mathnormal"},"x"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.7278em","vertical-align":"-0.0833em"}}),s("span",{class:"mord"},"0.5"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"×"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.7278em","vertical-align":"-0.0833em"}}),s("span",{class:"mord"},"1"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"+"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.7278em","vertical-align":"-0.0833em"}}),s("span",{class:"mord"},"0.5"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"×"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mopen"},"("),s("span",{class:"mord"},"−"),s("span",{class:"mord"},"1"),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6444em"}}),s("span",{class:"mord"},"0.5")])])])])]),s("p",null,[s("strong",null,"Problem: it may be impossible to know the precise distribution!!")]),s("p",null,"The model-free approach is based on sampling."),s("p",null,"We flip the coin many times, and then calculate the average of the outcomes."),s("p",null,[a("Suppose we get a sample sequence: "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"x"),s("mn",null,"1")]),s("mo",{separator:"true"},","),s("msub",null,[s("mi",null,"x"),s("mn",null,"2")]),s("mo",{separator:"true"},","),s("mi",{mathvariant:"normal"},"."),s("mi",{mathvariant:"normal"},"."),s("mi",{mathvariant:"normal"},"."),s("mo",{separator:"true"},","),s("msub",null,[s("mi",null,"x"),s("mi",null,"N")])]),s("annotation",{encoding:"application/x-tex"},"{x_1, x_2, . . . , x_N}")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.625em","vertical-align":"-0.1944em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"x"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"1")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"x"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"2")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},"..."),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"x"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3283em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.10903em"}},"N")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])])]),a(". Then, the mean can be approximated as:")]),s("p",{class:"katex-block"},[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("semantics",null,[s("mrow",null,[s("mrow",null,[s("mi",{mathvariant:"double-struck"},"E"),s("mo",{stretchy:"false"},"["),s("mi",{mathvariant:"double-struck"},"X"),s("mo",{stretchy:"false"},"]")]),s("mo",null,"≃"),s("mover",{accent:"true"},[s("mi",null,"x"),s("mo",null,"ˉ")]),s("mo",null,"="),s("munderover",null,[s("mo",null,"∑"),s("mrow",null,[s("mi",null,"j"),s("mo",null,"="),s("mn",null,"1")]),s("mi",null,"N")]),s("mstyle",{displaystyle:"false",scriptlevel:"0"},[s("mfrac",null,[s("msub",null,[s("mi",null,"x"),s("mi",null,"j")]),s("mi",null,"N")])])]),s("annotation",{encoding:"application/x-tex"}," \\mathbb{E[X]} \\simeq \\bar{x} = \\sum_{j=1}^{N} \\tfrac{x_j}{N} ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathbb"},"E"),s("span",{class:"mopen"},"["),s("span",{class:"mord mathbb"},"X"),s("span",{class:"mclose"},"]")]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"≃"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.5678em"}}),s("span",{class:"mord accent"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.5678em"}},[s("span",{style:{top:"-3em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord mathnormal"},"x")]),s("span",{style:{top:"-3em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"accent-body",style:{left:"-0.2222em"}},[s("span",{class:"mord"},"ˉ")])])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"3.2421em","vertical-align":"-1.4138em"}}),s("span",{class:"mop op-limits"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.8283em"}},[s("span",{style:{top:"-1.8723em","margin-left":"0em"}},[s("span",{class:"pstrut",style:{height:"3.05em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.05724em"}},"j"),s("span",{class:"mrel mtight"},"="),s("span",{class:"mord mtight"},"1")])])]),s("span",{style:{top:"-3.05em"}},[s("span",{class:"pstrut",style:{height:"3.05em"}}),s("span",null,[s("span",{class:"mop op-symbol large-op"},"∑")])]),s("span",{style:{top:"-4.3em","margin-left":"0em"}},[s("span",{class:"pstrut",style:{height:"3.05em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.10903em"}},"N")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.4138em"}},[s("span")])])])]),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},[s("span",{class:"mopen nulldelimiter"}),s("span",{class:"mfrac"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8087em"}},[s("span",{style:{top:"-2.655em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.10903em"}},"N")])])]),s("span",{style:{top:"-3.23em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"frac-line",style:{"border-bottom-width":"0.04em"}})]),s("span",{style:{top:"-3.5073em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"x"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3281em"}},[s("span",{style:{top:"-2.357em","margin-left":"0em","margin-right":"0.0714em"}},[s("span",{class:"pstrut",style:{height:"2.5em"}}),s("span",{class:"sizing reset-size3 size1 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.05724em"}},"j")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2819em"}},[s("span")])])])])])])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.345em"}},[s("span")])])])]),s("span",{class:"mclose nulldelimiter"})])])])])])]),s("p",null,[a("This is the idea of "),s("em",null,"Monte Carlo estimation"),a(" and it is supported by the "),s("em",null,"Law of Large Numbers"),a(" to be accurate.")])],-1),h=t('<h2 id="monte-carlo-mc-basic-algorithm" tabindex="-1"><a class="header-anchor" href="#monte-carlo-mc-basic-algorithm"><span>Monte Carlo (MC) Basic Algorithm</span></a></h2><p>The policy iteration involves two processes, namely <em>policy evaluation</em> and <em>policy improvement</em>.</p><p>As mentioned before, the differences between <em>model-based</em> and <em>model-free</em> is policy evaluation in which how to gain the action value is of paramount importance.</p><figure><img src="https://github.com/RyanLee-ljx/RyanLee-ljx.github.io/blob/image/RL/C5/1.png?raw=true" alt="model-based and model-free approach for calculating action value" tabindex="0" loading="lazy"><figcaption>model-based and model-free approach for calculating action value</figcaption></figure><p>Here we use the expression 2, namely estimate the expectation of every state-action pair as their real value.</p><p>Here is a thorough statement and the corresponding pseudocode.</p>',6),g=s("p",null,[a("Given an initial policy "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"π"),s("mn",null,"0")])]),s("annotation",{encoding:"application/x-tex"},"\\pi_{0}")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.5806em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"π"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0359em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"0")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])]),a(", there are two steps at the "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"k")]),s("annotation",{encoding:"application/x-tex"},"k")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6944em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.03148em"}},"k")])])]),a("th iteration.")],-1),u=s("ol",null,[s("li",null,[s("p",null,[s("strong",null,"Step 1 Policy Evaluation"),a(": This step is to obtain "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"q"),s("msub",null,[s("mi",null,"π"),s("mi",null,"k")])]),s("mo",{stretchy:"false"},"("),s("mi",null,"s"),s("mo",{separator:"true"},","),s("mi",null,"a"),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"},"q_{\\pi_{k}}(s, a)")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.0059em","vertical-align":"-0.2559em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"q"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1514em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0359em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"π"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3448em"}},[s("span",{style:{top:"-2.3488em","margin-left":"-0.0359em","margin-right":"0.0714em"}},[s("span",{class:"pstrut",style:{height:"2.5em"}}),s("span",{class:"sizing reset-size3 size1 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03148em"}},"k")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1512em"}},[s("span")])])])])])])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2559em"}},[s("span")])])])])]),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal"},"s"),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord mathnormal"},"a"),s("span",{class:"mclose"},")")])])]),a(" for all "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mo",{stretchy:"false"},"("),s("mi",null,"s"),s("mo",{separator:"true"},","),s("mi",null,"a"),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"},"(s, a)")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal"},"s"),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord mathnormal"},"a"),s("span",{class:"mclose"},")")])])]),a(". Specifically, for each action-state pair "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mo",{stretchy:"false"},"("),s("mi",null,"s"),s("mo",{separator:"true"},","),s("mi",null,"a"),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"},"(s, a)")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal"},"s"),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord mathnormal"},"a"),s("span",{class:"mclose"},")")])])]),a(", run an infinite number of (or sufficiently many) episodes. The average of their returns is used to approximate "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"q"),s("msub",null,[s("mi",null,"π"),s("mi",null,"k")])]),s("mo",{stretchy:"false"},"("),s("mi",null,"s"),s("mo",{separator:"true"},","),s("mi",null,"a"),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"},"q_{\\pi_{k}}(s, a)")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.0059em","vertical-align":"-0.2559em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"q"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1514em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0359em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"π"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3448em"}},[s("span",{style:{top:"-2.3488em","margin-left":"-0.0359em","margin-right":"0.0714em"}},[s("span",{class:"pstrut",style:{height:"2.5em"}}),s("span",{class:"sizing reset-size3 size1 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03148em"}},"k")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1512em"}},[s("span")])])])])])])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2559em"}},[s("span")])])])])]),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal"},"s"),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord mathnormal"},"a"),s("span",{class:"mclose"},")")])])]),a(".")])]),s("li",null,[s("p",null,[s("strong",null,"Step 2 Policy Improvement"),a(": Just like the policy improvement in the "),s("em",null,"policy iteration"),a(". Acquire the maximum action value in every state.")])])],-1),d=s("p",null,[a("Exactly the same as the policy iteration algorithm, except that we estimate $q_{\\pi_{k}}(s, a) $ directly, instead of solving "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"v"),s("msub",null,[s("mi",null,"π"),s("mi",null,"k")])]),s("mo",{stretchy:"false"},"("),s("mi",null,"s"),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"},"v_{\\pi_{k}}(s)")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.0059em","vertical-align":"-0.2559em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"v"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1514em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0359em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"π"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3448em"}},[s("span",{style:{top:"-2.3488em","margin-left":"-0.0359em","margin-right":"0.0714em"}},[s("span",{class:"pstrut",style:{height:"2.5em"}}),s("span",{class:"sizing reset-size3 size1 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03148em"}},"k")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1512em"}},[s("span")])])])])])])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2559em"}},[s("span")])])])])]),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal"},"s"),s("span",{class:"mclose"},")")])])]),a(".")],-1),y=t('<figure><img src="https://github.com/RyanLee-ljx/RyanLee-ljx.github.io/blob/image/RL/C5/2.png?raw=true" alt="MC Basic (a model-free variant of policy iteration)" tabindex="0" loading="lazy"><figcaption>MC Basic (a model-free variant of policy iteration)</figcaption></figure><div class="hint-container info"><p class="hint-container-title">Q</p><p>Why does MC Basic estimate action values instead of state values?</p><ul><li><p>That is because state values cannot be used to improve policies directly. When models are not available, we should directly estimate action values.</p></li><li><p>Since policy iteration is convergent, the convergence of MC Basic is also guaranteed to be convergent given sufficient episodes.</p></li></ul></div><h2 id="monte-carlo-mc-exploring-starts-algorithm" tabindex="-1"><a class="header-anchor" href="#monte-carlo-mc-exploring-starts-algorithm"><span>Monte Carlo (MC) Exploring Starts Algorithm</span></a></h2>',3),x=s("p",null,[a("In MC Basic Algorithm, we have to start from every state-action pair and do many samplings to estimate, which is less efficient. In detail, the episode also visits many other state-action pairs such as "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mo",{stretchy:"false"},"("),s("msub",null,[s("mi",null,"s"),s("mn",null,"2")]),s("mo",{separator:"true"},","),s("msub",null,[s("mi",null,"a"),s("mn",null,"4")]),s("mo",{stretchy:"false"},")"),s("mo",{separator:"true"},","),s("mo",{stretchy:"false"},"("),s("msub",null,[s("mi",null,"s"),s("mn",null,"2")]),s("mo",{separator:"true"},","),s("msub",null,[s("mi",null,"a"),s("mn",null,"3")]),s("mo",{stretchy:"false"},")"),s("mo",{separator:"true"},","),s("mi",null,"a"),s("mi",null,"n"),s("mi",null,"d"),s("mo",{stretchy:"false"},"("),s("msub",null,[s("mi",null,"s"),s("mn",null,"5")]),s("mo",{separator:"true"},","),s("msub",null,[s("mi",null,"a"),s("mn",null,"1")]),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"},"(s_2, a_4), (s_2, a_3), and (s_5, a_1)")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mopen"},"("),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"s"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"2")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"a"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"4")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mclose"},")"),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mopen"},"("),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"s"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"2")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"a"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"3")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mclose"},")"),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord mathnormal"},"an"),s("span",{class:"mord mathnormal"},"d"),s("span",{class:"mopen"},"("),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"s"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"5")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"a"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"1")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mclose"},")")])])]),a(". These visits can also be used to estimate the corresponding action values. In particular, we can decompose the episode into multiple subepisodes:")],-1),v=t('<figure><img src="https://github.com/RyanLee-ljx/RyanLee-ljx.github.io/blob/image/RL/C5/3.png?raw=true" alt="subepisodes" tabindex="0" loading="lazy"><figcaption>subepisodes</figcaption></figure><p>Compare with <em>MC Basic</em>, <em>MC Exploring Starts</em> sufficiently utilize data. The method goes:</p><p>Given a episode, it also focuses on other state-action pairs. Each can be regarded as a start and can be used to do a self-estimation.</p><p>For data appears in one episode, there are two methods:</p><ul><li><p>first-visit: only the one first appear in the episode will be leveraged to do average.</p></li><li><p>every-visit: no matter how many times it appear, all will be used to do average.</p></li></ul><p>For when to update the policy. Also, there are two methods:</p><ul><li>The first method is, in the policy evaluation step, to collect all the episodes starting from a state-action pair and then use the average return to approximate the action value.</li></ul><p>This is the one adopted by the <em>MC Basic algorithm</em>.</p><p>The problem of this method is that the agent has to <strong>wait</strong> until all episodes have been collected.</p><ul><li>The second method uses the return of a single episode to approximate the action value. That means we improve the policy right after one episode not all the episodes. We do not care about the accuracy but should make sure all state-action pairs are considered.(by starting from every state-action pair).</li></ul><p>In this way, we can improve the policy episode-by-episode.</p><p>In fact, this strategy falls into the scope of <em>generalized policy iteration</em> introduced in the last chapter. That is, we can still update the policy even if the value estimate is not sufficiently accurate.</p><figure><img src="https://github.com/RyanLee-ljx/RyanLee-ljx.github.io/blob/image/RL/C5/4.png?raw=true" alt="MC Exploring Starts (an efficient variant of MC Basic)" tabindex="0" loading="lazy"><figcaption>MC Exploring Starts (an efficient variant of MC Basic)</figcaption></figure><p>Q: if a state-action pair does not appear in the first episode. How to calculate the average action value?</p>',14),b=s("h2",{id:"monte-carlo-mc-greedy-algorithm",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#monte-carlo-mc-greedy-algorithm"},[s("span",null,[a("Monte Carlo (MC) "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"ε")]),s("annotation",{encoding:"application/x-tex"},"\\varepsilon")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.4306em"}}),s("span",{class:"mord mathnormal"},"ε")])])]),a("-greedy Algorithm")])])],-1),w=s("p",null,[a("A policy is called "),s("em",null,"soft"),a(" if the probability to take any action is positive.")],-1),f=s("p",null,"With a soft policy, a few episodes that are sufficiently long can visit every state-action pair for sufficiently many times. Then, we do not need to have a large number of episodes starting from every state-action pair. Hence, the requirement of exploring starts (start from every state-action) can thus be removed.",-1),k=s("p",null,[a("The difference between "),s("em",null,"exploring starts"),a(" and "),s("em",null,[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"ε")]),s("annotation",{encoding:"application/x-tex"},"\\varepsilon")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.4306em"}}),s("span",{class:"mord mathnormal"},"ε")])])]),a("-greedy")]),a(" is just the policy turned from deterministic to stochastic. That is, to integrate "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"ε")]),s("annotation",{encoding:"application/x-tex"},"\\varepsilon")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.4306em"}}),s("span",{class:"mord mathnormal"},"ε")])])]),a("-greedy policies into MC learning, we only need to change the policy improvement step from greedy to "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"ε")]),s("annotation",{encoding:"application/x-tex"},"\\varepsilon")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.4306em"}}),s("span",{class:"mord mathnormal"},"ε")])])]),a("-greedy.")],-1),M=s("figure",null,[s("img",{src:"https://github.com/RyanLee-ljx/RyanLee-ljx.github.io/blob/image/RL/C5/5.png?raw=true",alt:"-greedy policy",tabindex:"0",loading:"lazy"}),s("figcaption",null,[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"ε")]),s("annotation",{encoding:"application/x-tex"},"\\varepsilon")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.4306em"}}),s("span",{class:"mord mathnormal"},"ε")])])]),a("-greedy policy")])],-1),L=s("figure",null,[s("img",{src:"https://github.com/RyanLee-ljx/RyanLee-ljx.github.io/blob/image/RL/C5/6.png?raw=true",alt:"-greedy algorithm",tabindex:"0",loading:"lazy"}),s("figcaption",null,[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"ε")]),s("annotation",{encoding:"application/x-tex"},"\\varepsilon")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.4306em"}}),s("span",{class:"mord mathnormal"},"ε")])])]),a("-greedy algorithm")])],-1),_=s("p",null,"It does not require exploring starts, but still requires to visit all state-action pairs in a different form.",-1),z=s("p",null,[a("There are two parameters, namely the "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"ε")]),s("annotation",{encoding:"application/x-tex"},"\\varepsilon")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.4306em"}}),s("span",{class:"mord mathnormal"},"ε")])])]),a(" and the step length of the episode. All of them will affect the algorithm performance.")],-1),R=s("p",null,"We can see from below examples:",-1),C=s("figure",null,[s("img",{src:"https://github.com/RyanLee-ljx/RyanLee-ljx.github.io/blob/image/RL/C5/7.png?raw=true",alt:"example",tabindex:"0",loading:"lazy"}),s("figcaption",null,"example")],-1),j=s("figure",null,[s("img",{src:"https://github.com/RyanLee-ljx/RyanLee-ljx.github.io/blob/image/RL/C5/8.png?raw=true",alt:"example",tabindex:"0",loading:"lazy"}),s("figcaption",null,"example")],-1),T=s("p",null,[a("The feature or advantage of "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"ε")]),s("annotation",{encoding:"application/x-tex"},"\\varepsilon")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.4306em"}}),s("span",{class:"mord mathnormal"},"ε")])])]),a("-greedy is that it has stronger exploration ability so that the exploring starts condition is not required. But it cannot guarantee policy optimalitly in general since its stochastic characteristic. We can only prove there exists the optimal policy and if we take every action with the largest probability, the policy is the same as the optimal. We can say the "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"ε")]),s("annotation",{encoding:"application/x-tex"},"\\varepsilon")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.4306em"}}),s("span",{class:"mord mathnormal"},"ε")])])]),a("-greedy policy is "),s("em",null,"consistent"),a(" with the optimal policy.")],-1),q=s("figure",null,[s("img",{src:"https://github.com/RyanLee-ljx/RyanLee-ljx.github.io/blob/image/RL/C5/9.png?raw=true",alt:"Iteration",tabindex:"0",loading:"lazy"}),s("figcaption",null,"Iteration")],-1),E=s("figure",null,[s("img",{src:"https://github.com/RyanLee-ljx/RyanLee-ljx.github.io/blob/image/RL/C5/10.png?raw=true",alt:"consistency",tabindex:"0",loading:"lazy"}),s("figcaption",null,"consistency")],-1),X=s("p",null,[a("We can see when "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"ε")]),s("annotation",{encoding:"application/x-tex"},"\\varepsilon")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.4306em"}}),s("span",{class:"mord mathnormal"},"ε")])])]),a(" is large, the policy is not consistent with optimal policy.")],-1),B=s("p",null,[a("So in practice, the set of "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"ε")]),s("annotation",{encoding:"application/x-tex"},"\\varepsilon")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.4306em"}}),s("span",{class:"mord mathnormal"},"ε")])])]),a(" can not be too large. And after we gain the consistent "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"ε")]),s("annotation",{encoding:"application/x-tex"},"\\varepsilon")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.4306em"}}),s("span",{class:"mord mathnormal"},"ε")])])]),a("-greedy policy, we turn it into deterministic (fully greedy).")],-1),N=[m,r,p,o,c,h,g,u,d,y,x,v,b,w,f,k,M,L,_,z,R,C,j,T,q,E,X,B];function A(S,I){return l(),n("div",null,N)}const P=e(i,[["render",A],["__file","C5.html.vue"]]),F=JSON.parse(`{"path":"/ML/C5.html","title":"Chapter 5 Monte Carlo Learning","lang":"zh-CN","frontmatter":{"icon":"qianghuaxuexi","ReadingTime":true,"date":"2025-07-12T00:00:00.000Z","Word":true,"PageView":true,"category":"RL","description":"Chapter 5 Monte Carlo Learning This chapter we will introduce a model-free approach for deriving optimal policy. Here, model-free refers that we do not rely on a specific mathem...","head":[["meta",{"property":"og:url","content":"https://ryanlee-ljx.github.io/ML/C5.html"}],["meta",{"property":"og:site_name","content":"RyanLee's blog"}],["meta",{"property":"og:title","content":"Chapter 5 Monte Carlo Learning"}],["meta",{"property":"og:description","content":"Chapter 5 Monte Carlo Learning This chapter we will introduce a model-free approach for deriving optimal policy. Here, model-free refers that we do not rely on a specific mathem..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://github.com/RyanLee-ljx/RyanLee-ljx.github.io/blob/image/RL/C5/1.png?raw=true"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-07-13T05:50:13.000Z"}],["meta",{"property":"article:author","content":"RyanLee_ljx"}],["meta",{"property":"article:published_time","content":"2025-07-12T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-07-13T05:50:13.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Chapter 5 Monte Carlo Learning\\",\\"image\\":[\\"https://github.com/RyanLee-ljx/RyanLee-ljx.github.io/blob/image/RL/C5/1.png?raw=true\\",\\"https://github.com/RyanLee-ljx/RyanLee-ljx.github.io/blob/image/RL/C5/2.png?raw=true\\",\\"https://github.com/RyanLee-ljx/RyanLee-ljx.github.io/blob/image/RL/C5/3.png?raw=true\\",\\"https://github.com/RyanLee-ljx/RyanLee-ljx.github.io/blob/image/RL/C5/4.png?raw=true\\",\\"https://github.com/RyanLee-ljx/RyanLee-ljx.github.io/blob/image/RL/C5/5.png?raw=true\\",\\"https://github.com/RyanLee-ljx/RyanLee-ljx.github.io/blob/image/RL/C5/6.png?raw=true\\",\\"https://github.com/RyanLee-ljx/RyanLee-ljx.github.io/blob/image/RL/C5/7.png?raw=true\\",\\"https://github.com/RyanLee-ljx/RyanLee-ljx.github.io/blob/image/RL/C5/8.png?raw=true\\",\\"https://github.com/RyanLee-ljx/RyanLee-ljx.github.io/blob/image/RL/C5/9.png?raw=true\\",\\"https://github.com/RyanLee-ljx/RyanLee-ljx.github.io/blob/image/RL/C5/10.png?raw=true\\"],\\"datePublished\\":\\"2025-07-12T00:00:00.000Z\\",\\"dateModified\\":\\"2025-07-13T05:50:13.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"RyanLee_ljx\\",\\"email\\":\\"2284771024@qq.com\\"}]}"]]},"headers":[{"level":2,"title":"Monte Carlo (MC) Basic Algorithm","slug":"monte-carlo-mc-basic-algorithm","link":"#monte-carlo-mc-basic-algorithm","children":[]},{"level":2,"title":"Monte Carlo (MC) Exploring Starts Algorithm","slug":"monte-carlo-mc-exploring-starts-algorithm","link":"#monte-carlo-mc-exploring-starts-algorithm","children":[]},{"level":2,"title":"Monte Carlo (MC) -greedy Algorithm","slug":"monte-carlo-mc-greedy-algorithm","link":"#monte-carlo-mc-greedy-algorithm","children":[]}],"git":{"createdTime":1752385813000,"updatedTime":1752385813000,"contributors":[{"name":"Flame","email":"2284771024@qq.com","commits":1}]},"readingTime":{"minutes":4.07,"words":1221},"filePathRelative":"ML/C5.md","localizedDate":"2025年7月12日","excerpt":"\\n<p>This chapter we will introduce a model-free approach for deriving optimal policy.</p>\\n<p>Here, model-free refers that we do not rely on a specific mathematical model to obtain state value or action value. Like, in the policy evaluation, we use <strong>BOE</strong> to obtain state value, which is just <em>model-based</em>. For model-free, we do not use that equation anymore. Instead, we leverage the <em>mean estimation</em> methods.</p>","autoDesc":true}`);export{P as comp,F as data};
