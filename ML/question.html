<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.9" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.36" />
    <style>
      html {
        background: var(--bg-color, #fff);
      }

      html[data-theme="dark"] {
        background: var(--bg-color, #1d1e1f);
      }

      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://ryanlee-ljx.github.io/ML/question.html"><meta property="og:site_name" content="RyanLee's blog"><meta property="og:title" content="机器学习"><meta property="og:description" content="机器学习 本节整理机器学习的基本问题 基本概念 magnitude: 幅度 order of magnitude：数量级 converge: 收敛 oscillate: 振荡 fit: 拟合 fine-tune: 调参 generalization ability：泛化能力 dimension: 量纲 deviation：偏离 一、绪论 什么是机器学习..."><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2024-07-02T22:22:33.000Z"><meta property="article:author" content="RyanLee_ljx"><meta property="article:published_time" content="2024-05-03T00:00:00.000Z"><meta property="article:modified_time" content="2024-07-02T22:22:33.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"机器学习","image":[""],"datePublished":"2024-05-03T00:00:00.000Z","dateModified":"2024-07-02T22:22:33.000Z","author":[{"@type":"Person","name":"RyanLee_ljx","email":"2284771024@qq.com"}]}</script><link rel="icon" href="/logo11.png"><title>机器学习 | RyanLee's blog</title><meta name="description" content="机器学习 本节整理机器学习的基本问题 基本概念 magnitude: 幅度 order of magnitude：数量级 converge: 收敛 oscillate: 振荡 fit: 拟合 fine-tune: 调参 generalization ability：泛化能力 dimension: 量纲 deviation：偏离 一、绪论 什么是机器学习...">
    <link rel="preload" href="/assets/style-DcamKQzH.css" as="style"><link rel="stylesheet" href="/assets/style-DcamKQzH.css">
    <link rel="modulepreload" href="/assets/app-BjuHxB-x.js"><link rel="modulepreload" href="/assets/question.html-COgA0SpX.js"><link rel="modulepreload" href="/assets/plugin-vue_export-helper-DlAUqK2U.js">
    <link rel="prefetch" href="/assets/index.html-B3Bmdw0u.js" as="script"><link rel="prefetch" href="/assets/intro.html-l4AJPPFw.js" as="script"><link rel="prefetch" href="/assets/convolution.html-D53b7QKz.js" as="script"><link rel="prefetch" href="/assets/tec_1.html-CC6FogtU.js" as="script"><link rel="prefetch" href="/assets/vocabulary.html-hQsj0xMw.js" as="script"><link rel="prefetch" href="/assets/chapter1.html-CjEvpWmN.js" as="script"><link rel="prefetch" href="/assets/chapter2.html-CxVkZKS5.js" as="script"><link rel="prefetch" href="/assets/CA.html-hRw-80Nj.js" as="script"><link rel="prefetch" href="/assets/ns.html-Dokw2mni.js" as="script"><link rel="prefetch" href="/assets/pedestrian.html-CYYBKjRY.js" as="script"><link rel="prefetch" href="/assets/404.html-BgtHgQgQ.js" as="script"><link rel="prefetch" href="/assets/index.html-MAcoFw9j.js" as="script"><link rel="prefetch" href="/assets/index.html-DPQTxU2a.js" as="script"><link rel="prefetch" href="/assets/index.html-CUloQJUI.js" as="script"><link rel="prefetch" href="/assets/index.html-BxIXvxos.js" as="script"><link rel="prefetch" href="/assets/index.html-9vk9677B.js" as="script"><link rel="prefetch" href="/assets/index.html-BInewicQ.js" as="script"><link rel="prefetch" href="/assets/index.html-EBEgT8SN.js" as="script"><link rel="prefetch" href="/assets/index.html-6h7Vc68s.js" as="script"><link rel="prefetch" href="/assets/index.html-yU_wJ4uO.js" as="script"><link rel="prefetch" href="/assets/index.html-Di-fkMsp.js" as="script"><link rel="prefetch" href="/assets/index.html-Bh78ZtcV.js" as="script"><link rel="prefetch" href="/assets/index.html-BXAayrRW.js" as="script"><link rel="prefetch" href="/assets/index.html-CMhrwTwI.js" as="script"><link rel="prefetch" href="/assets/waline-meta-l0sNRNKZ.js" as="script"><link rel="prefetch" href="/assets/component-BXR6gvhV.js" as="script"><link rel="prefetch" href="/assets/auto-BwZvv_Gp.js" as="script"><link rel="prefetch" href="/assets/index-uOBkQLRT.js" as="script"><link rel="prefetch" href="/assets/photoswipe.esm-SzV8tJDW.js" as="script"><link rel="prefetch" href="/assets/SearchResult-BA7lq-MK.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><!--[--><div class="theme-container has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><!--[--><a class="route-link vp-brand" href="/"><img class="vp-nav-logo" src="/logo1.png" alt><!----><span class="vp-site-name hide-in-pad">RyanLee&#39;s blog</span></a><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-center"><!--[--><!----><!--]--><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="route-link nav-link" href="/traffic/" aria-label="交通"><span class="font-icon icon iconfont icon-zhihuijiaotong" style=""></span>交通<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link nav-link active" href="/ML/" aria-label="机器学习"><span class="font-icon icon iconfont icon-Machinelearning" style=""></span>机器学习<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link nav-link" href="/path/" aria-label="路径规划"><span class="font-icon icon iconfont icon-lujingguihua" style=""></span>路径规划<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link nav-link" href="/english/" aria-label="英语"><span class="font-icon icon iconfont icon-english" style=""></span>英语<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link nav-link" href="/intro.html" aria-label="Myself"><img class="icon" src="/edg.png" alt aria-hidden no-view style="">Myself<!----></a></div></nav><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!--]--><!--[--><!----><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://github.com/RyanLee-ljx/RyanLee-ljx.github.io" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="vp-nav-item hide-in-mobile"><button type="button" id="appearance-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" style="display:none;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" style="display:block;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><!--[--><button type="button" class="search-pro-button" aria-label="搜索"><svg xmlns="http://www.w3.org/2000/svg" class="icon search-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="search icon"><path d="M192 480a256 256 0 1 1 512 0 256 256 0 0 1-512 0m631.776 362.496-143.2-143.168A318.464 318.464 0 0 0 768 480c0-176.736-143.264-320-320-320S128 303.264 128 480s143.264 320 320 320a318.016 318.016 0 0 0 184.16-58.592l146.336 146.368c12.512 12.48 32.768 12.48 45.28 0 12.48-12.512 12.48-32.768 0-45.28"></path></svg><div class="search-pro-placeholder">搜索</div><div class="search-pro-key-hints"><kbd class="search-pro-key">Ctrl</kbd><kbd class="search-pro-key">K</kbd></div></button><!--]--><!--]--><!--[--><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!--[--><!----><!--]--><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><p class="vp-sidebar-header clickable active"><!----><a class="route-link nav-link active vp-sidebar-title" href="/ML/" aria-label="基本知识"><!---->基本知识<!----></a><!----></p><ul class="vp-sidebar-links"><li><a class="route-link nav-link active vp-sidebar-link vp-sidebar-page active" href="/ML/question.html" aria-label="机器学习"><span class="font-icon icon iconfont icon-jiqixuexi" style=""></span>机器学习<!----></a></li></ul></section></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-header clickable"><!----><a class="route-link nav-link active vp-sidebar-title" href="/ML/" aria-label="卷积"><!---->卷积<!----></a><!----></p><ul class="vp-sidebar-links"><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/ML/convolution.html" aria-label="卷积"><span class="font-icon icon iconfont icon-juanjiceng" style=""></span>卷积<!----></a></li></ul></section></li></ul><!--[--><!----><!--]--></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!--[--><!----><!--]--><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><span class="font-icon icon iconfont icon-jiqixuexi" style=""></span>机器学习</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="page-author-item">RyanLee_ljx</span></span><span property="author" content="RyanLee_ljx"></span></span><!----><span class="page-date-info" aria-label="写作日期📅" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2024-05-03T00:00:00.000Z"></span><span class="page-pageview-info" aria-label="访问量🔢" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon eye-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="eye icon"><path d="M992 512.096c0-5.76-.992-10.592-1.28-11.136-.192-2.88-1.152-8.064-2.08-10.816-.256-.672-.544-1.376-.832-2.08-.48-1.568-1.024-3.104-1.6-4.32C897.664 290.112 707.104 160 512 160c-195.072 0-385.632 130.016-473.76 322.592-1.056 2.112-1.792 4.096-2.272 5.856a55.512 55.512 0 00-.64 1.6c-1.76 5.088-1.792 8.64-1.632 7.744-.832 3.744-1.568 11.168-1.568 11.168-.224 2.272-.224 4.032.032 6.304 0 0 .736 6.464 1.088 7.808.128 1.824.576 4.512 1.12 6.976h-.032c.448 2.08 1.12 4.096 1.984 6.08.48 1.536.992 2.976 1.472 4.032C126.432 733.856 316.992 864 512 864c195.136 0 385.696-130.048 473.216-321.696 1.376-2.496 2.24-4.832 2.848-6.912.256-.608.48-1.184.672-1.728 1.536-4.48 1.856-8.32 1.728-8.32l-.032.032c.608-3.104 1.568-7.744 1.568-13.28zM512 672c-88.224 0-160-71.776-160-160s71.776-160 160-160 160 71.776 160 160-71.776 160-160 160z"></path></svg><span id="ArtalkPV" class="vp-pageview waline-pageview-count" data-path="/ML/question.html" data-page-key="/ML/question.html">...</span></span><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 22 分钟</span><meta property="timeRequired" content="PT22M"></span><span class="page-category-info" aria-label="分类🌈" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item category8 clickable" role="navigation">ML</span><!--]--><meta property="articleSection" content="ML"></span><!----></div><hr></div><div class="vp-toc-placeholder"><aside id="toc"><!--[--><!----><!--]--><div class="vp-toc-header">此页内容<button type="button" class="print-button" title="打印"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button><div class="arrow end"></div></div><div class="vp-toc-wrapper"><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#基本概念">基本概念</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#一、绪论">一、绪论</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#二、模型评估与选择">二、模型评估与选择</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#三、线型模型">三、线型模型</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#四、决策树">四、决策树</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#五、神经网络">五、神经网络</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#六、支持向量机-svm">六、支持向量机 SVM</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#七、聚类-cluster">七、聚类 Cluster</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#八、降维">八、降维</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_1-what-is-regularization">1. What is Regularization?</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_2-what-is-cross-validation">2. What is Cross Validation?</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_3-pca-principle">3. PCA Principle</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_4-k-means-principle">4. K-Means Principle</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_5-support-vector-machine-principle">5. Support Vector Machine Principle</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_6-decision-tree-principle">6. Decision Tree Principle</a></li><!----><!--]--></ul></li><!--]--></ul><div class="vp-toc-marker" style="top:-1.7rem;"></div></div><!--[--><!----><!--]--></aside></div><!--[--><!----><!--]--><div class="theme-hope-content"><h1 id="机器学习" tabindex="-1"><a class="header-anchor" href="#机器学习"><span>机器学习</span></a></h1><p>本节整理机器学习的基本问题</p><h2 id="基本概念" tabindex="-1"><a class="header-anchor" href="#基本概念"><span>基本概念</span></a></h2><p>magnitude: 幅度 order of magnitude：数量级 converge: 收敛 oscillate: 振荡 fit: 拟合 fine-tune: 调参 generalization ability：泛化能力 dimension: 量纲 deviation：偏离</p><h3 id="一、绪论" tabindex="-1"><a class="header-anchor" href="#一、绪论"><span>一、绪论</span></a></h3><ol><li><p>什么是机器学习和深度学习？ 机器学习是一种实现人工智能的方法。人工智能是想要达成的目标，而机器学习是想要达成目标的手段：希望机器通过学习的手段，可以跟人一样聪明。机器学习是研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构，使不断改善自身的性能。而深度学习，就是机器学习的其中一种方法。</p></li><li><p>机器学习流程？ **表示 represent：**将数据对象进行特征(feature)化表示</p><p>**训练 train：**给定一个数据样本集，从中学习出规律（模型），目标：该规律不仅适用于训练数据，也适用于未知数据(称为泛化能力)</p><p>**测试 test：**对于一个新的数据样本，利用学到的模型进行预测</p></li><li><p>在机器学习中，学习率这种参数叫什么？学习率太大和太小的可能影响？</p><p>学习率(learning rate)也叫步长，指更新参数步幅。表征了参数每次更新的幅度（represent the magnitude of parameter update）</p></li></ol><p>If the learning rate is too large, the gradient descent algorithm will not converge and will diverge or oscillate; if the learning rate is too small, the gradient descent algorithm will converge very slowly.</p><ol start="4"><li><p>根据学习方式的划分，机器学习三个主要分类是什么？请简要说明他们之间的关系。 划分为三个主要分类：监督学习、非监督学习、强化学习。关系见下</p></li><li><p>简述监督学习，非监督学习以及强化学习的定义和区别？ <strong>定义：</strong></p></li></ol><p>（1）监督学习：是一种通过使用已知输出来训练模型的学习方式。在监督学习中，训练数据包括输入数据和对应的输出数据（也称为标签或目标），算法通过学习这些数据，建立输入和输出之间的映射关系，以预测新的输入数据的输出。监督学习通常用于分类（分类器）和回归（回归器）问题。</p><p>（2）非监督学习：是一种在没有标签或目标的情况下，从数据中发现模式或结构的学习方式。在非监督学习中，算法只能使用输入数据进行学习，目标是找到输入数据之间的相似性和区别，以便对数据进行聚类、降维、异常检测等操作。</p><p>（3）强化学习：又称为再励学习，是指从环境状态到行为映射的学习，使系统行为从环境中获得的累积奖励值最大的一种机器学习方法。</p><p><strong>区别</strong>：</p><p>（1）监督学习有反馈，无监督学习无反馈，强化学习执行多步后反馈；</p><p>（2）强化学习的目标与监督学习目标不同，强化学习看重行为序列下的长期收益，监督学习关注与标签或已知输出的误差；</p><p>（3）强化学习的奖惩概念没有正确和错误之分，而监督学习的标签是正确的。 强化学习是一个学习+决策的过程，有和环境交互的能力，监督学习不具备。</p><ol start="6"><li>简要说明监督学习和非监督学习之间的区别，并分别给出监督和非监督学习的两种算法。</li></ol><p><strong>区别</strong></p><p>监督学习和非监督学习是机器学习中两种不同的学习方式。监督学习需要已知的输入和输出数据，目标是学习输入 和输出之间的映射关系。非监督学习只需要输入数据，目标是从数据中发现模式和结构，而不需要预先定义的目标 变量。在实际应用中，监督学习和非监督学习常常结合使用，以提高机器学习的效果和性能。</p><p><strong>举例</strong></p><p>常见的监督学习算法包括线性回归（linear regression）、逻辑回归(logistic regression)、决策树(decision tree)、支持向量机(support vector machine)、朴素贝叶斯、神经网络等。</p><p>常见的非监督学习算法包括聚类、主成分分析（Principle Component Analysis）、独立成分分析（ICA）、自编码器、变分自编码器等。</p><ol start="7"><li>训练集、测试集、验证集区别联系？</li></ol><ul><li><p>训练集:用于模型拟合的数据样本,即用于训练的样本集合,主要用来训练神经网络中的参数; train</p></li><li><p>验证集：模型训练过程中单独留出的样本集，它可以用于调整模型的超参数和用于对模型的能力进行初步评估; tuning</p></li><li><p>测试集：用来评估模最终模型的泛化能力。但不能作为调参、选择特征等算法相关的选择的依据。 evaluate</p></li></ul><p>各数据集的作用</p><p><strong>训练集的作用</strong></p><p>拟合模型，调整网络权重。</p><p><strong>验证集的作用</strong></p><ul><li><p>作用 1：快速调参，也就是通过验证集我们可以选择超参数（网络层数、网络节点数、迭代次数 epoch、学习率 learning rate、优化器）等。</p></li><li><p>作用 2：选择超参数，为了让我们的模型在测试集表现得更好，调参是不可避免地一部分，如果把测试集当验证集，调参去拟合测试集合，是不可行地，这相当于作弊。</p></li><li><p>作用 3：监控模型是否正常</p></li></ul><div class="hint-container important"><p class="hint-container-title">验证集的重要性</p><p>如果没有设置验证集，我们通常得等到测试集才可以知道我们模型真正得实力，然后再来调整参数，这样子时间代价较高，通过验证集我们可以训练几个 epoch 后查看模型的训练效果及我们的网络是否出现异常，然后决定怎么调整我们的超参数。</p></div><p><strong>测试集的作用</strong></p><p>仅仅用来评估模最终模型的泛化能力，确认网络的实际预测能力。</p><ol start="8"><li><p>什么是泛化性能？（generalization ability） 是指机器学习算法对新鲜样本的适应能力。 学习的目的是学到隐含在数据背后的规律，对具有同一规律的学习集以外的数据，经过训练的网络也能给出合适的输出，该能力称为泛化能力。</p></li><li><p>在数据处理时，为什么通常要进行标准化处理 在实际问题中，我们使用的样本通常是多维数据，每一维对应一个特征，这些特征的量纲和数量级都是不一样的 这时需要对数据进行标准化处理，使得所有的特征具有同样的尺度。</p></li></ol><h3 id="二、模型评估与选择" tabindex="-1"><a class="header-anchor" href="#二、模型评估与选择"><span>二、模型评估与选择</span></a></h3><ol><li>过拟合、欠拟合定义、原因、解决办法？</li></ol><p><strong>（1）定义：</strong></p><p>过拟合：学习器把训练样本学习的“太好”，将训练样本本身的特点当做所有（潜在）样本（都会具有）的一般性质，导致泛化性能下降。</p><p>欠拟合：训练样本的一般性质尚未学好，在训练样本上都存在较大的经验误差。</p><p><strong>（2）原因：</strong></p><p>过拟合原因： 1）模型复杂度过低 2）特征量过少 3）训练样本过少</p><p>欠拟合原因： 1）训练集的数量级 N 与模型复杂度 M 不匹配 2）训练集与测试集的特征分布不一致 3）样本噪声过多，模型过分记住了噪声，忽略了真实的输入输出 4）权值学习迭代次数过多，拟合了不具代表性的特征</p><p><strong>（3）解决办法：</strong></p><p>过拟合: 1）清洗数据 2）减小模型复杂度 3）增广训练集 4）交叉验证 5）正则化 regularization，约束模型特征 6）early stopping 迭代次数截断 7）dropout，让一些神经元以一定的概率不工作.</p><p>欠拟合应对： 1）增加新特征 2）增加模型复杂度，如决策树的扩展分支，神经网络的训练轮数等 3）扩大训练集。</p><ol start="2"><li>错误率及误差概念？</li></ol><p>错误率 &amp; 误差：</p><p>（1）错误率：错分样本的占比</p><p>（2）误差：样本真实输出与预测输出之间的差异</p><p>（3）训练(经验)误差： 训练集上</p><p>（4）测试误差： 测试集上</p><p>（5）泛化误差： 除训练集外所有样本</p><ol start="3"><li><p>评估方法</p><p>留出法、交叉验证法（k=m 时是留一法）、自助法等。</p></li><li><p>交叉验证法和自助法异同？</p></li></ol><p><strong>相同点</strong></p><p>交叉验证法和自助法都是随机采样法。它们作为人工智能中评估模型的方法，根据一定规则从数据集 D 中划分训练集和测试（验证）集，从而评价模型在数据集上的表现，便于我们选择合适的模型。</p><p><strong>不同点</strong></p><p>这两种方法最大的不同点在于每次划分过程中每个样本点是否只有一次被划入训练集或测试集的机会。下面将针对这方面详细展开论述：</p><ul><li><p>交叉验证法采用的是无放回的随机采样方式，这种方式可以保持数据分布的一致性条件，并严格划分训练集与测试集的界限，从而增强测试评估的稳定性和可靠性。</p></li><li><p>自助法主要面向数据集同规模的划分问题。其采用的是有放回的随机抽样方法，可以使得得到的模型更为稳健，解决了交叉验证法中模型选择阶段和最终模型训练阶段的训练集规模差异问题；但训练集 T 和原始数据集 D 中数据的分布未必相一致，因此对一些对数据分布敏感的模型选择并不适用。</p></li></ul><ol start="5"><li>偏差、方差、噪声含义？</li></ol><ul><li><p>偏差度量了学习算法期望预测与真实结果的偏离程度: 即刻画了学习法本身的拟合能力;</p></li><li><p>方差度量了同样大小训练集的变动所导致的学习性能的变化: 即刻画了数据扰动所造成的影响;</p></li><li><p>噪声表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界即刻画了学习问题本身的难度。</p></li><li><p>Bias measures the degree of deviation between the expected prediction of the learning algorithm and the actual result: it depicts the fitting ability of the learning method itself;</p></li><li><p>Variance measures the change in learning performance caused by changes in the same size training set: it depicts the impact of data perturbations;</p></li><li><p>Noise expresses the lower bound of the expected generalization error that any learning algorithm can achieve on the current task, which depicts the difficulty of the learning problem itself.</p></li></ul><ol start="6"><li><p>偏差-方差分解角度解释泛化性能 泛化性能是出学习算法的能力、数据的充分性以及学习任务本身的难度所共同决定的。给定学习任务为了取得好的泛化性能，需要使偏差小(充分拟合数据)而且方差较小(减少数据扰动产生的影响)。</p></li><li><p>偏差方差冲突 PPT P9</p></li></ol><p>8．请给出你对泛化误差的理解</p><p><strong>泛化误差 = 偏差+方差+噪声</strong></p><p>偏差：度量了学习算法的期望预测与真实结果的偏离程度，刻画了学习算法本身的拟合能力</p><p>方差：度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响</p><p>噪声：表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度</p><h3 id="三、线型模型" tabindex="-1"><a class="header-anchor" href="#三、线型模型"><span>三、线型模型</span></a></h3><ol><li><p>线型模型优势与不足？ 优势：第三章 3.1.2 P1 不足： 第三章 3.2.7 P10</p></li><li><p>什么是回归（分析）？ 第三章 3.2 P2</p></li><li><p>简述 LDA 算法的基本思想及算法流程。</p></li></ol><p>基本思想是将高维的模式样本投影到最佳鉴别矢量空间，以达到抽取分类信息和压缩特征空间维数的效果，投影后保证模式样本在新的子空间有最大的类间距离和最小的类内距离,即模式在该空间中有最佳的可分离性。 流程：</p><ol start="4"><li>逻辑回归和线性回归的异同</li></ol><p><strong>不同之处</strong>：</p><p>（1）逻辑回归解决的是分类问题，因此因变量是离散的；而线性回归解决的是回归问题，因此因变量是连续的。这是两者最本质的区别；</p><p>（2）在自变量和超参数确定的情况下逻辑回归可看作广义的线性模型在因变量下服从二元分布的一个特殊情况</p><p>（3）使用最小二乘法求解线性回归时我们认为因变量服从正态分布。</p><p><strong>相同之处</strong>：</p><p>（1）二者在求解超参数的过程中都使用梯度下降的方法 ；</p><p>（2）二者都使用了极大似然估计对训练样本进行建模。</p><h3 id="四、决策树" tabindex="-1"><a class="header-anchor" href="#四、决策树"><span>四、决策树</span></a></h3><ol><li><p>决策树三种导致递归返回的情况</p><p>PPT 第四章 4.1 P1</p></li><li><p>决策树中剪枝方式分为哪两种？请简述这两种方式的优缺点？</p><p>预剪枝（prepruning）和后剪枝(postpruning)。</p><p><strong>预剪枝</strong></p><ul><li><p>优点：降低过拟合风险，显著降低训练时间和测试时间的开销。</p></li><li><p>缺点：有些分支当前划分虽然不能提升泛化性能，但在其基础上进行的后续划分有可能使得性能显著提高，预剪枝基于“贪心”，禁止这些分支展开，有欠拟合风险。</p></li></ul><p><strong>后剪枝</strong></p><ul><li><p>优点：比预剪枝保留了更多分支，欠拟合风险小，泛化性能更好。</p></li><li><p>缺点：训练时间开销大，后剪枝过程是在生成完全的决策树之后，自底向上对所有非叶节点逐一考察。</p></li></ul></li></ol><h3 id="五、神经网络" tabindex="-1"><a class="header-anchor" href="#五、神经网络"><span>五、神经网络</span></a></h3><ol><li>在神经网络中，非线性激活函数的主要作用是什么？请给出常用的几种非线性激活函数及其导数；</li></ol><p>激活函数（activation function）可以加入非线性因素，解决线性模型所不能解决的问题。激活函数是神经网络的一个重要组成部分。如果不使用激活函数，则无论该神经网络有多少层，最终的输出都是输入的线性组合，与没有隐藏层的效果相当，这种情况就是最原始的感知机（perceptron），无法解决线性不可分问题。</p><ol start="2"><li>神经网络分类</li></ol><p>神经网络根据是否存在网络回路（联接方式），可以分为：前向型和反馈型。 按学习方式：有导师的学习（监督学习）、无导师的学习（无监督学习）、再励学习（强化学习），具体解释： PPT 第五章 P6-7</p><ol start="3"><li><p>神经网络与人脑相比计算特能力特点： PPT 第五章 P5</p></li><li><p>简述神经网络的学习过程 PPT 第五章 P5</p></li><li><p>简要介绍卷积概念及其作用、池化的作用是什么？卷积前后图像尺寸之间的关系是什么？</p></li><li><p>简述神经网络中梯度下降方法的原理和作用（作用请从机器学习训练阶段的三个步骤的角度来阐述）。 梯度下降方法的原理：梯度下降方法通过求出损失函数在某点对于参数 θ 的微分值，并以负梯度方向为搜索方向，沿着梯度下降的方向求解极小值；作用是在训练阶段的第三个步骤中，通过梯度下降来寻找更优的学习参数 b 和 w，达到优化模型的效果。</p></li></ol><h3 id="六、支持向量机-svm" tabindex="-1"><a class="header-anchor" href="#六、支持向量机-svm"><span>六、支持向量机 SVM</span></a></h3><ol><li>试述硬间隔、软间隔、基于核函数的 SVM 的原理、优缺点、三者最终计算方式以及限制条件。 <strong>硬间隔 SVM</strong>：</li></ol><ul><li><p>原理：硬间隔 SVM 假设数据本身是线性可分的，即存在一个超平面可以将不同类别的样本完全分开。这个超平面需要满足离其最近的点到其的距离最大化。</p></li><li><p>优点：简单明了，适用于线性可分的数据集。</p></li><li><p>缺点：对于非线性可分的数据集，硬间隔 SVM 无法找到一个有效的超平面。此外，硬间隔 SVM 对异常点非常敏感，因为异常点可能导致无法找到一个满足所有约束条件的超平面。</p></li></ul><p><strong>软间隔 SVM</strong>：</p><ul><li><p>原理：软间隔 SVM 放松了对数据线性可分的假设，允许在某些情况下出现分类错误。软间隔 SVM 通过引入松弛变量来处理噪声和异常点。通过调整这些变量，可以控制对分类错误的容忍程度。</p></li><li><p>优点：能够处理非线性可分的数据集和噪声数据。</p></li><li><p>缺点：需要调整松弛变量和惩罚参数，以找到最佳的分类效果。</p></li></ul><p><strong>基于核函数的 SVM</strong>：</p><ul><li><p>原理：对于非线性可分的数据集，基于核函数的 SVM 通过使用核函数将输入空间映射到高维特征空间，使得在高维特征空间中数据变得线性可分。常用的核函数有线性核、多项式核和 RBF 核（高斯核）。</p></li><li><p>优点：能够处理非线性可分的数据集。</p></li><li><p>缺点：选择合适的核函数和参数是一个挑战，同时计算复杂度可能会较高。</p></li></ul><ol start="2"><li>SVM 与 logistic 回归区别联系</li></ol><p><strong>相同点</strong>:</p><p>都是分类算法 如果不考虑核函数，LR 和 SVM 都是线性分类算法 LR 和 SVM 都是监督学习算法。 LR 和 SVM 都是判别模型.</p><p><strong>不同点</strong>:</p><p>本质上是其 loss function 不同 支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局. 在解决非线性问题时，支持向量机采用核函数的机制，而 LR 通常不采用核函数的方法 线性 SVM 依赖数据表达的距离测度，所以需要对数据先做 normalization，LR 不受其影响。 SVM 的损失函数就自带正则。</p><ol start="3"><li>SVM、logistic 回归、决策树各自优缺点</li></ol><p><strong>逻辑回归</strong></p><p>逻辑回归的优点：</p><p>（1）便利的观测样本概率分数；</p><p>（2）已有工具的高效实现；</p><p>（3）对逻辑回归而言，多重共线性并不是问题，它可以结合 L2 正则化来解决；</p><p>（4）逻辑回归广泛的应用于工业问题上（这一点很重要）。</p><p>逻辑回归的缺点： （1）当特征空间很大时，逻辑回归的性能不是很好；</p><p>（2）不能很好地处理大量多类特征或变量；</p><p>（3）对于非线性特征，需要进行转换；</p><p>（4）依赖于全部的数据（个人觉得这并不是一个很严重的缺点）。</p><p><strong>决策树</strong></p><p>（1）直观的决策规则</p><p>（2）可以处理非线性特征</p><p>（3）考虑了变量之间的相互作用</p><p>（4）训练集上的效果高度优于测试集，即过拟合[随机森林克服了此缺点</p><p>（5）没有将排名分数作为直接结果</p><p><strong>SVM</strong></p><p>SVM 的优点：</p><p>（1）能够处理大型特征空间</p><p>（2）能够处理非线性特征之间的相互作用</p><p>（3）无需依赖整个数据</p><p>SVM 的缺点：</p><p>（1）当观测样本很多时，效率并不是很高</p><p>（2）有时候很难找到一个合适的核函数</p><ol start="4"><li>为什么要引入对偶问题? （1）对偶问题将原始问题中的约束转为了对偶问题中的等式约束。</li></ol><p>（2）改变了问题的复杂度。由求特征向量转化为求比例系数。在原始问题下，求解的复杂度与样本的维度有关即 w 的维度。在对偶问题下，求解的是 a，复杂度只与样本数量有关。</p><p>（3）可以自然地引入核函数，从而推广到非线性分类问题</p><h3 id="七、聚类-cluster" tabindex="-1"><a class="header-anchor" href="#七、聚类-cluster"><span>七、聚类 Cluster</span></a></h3><ol><li>聚类方法分类？</li></ol><p>无监督学习方法主要有两大类：基于概率密度函数的估计方法和基于样本间相似性度量的间接聚类方法。</p><ol start="2"><li>K-means、层次聚类、DBSCAN 聚类方法原理、区别、优缺点</li></ol><p><strong>K-means 聚类</strong></p><p>工作原理： K-means 算法将数据划分为 K 个簇，每个簇包含最接近其质心的数据点。它通过迭代地将数据点分配给最近的质心并更新质心来执行聚类。</p><p>优点： 简单且高效，适用于大型数据集。它的结果易于解释和可视化。</p><p>缺点：需要事先指定簇数 K。对于非球形簇或具有不同密度的簇效果较差。</p><p><strong>层次聚类</strong></p><p>工作原理： 层次聚类将数据集逐渐分割或合并成不同的层次簇。它可以是自底向上的聚合聚类（凝聚型）或自顶向下的分裂聚类（分裂型）。</p><p>优点： 不需要预先指定簇数，可视化结果以树状结构呈现，对不同形状的簇和噪声具有较好的鲁棒性。</p><p>缺点：不具有很好的可伸缩性: 时间复杂性至少是 0(n^2)，其中 n 对象总数。合并或分裂的决定需要检查和估算大量的对象或簇。不能撤消已做的处理，聚类之间不能交换对象。如果某一步没有很好地选择合并或分裂的决定，可能会导致低质量的聚类结果</p><p><strong>DBSCAN（密度聚类）</strong></p><p>工作原理： DBSCAN 根据数据点的密度将它们分为核心点、边界点和噪声点。核心点是在指定半径范围内有足够多邻居的点，它们被用于扩展簇。</p><p>优点： 可以对任意形状的稠密数据集进行聚类。可以在聚类的同时发现异常点，对数据集异常点不敏感。聚类结果没有偏倚。</p><p>缺点： 对参数的选择敏感，可能需要调整半径参数和最小邻居数。对高维数据和不均匀密度数据的处理相对困难。样本集较大时，聚类收敛时间较长。</p><p>区别：</p><p>（1）簇数的预先指定： K-means 需要提前指定簇数 K，而层次聚类和 DBSCAN 不需要。层次聚类会生成层次结构，可以根据需要切割簇。DBSCAN 通过密度自动确定簇的数量。</p><p>（2）对簇形状和密度的适应性： K-means 假定簇是球形且密度均匀，不适合不规则形状和不同密度的簇。层次聚类和 DBSCAN 能够发现各种形状和密度的簇。</p><p>（3）计算复杂度： K-means 通常是最快的，DBSCAN 次之，而层次聚类较慢，特别是在大型数据集上。</p><p>（4）噪声处理： DBSCAN 在处理噪声点时比较鲁棒，可以将它们识别为噪声。K-means 和层次聚类通常需要额外的后处理步骤来处理噪声点。</p><h3 id="八、降维" tabindex="-1"><a class="header-anchor" href="#八、降维"><span>八、降维</span></a></h3><ol><li>请简要说明主成分分析（PCA）和线性判别分析（LDA）之间的区别和联系</li></ol><p><strong>相同点：</strong></p><p>（1）两者均可以对数据进行降维。</p><p>（2）两者在降维时均使用了矩阵特征分解的思想。</p><p>（3）两者都假设数据符合高斯分布。</p><p><strong>不同点：</strong></p><p>（1）LDA 是有监督的降维方法，而 PCA 是无监督的降维方法</p><p>（2）LDA 降维最多降到类别数 k-1 的维数，而 PCA 没有这个限制。</p><p>（3）LDA 除了可以用于降维，还可以用于分类。</p><p>（4）LDA 选择分类性能最好的投影方向，而 PCA 选择样本点投影具有最大方差的方向。</p><h3 id="_1-what-is-regularization" tabindex="-1"><a class="header-anchor" href="#_1-what-is-regularization"><span>1. What is Regularization?</span></a></h3><p>Regularization is a technique used in machine learning to prevent overfitting by adding a <strong>penalty</strong>（惩罚项） to the model&#39;s complexity. It works by incorporating additional terms to the loss function, such as <strong>L1 (Lasso)</strong>（L1 正则化） or <strong>L2 (Ridge)</strong>（L2 正则化） penalties, which constrain the magnitude of the model parameters. This helps in reducing the variance without substantially increasing the bias, leading to better generalization on unseen data.</p><h3 id="_2-what-is-cross-validation" tabindex="-1"><a class="header-anchor" href="#_2-what-is-cross-validation"><span>2. What is Cross Validation?</span></a></h3><p>Cross validation is a technique for assessing the performance and robustness of a machine learning model. It involves partitioning the dataset into training and validation sets multiple times to ensure that the model&#39;s performance is evaluated on different subsets of data. The most common method is <strong>k-fold cross validation</strong>（k 折交叉验证）, where the dataset is divided into k equally sized folds, and the model is trained and validated k times, each time using a different fold as the validation set and the remaining folds as the training set. This helps in obtaining a more reliable estimate of model performance.</p><h3 id="_3-pca-principle" tabindex="-1"><a class="header-anchor" href="#_3-pca-principle"><span>3. PCA Principle</span></a></h3><p><strong>Principal Component Analysis (PCA)</strong>（主成分分析） is a dimensionality reduction technique that transforms the data into a new coordinate system such that the greatest variances by any projection of the data come to lie on the first coordinates (called <strong>principal components</strong>（主成分）). It achieves this by calculating the <strong>eigenvectors</strong>（特征向量） and <strong>eigenvalues</strong>（特征值） of the data&#39;s covariance matrix. The principal components are orthogonal to each other, and by selecting the top k principal components, we can reduce the dimensionality of the data while preserving most of its variance.</p><h3 id="_4-k-means-principle" tabindex="-1"><a class="header-anchor" href="#_4-k-means-principle"><span>4. K-Means Principle</span></a></h3><p><strong>K-Means</strong>（K 均值） is a clustering algorithm that partitions a dataset into k distinct, non-overlapping subgroups (<strong>clusters</strong>（簇）). It works by initializing k <strong>centroids</strong>（质心） randomly, assigning each data point to the nearest centroid, and then updating the centroids to be the mean of the data points assigned to them. This process iterates until the centroids no longer change significantly. The goal is to minimize the within-cluster variance, resulting in compact and well-separated clusters.</p><h3 id="_5-support-vector-machine-principle" tabindex="-1"><a class="header-anchor" href="#_5-support-vector-machine-principle"><span>5. Support Vector Machine Principle</span></a></h3><p><strong>Support Vector Machine (SVM)</strong>（支持向量机） is a supervised learning algorithm used for classification and regression tasks. The principle of SVM is to find the optimal <strong>hyperplane</strong>（超平面） that best separates the data into different classes. This hyperplane is defined by <strong>support vectors</strong>（支持向量）, which are the data points closest to the hyperplane. SVM aims to maximize the <strong>margin</strong>（间隔）, which is the distance between the hyperplane and the support vectors. For non-linearly separable data, SVM uses <strong>kernel functions</strong>（核函数） to transform the data into a higher-dimensional space where a linear separator can be found.</p><h3 id="_6-decision-tree-principle" tabindex="-1"><a class="header-anchor" href="#_6-decision-tree-principle"><span>6. Decision Tree Principle</span></a></h3><p>A <strong>Decision Tree</strong>（决策树） is a supervised learning algorithm used for both classification and regression tasks. It works by recursively splitting the data based on feature values to create a tree structure, where each internal node represents a decision based on a feature, each branch represents the outcome of the decision, and each leaf node represents a class label or a continuous value. The splits are chosen to maximize the reduction in <strong>impurity</strong>（杂质）, commonly measured by metrics like <strong>Gini impurity</strong>（基尼杂质） or <strong>information gain (entropy)</strong>（信息增益或熵）. The resulting model is a tree that predicts the target variable by traversing from the root to a leaf node based on the feature values of the input data.</p></div><!--[--><!----><!--]--><footer class="vp-page-meta"><div class="vp-meta-item edit-link"><a href="https://github.com/RyanLee-ljx/RyanLee-ljx.github.io/edit/main/src/ML/question.md" rel="noopener noreferrer" target="_blank" aria-label="Edit this page on GitHub" class="nav-link vp-meta-label"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->Edit this page on GitHub<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!----></a></div><div class="vp-meta-item git-info"><div class="update-time"><span class="vp-meta-label">上次编辑于: </span><!----></div><div class="contributors"><span class="vp-meta-label">贡献者: </span><!--[--><!--[--><span class="vp-meta-info" title="email: 2284771024@qq.com">Flame</span><!--]--><!--]--></div></div></footer><!----><div id="vp-comment" class="waline-wrapper" darkmode="false" style="display:block;"><div data-waline provider="Waline"><!--v-if--><div class="wl-comment"><!--v-if--><div class="wl-panel"><div class="wl-header item3"><!--[--><div class="wl-header-item"><label for="wl-nick">昵称</label><input id="wl-nick" class="wl-input wl-nick" name="nick" type="text" value></div><div class="wl-header-item"><label for="wl-mail">邮箱</label><input id="wl-mail" class="wl-input wl-mail" name="mail" type="email" value></div><div class="wl-header-item"><label for="wl-link">网址</label><input id="wl-link" class="wl-input wl-link" name="link" type="text" value></div><!--]--></div><textarea id="wl-edit" class="wl-editor" placeholder="请留言。(填写邮箱可在被回复时收到邮件提醒)"></textarea><div class="wl-preview" style="display:none;"><hr><h4>预览:</h4><div class="wl-content"></div></div><div class="wl-footer"><div class="wl-actions"><a href="https://guides.github.com/features/mastering-markdown/" title="Markdown Guide" aria-label="Markdown is supported" class="wl-action" target="_blank" rel="noopener noreferrer"><svg width="16" height="16" ariaHidden="true"><path d="M14.85 3H1.15C.52 3 0 3.52 0 4.15v7.69C0 12.48.52 13 1.15 13h13.69c.64 0 1.15-.52 1.15-1.15v-7.7C16 3.52 15.48 3 14.85 3zM9 11H7V8L5.5 9.92 4 8v3H2V5h2l1.5 2L7 5h2v6zm2.99.5L9.5 8H11V5h2v3h1.5l-2.51 3.5z" fill="currentColor"></path></svg></a><button type="button" class="wl-action" title="表情" style="display:none;"><svg viewBox="0 0 1024 1024" width="24" height="24"><path d="M563.2 463.3 677 540c1.7 1.2 3.7 1.8 5.8 1.8.7 0 1.4-.1 2-.2 2.7-.5 5.1-2.1 6.6-4.4l25.3-37.8c1.5-2.3 2.1-5.1 1.6-7.8s-2.1-5.1-4.4-6.6l-73.6-49.1 73.6-49.1c2.3-1.5 3.9-3.9 4.4-6.6.5-2.7 0-5.5-1.6-7.8l-25.3-37.8a10.1 10.1 0 0 0-6.6-4.4c-.7-.1-1.3-.2-2-.2-2.1 0-4.1.6-5.8 1.8l-113.8 76.6c-9.2 6.2-14.7 16.4-14.7 27.5.1 11 5.5 21.3 14.7 27.4zM387 348.8h-45.5c-5.7 0-10.4 4.7-10.4 10.4v153.3c0 5.7 4.7 10.4 10.4 10.4H387c5.7 0 10.4-4.7 10.4-10.4V359.2c0-5.7-4.7-10.4-10.4-10.4zm333.8 241.3-41-20a10.3 10.3 0 0 0-8.1-.5c-2.6.9-4.8 2.9-5.9 5.4-30.1 64.9-93.1 109.1-164.4 115.2-5.7.5-9.9 5.5-9.5 11.2l3.9 45.5c.5 5.3 5 9.5 10.3 9.5h.9c94.8-8 178.5-66.5 218.6-152.7 2.4-5 .3-11.2-4.8-13.6zm186-186.1c-11.9-42-30.5-81.4-55.2-117.1-24.1-34.9-53.5-65.6-87.5-91.2-33.9-25.6-71.5-45.5-111.6-59.2-41.2-14-84.1-21.1-127.8-21.1h-1.2c-75.4 0-148.8 21.4-212.5 61.7-63.7 40.3-114.3 97.6-146.5 165.8-32.2 68.1-44.3 143.6-35.1 218.4 9.3 74.8 39.4 145 87.3 203.3.1.2.3.3.4.5l36.2 38.4c1.1 1.2 2.5 2.1 3.9 2.6 73.3 66.7 168.2 103.5 267.5 103.5 73.3 0 145.2-20.3 207.7-58.7 37.3-22.9 70.3-51.5 98.1-85 27.1-32.7 48.7-69.5 64.2-109.1 15.5-39.7 24.4-81.3 26.6-123.8 2.4-43.6-2.5-87-14.5-129zm-60.5 181.1c-8.3 37-22.8 72-43 104-19.7 31.1-44.3 58.6-73.1 81.7-28.8 23.1-61 41-95.7 53.4-35.6 12.7-72.9 19.1-110.9 19.1-82.6 0-161.7-30.6-222.8-86.2l-34.1-35.8c-23.9-29.3-42.4-62.2-55.1-97.7-12.4-34.7-18.8-71-19.2-107.9-.4-36.9 5.4-73.3 17.1-108.2 12-35.8 30-69.2 53.4-99.1 31.7-40.4 71.1-72 117.2-94.1 44.5-21.3 94-32.6 143.4-32.6 49.3 0 97 10.8 141.8 32 34.3 16.3 65.3 38.1 92 64.8 26.1 26 47.5 56 63.6 89.2 16.2 33.2 26.6 68.5 31 105.1 4.6 37.5 2.7 75.3-5.6 112.3z" fill="currentColor"></path></svg></button><button type="button" class="wl-action" title="表情包"><svg width="24" height="24" fill="currentcolor" viewBox="0 0 24 24"><path style="transform: translateY(0.5px)" d="M18.968 10.5H15.968V11.484H17.984V12.984H15.968V15H14.468V9H18.968V10.5V10.5ZM8.984 9C9.26533 9 9.49967 9.09367 9.687 9.281C9.87433 9.46833 9.968 9.70267 9.968 9.984V10.5H6.499V13.5H8.468V12H9.968V14.016C9.968 14.2973 9.87433 14.5317 9.687 14.719C9.49967 14.9063 9.26533 15 8.984 15H5.984C5.70267 15 5.46833 14.9063 5.281 14.719C5.09367 14.5317 5 14.2973 5 14.016V9.985C5 9.70367 5.09367 9.46933 5.281 9.282C5.46833 9.09467 5.70267 9.001 5.984 9.001H8.984V9ZM11.468 9H12.968V15H11.468V9V9Z"></path><path d="M18.5 3H5.75C3.6875 3 2 4.6875 2 6.75V18C2 20.0625 3.6875 21.75 5.75 21.75H18.5C20.5625 21.75 22.25 20.0625 22.25 18V6.75C22.25 4.6875 20.5625 3 18.5 3ZM20.75 18C20.75 19.2375 19.7375 20.25 18.5 20.25H5.75C4.5125 20.25 3.5 19.2375 3.5 18V6.75C3.5 5.5125 4.5125 4.5 5.75 4.5H18.5C19.7375 4.5 20.75 5.5125 20.75 6.75V18Z"></path></svg></button><input id="wl-image-upload" class="upload" type="file" accept=".png,.jpg,.jpeg,.webp,.bmp,.gif"><label for="wl-image-upload" class="wl-action" title="上传图片"><svg viewBox="0 0 1024 1024" width="24" height="24"><path d="M784 112H240c-88 0-160 72-160 160v480c0 88 72 160 160 160h544c88 0 160-72 160-160V272c0-88-72-160-160-160zm96 640c0 52.8-43.2 96-96 96H240c-52.8 0-96-43.2-96-96V272c0-52.8 43.2-96 96-96h544c52.8 0 96 43.2 96 96v480z" fill="currentColor"></path><path d="M352 480c52.8 0 96-43.2 96-96s-43.2-96-96-96-96 43.2-96 96 43.2 96 96 96zm0-128c17.6 0 32 14.4 32 32s-14.4 32-32 32-32-14.4-32-32 14.4-32 32-32zm462.4 379.2-3.2-3.2-177.6-177.6c-25.6-25.6-65.6-25.6-91.2 0l-80 80-36.8-36.8c-25.6-25.6-65.6-25.6-91.2 0L200 728c-4.8 6.4-8 14.4-8 24 0 17.6 14.4 32 32 32 9.6 0 16-3.2 22.4-9.6L380.8 640l134.4 134.4c6.4 6.4 14.4 9.6 24 9.6 17.6 0 32-14.4 32-32 0-9.6-4.8-17.6-9.6-24l-52.8-52.8 80-80L769.6 776c6.4 4.8 12.8 8 20.8 8 17.6 0 32-14.4 32-32 0-8-3.2-16-8-20.8z" fill="currentColor"></path></svg></label><button type="button" class="wl-action" title="预览"><svg viewBox="0 0 1024 1024" width="24" height="24"><path d="M710.816 654.301c70.323-96.639 61.084-230.578-23.705-314.843-46.098-46.098-107.183-71.109-172.28-71.109-65.008 0-126.092 25.444-172.28 71.109-45.227 46.098-70.756 107.183-70.756 172.106 0 64.923 25.444 126.007 71.194 172.106 46.099 46.098 107.184 71.109 172.28 71.109 51.414 0 100.648-16.212 142.824-47.404l126.53 126.006c7.058 7.06 16.297 10.979 26.406 10.979 10.105 0 19.343-3.919 26.402-10.979 14.467-14.467 14.467-38.172 0-52.723L710.816 654.301zm-315.107-23.265c-65.88-65.88-65.88-172.54 0-238.42 32.069-32.07 74.245-49.149 119.471-49.149 45.227 0 87.407 17.603 119.472 49.149 65.88 65.879 65.88 172.539 0 238.42-63.612 63.178-175.242 63.178-238.943 0zm0 0" fill="currentColor"></path><path d="M703.319 121.603H321.03c-109.8 0-199.469 89.146-199.469 199.38v382.034c0 109.796 89.236 199.38 199.469 199.38h207.397c20.653 0 37.384-16.645 37.384-37.299 0-20.649-16.731-37.296-37.384-37.296H321.03c-68.582 0-124.352-55.77-124.352-124.267V321.421c0-68.496 55.77-124.267 124.352-124.267h382.289c68.582 0 124.352 55.771 124.352 124.267V524.72c0 20.654 16.736 37.299 37.385 37.299 20.654 0 37.384-16.645 37.384-37.299V320.549c-.085-109.8-89.321-198.946-199.121-198.946zm0 0" fill="currentColor"></path></svg></button></div><div class="wl-info"><div class="wl-captcha-container"></div><div class="wl-text-number">0 <!--v-if-->  字</div><button type="button" class="wl-btn">登录</button><button type="submit" class="primary wl-btn" title="Cmd|Ctrl + Enter"><!--[-->提交<!--]--></button></div><div class="wl-gif-popup"><input type="text" placeholder="搜索表情包"><!--v-if--><div class="wl-loading"><svg width="30" height="30" viewBox="0 0 100 100" preserveAspectRatio="xMidYMid"><circle cx="50" cy="50" fill="none" stroke="currentColor" strokeWidth="4" r="40" stroke-dasharray="85 30"><animateTransform attributeName="transform" type="rotate" repeatCount="indefinite" dur="1s" values="0 50 50;360 50 50" keyTimes="0;1"></animateTransform></circle></svg></div></div><div class="wl-emoji-popup"><!--[--><!--]--><!--v-if--></div></div></div><!--v-if--></div><div class="wl-meta-head"><div class="wl-count"><!--v-if--> 评论</div><ul class="wl-sort"><!--[--><li class="active">按正序</li><li class="">按倒序</li><li class="">按热度</li><!--]--></ul></div><div class="wl-cards"><!--[--><!--]--></div><div class="wl-loading"><svg width="30" height="30" viewBox="0 0 100 100" preserveAspectRatio="xMidYMid"><circle cx="50" cy="50" fill="none" stroke="currentColor" strokeWidth="4" r="40" stroke-dasharray="85 30"><animateTransform attributeName="transform" type="rotate" repeatCount="indefinite" dur="1s" values="0 50 50;360 50 50" keyTimes="0;1"></animateTransform></circle></svg></div><div class="wl-power"> Powered by <a href="https://github.com/walinejs/waline" target="_blank" rel="noopener noreferrer"> Waline </a> v3.1.3</div></div></div><!--[--><!----><!--]--><!--]--></main><!--]--><footer class="vp-footer-wrapper"><div class="vp-footer">RyanLee's Blog</div><div class="vp-copyright">Copyright © 2024 RyanLee_ljx </div></footer></div><!--]--><!--]--><!--[--><!----><!----><!--]--><!--]--></div>
    <script type="module" src="/assets/app-BjuHxB-x.js" defer></script>
  </body>
</html>
