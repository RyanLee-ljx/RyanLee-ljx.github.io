---
icon: qianghuaxuexi
ReadingTime: true
date: 2025-07-04
Word: true
PageView: true
category: RL
---

# Chapter 1 Basic Concepts of Reinforcement Learning

Reinforcement Learning (RL) can be described by the grid world example.

We place one agent in an environment, the goal of the agent is to find a good route to the target. Every cell/grid the agent placed can be seen as a state. Agent can take one action at each state according to a certain policy. The goal of RL is to find a good policy to guide the agent taking a sequence of acitons, travelling from the start place, moving from one state to another, and finally reach the target.

![grid world](https://github.com/RyanLee-ljx/RyanLee-ljx.github.io/blob/image/RL/C1/ba3a55ea95b2571161575dba273af8b.png?raw=true)

## Basic Concepts from the perspective of Markov decision process

Reinforcement learning utilizes the MDP framework to model the interaction between a learning agent and its environment. Actually the above example is just one simple description of the Markov decision preocess. It reflects how the agent interacts with the environment. The process directly involves three basic concepts *state*, *action*, *policy* and one transition *state transition*. It also implictly includes another concept *reward*.

We will introduce the key concepts one by one in the following.

### State

_State_ is the status of the agent with respect to the environment. Its set is the _state space_ $S = {s_i}$.

### Action

_Action_ is what the agent do at a certain state. The agent will obtain a new state after taking one aciton. Similarly, its set is the _action space of a state_ denoted as $A(s_i) = {a_i}$.

### Policy

_Policy_ denoted as $\pi$, tells the agent what actions to take at a state. It gives the probability of each action to be taken at a certain state denoted as $\pi(a_t|s_t)$. In mathematical form ,we use _tabular representation_ to display one policy. In programming, we use one array, matrix to represent a policy.


![tabular representation](https://github.com/RyanLee-ljx/RyanLee-ljx.github.io/blob/image/RL/C1/97cd3c24fba9676822a512a14a8ea73.png?raw=true)

### Reward

_Reward_ guides the agent to our target. Agent wants more reward, which means the agent will minimize (the reward is negative) or maximum (the reward is postive) the reward in the process.

**Reward depends on the current state and action not the next state**.

### Probability Distribution

Involve two probability form:

- State transition probability: at state $s$, taking action $a$, the probability to transit to state $s'$ is $p(s'|s,a)$

- Reward probability: at state $s$, taking action $a$, the probability to get reward $r$ is $p(r|s, a)$

### Markov Property

Memoryless property: The state transiting to the next depends on current state and action rather than previous.

## Other concepts

### Trajectory and Return

A _trajectory_ is a state-action-reward chain:

![chain](https://github.com/RyanLee-ljx/RyanLee-ljx.github.io/blob/image/RL/C1/77fc0d671ff46edc0f0f6c95381e7e0.png?raw=true)

The return of this trajectory is the sum of all the rewards collected along the
trajectory. It can be finite, e.g., transit from target to target $s_1 \to s_2 \to s_3 \to s_5 \to s_5 \to s_5...$.

$$ return = 0 + 0 + 0 + 1 = 1 $$

*Return* can be used to evaluate a *policy*.

### Discounted Rate

_discount rate_ $γ \in [0, 1)$

Roles: 1) the sum of the return becomes finite instead of infinite; 2) balance the far and near future rewards:

- If $γ$ is close to 0, the value of the discounted return is dominated by the
  rewards obtained in the near future.

- If $γ$ is close to 1, the value of the discounted return is dominated by the
  rewards obtained in the far future

$$ return = 0 × γ + 0 × γ^2 + 0 × γ^3 + 1 × γ^4 + 1 × γ^5 ..... = \frac{\gamma}{1-\gamma}  $$

### Episode

When interacting with the environment following a policy, the agent may stop
at some terminal states. The resulting trajectory is called an _episode_ (or a
_trial_), e.g. $s_1 \to s_2 \to s_3 \to s_5$,

An episode is usually assumed to be a finite trajectory. Tasks with episodes are
called episodic tasks.

We can treat episodic and continuing tasks in a unified mathematical way by converting episodic tasks to continuing tasks:

- Option 1: Treat the target state as a special absorbing state. Once the agent
  reaches an absorbing state, it will never leave. The consequent rewards
  r = 0.

- Option 2: Treat the target state as a normal state with a policy. The agent
  can still leave the target state and gain r = +1 when entering the target
  state.

This tutorial course considers option 2 so as to not distinguish the target state from the others and can treat it as a normal state.
