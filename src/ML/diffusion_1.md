---
icon: kuosanmoxing
ReadingTime: true
date: 2025-11-14
Word: true
PageView: true
category: ML
---


# Preliminaries

## 对数据的认识

机器学习就是对一个未知分布的数据建模的过程。无论是机器学习哪种学派，其都认为观察到的数据并不是凭空产生的，而是由一个潜在的、客观存在的**数据生成过程**所产生。这个数据生成过程可以用一个概率分布来描述。

例如抛硬币，会出现正面或反面，我们抛了$k$次，得到$k$个数据。这个结果就可以看作是由一个伯努利分布生成（采样）的。

因此，我们可以得出以下总结：

假设观测数据$x_1, x_2, ..., x_n$是从某个真实但未知的分布$P_{data}(x)$中独立采样得到的。为了表示这个数据生成过程，我们可以使用一个参数化的模型$P_{model}(x|\theta)$来近似真实分布，这里的$\theta$就是对该参数化模型的描述，一个好的$\theta$会让我们的参数化模型更好地接近真实的数据分布，因此我们的目标就是优化参数 $\theta$使模型分布尽可能接近真实数据分布。

如何优化参数$\theta$呢，这就引出了机器学习的两大学派——**频率学派**以及**贝叶斯学派。**

## 机器学习两大学派

部分内容取自：[机器学习中的频率派和贝叶斯派-以极大似然估计和最大后验估计为例 - 知乎](https://zhuanlan.zhihu.com/p/368004790)

### 频率学派

频率派认为上面的$P_{model}(x|\theta)$中的$\theta$是一个未知的但客观存在的常量，可以通过现有数据进行直接的估计。

回到上面抛硬币例子，假如$k$次实验中，有$a$次向上的结果，频率派认为，这不是偶然的，这个已发生的事件肯定反映了硬币向上的真实概率。所以我们要做的就是让出现$a$次朝上，$k-a$次朝下这种事件的概率最大，把模型中的参数$\theta$依据数据给估计出来，这样就可以还原真实世界对于抛硬币的情况了（即以后抛硬币都像这次观测的一样）。

实际上，$P_{model}(x|\theta)$对应的就是似然函数的概念，即对于某一$\theta$，出现当前数据$x_1, x_2, ..., x_n$的概率大小，上述过程可以表示为：

$$
L(\theta)=L(x_1,x_2,x_3,\ldots,x_n;\theta)=\prod_{i=1}^np(x_i;\theta)
$$

也可以写成以下式子：

$$
P(x|\theta) = \prod_{i=1}^np(x_i;\theta)
$$

这就是似然函数。似然函数反应了，假定参数$\theta$下，取得当前观测结果的概率大小（也就是当前的观测结果），那么这个概率越大，就越说明参数$\theta$的准确，也就是说模型逼近真实分布的程度越好。

所以频率学派就是**通过优化参数$\theta$使得似然函数尽可能的大**，从而使出现当前这种情况的概率最大。那么如何最大化似然函数呢？

这就引入了我们熟悉的**最大似然估计 (MLE):**

$$
\theta_{MLE}=\underset{\theta}{\arg \max }  \sum_{i=1}^{N}\log p\left(x_{i} \mid \theta\right)
$$

即把问题转换为了一个优化问题，其遵循的优化过程为：

1. **建立模型**，该模型可以是一个简单的线性回归 (那么斜率$k$和偏置$b$就是我们要估计的参数)，或是一个神经网络 (很难显式表达，网络里所有权重就是我们要估计的参数)；
2. **设计损失函数**，如上述例如常见的MSE (回归问题)，或是交叉熵 (分类问题)；
3. **优化**，例如梯度下降，牛顿法等。

::: tip

**为什么常见的损失函数MSE和交叉熵等同于上述的MLE的表达？**

**交叉熵：**[人工智能概率统计基础——似然函数及其与的交叉熵关系 - z_s_s - 博客园](https://www.cnblogs.com/zhoushusheng/p/18563410)

**MSE：**[在回归问题中，为何对MSE损失的最小化等效于最大似然估计？ - 知乎](https://www.zhihu.com/question/426901520)

:::

### 贝叶斯学派

贝叶斯学派对参数$\theta$的看法与频率派完全不同。贝叶斯派认为$\theta$不是一个未知的固定常量，而是一个**随机变量**。这意味着$\theta$本身是**不确定**的，它有自己的概率分布，这个概率分布就是**先验**。

因此，贝叶斯学派的核心思想不是去估计一个唯一的、真实的$\theta$值，而是要根据观测到的数据+先验知识，来**更新我们对**$\theta$**不确定性的认知**。

回到抛硬币的例子，贝叶斯派在抛硬币之前，就会对硬币朝上的概率$\theta$有一个**先验（信念）**，用一个概率分布$P(\theta)$来表示，例如：

1. 如果我们认为这很可能是一枚均匀的硬币，我们可能会用一个在0.5处有高概率、两端概率低的Beta分布来作为先验。
2. 如果我们对硬币一无所知，我们可能会使用均匀分布$U(0, 1)$作为先验，即认为$\theta$取[0, 1]中任意值的可能性都一样（即无信息先验）。

然后，我们抛$k$次，出现$a$次朝上的观测数据，这可以用前面提到的**似然函数**$P( x| \theta)$来表示。

所以，我们可以用$P(x | \theta) P(\theta)$来表示发生当前观测数据的情况。

由于$\theta$可能有许多不同的取值，对应每种取值都有可能发生当前观测数据$x$的情况，那么对应所有不同$\theta$下，发生观测$x$的情况可以表示为：$p(x) = \int P(x | \theta) P(\theta) d\theta$。

因此，发生当前观测，对应某一$\theta$的概率大小就可以表示为：

$$
\frac{P(x | \theta) P(\theta)}{P(x)}=\frac{P(x | \theta) P(\theta)}{\int P(x | \theta) P(\theta) d\theta}
$$

这其实就是当前情况的一种归一化。

我们把这种对于当前情况的描述称为**后验概率**，即:

$$
P(\theta | x) = \frac{P(x | \theta) P(\theta)}{P(x)}=\frac{P(x | \theta) P(\theta)}{\int P(x | \theta) P(\theta) d\theta}
$$

这其实也就是我们常见的**贝叶斯公式**。那么贝叶斯派就是要真正掌握这样一个后验分布，也就是说，给出当前观测，对应的$\theta$不同取值的概率大小，或者说有多大概率是当前$\theta$产生的当前观测。

OK，这里我们暂停一下，回去与频率派对比下，你会发现，频率派就是在最大化上面的似然，因为频率派认为$\theta$是一个固定值，他不存在其他情况，所以可以直接用似然表示，但贝叶斯派认为，$\theta$可能有很多取值，都有可能导致现在情况发生，因此写出了这样一种归一化的形式，要找到一种普世的，“”掌控全局“”的表达。

总结一下上面的表达式各项含义：

- $P(\theta | D)$是**后验概率**：在观测到数据$x$之后，我们对$\theta$的信念分布。这是贝叶斯推断的核心目标。
- $P(D | \theta)$ 是**似然**：这与频率派中的似然函数$L(\theta)$是同一个东西，就是当前的观测结果。
- $P(\theta)$是**先验**：在观测数据$x$之前，我们对$\theta$的初始信念。
- $P(x)$ 是一个归一化常数，也叫做证据或边际似然，它确保后验概率分布$P(\theta | x)$对所有 $\theta$积分后等于1。

贝叶斯学派的重心在于求解完整的后验分布$P(\theta | x)$。这个分布包含了给定数据和先验信念下，关于$\theta$的所有信息。但在实际应用中，计算$P(x)$的积分（尤其是在$\theta$高维时，如神经网络）通常是极其困难的。
因此，人们提出了一种简化的方法，即只寻找后验分布$P(\theta | x)$中概率密度最大的那个点 $\theta$，以此作为$\theta$的一个代表性估计。这就是**最大后验估计 (MAP)**：

$$
\theta_{MAP} = \underset{\theta}{\arg \max } P(\theta | x)
$$

由于$P(x)$是一个与$\theta$无关的常数，在优化时可以被忽略：

$$
\theta_{MAP} = \underset{\theta}{\arg \max } \frac{P(x | \theta) P(\theta)}{P(x)} = \underset{\theta}{\arg \max } P(x | \theta) P(\theta)
$$

为了计算方便，我们同样可以对其取对数

$$
\theta_{MAP} = \underset{\theta}{\arg \max } \left( \log P(x| \theta) + \log P(\theta) \right)

$$

如果我们假设数据是独立同分布的，那么似然$P(x | \theta)$就可以像MLE一样被展开：

$$
\theta_{MAP} = \underset{\theta}{\arg \max } \left( \sum_{i=1}^{N} \log p(x_i | \theta) + \log P(\theta) \right)
$$

通过比较$\theta_{MAP}$和$\theta_{MLE}$的公式：
我们可以得出一个非常非常重要的结论：
**最大后验估计 (MAP) = 最大似然估计 (MLE) + 先验信念** $\log P(\theta)$

MAP的优化过程如下：

1. 建立模型。
2. 设计损失函数。在MAP中，损失函数不仅仅是似然（如MSE或交叉熵），还额外包含了一个先验项$\log P(\theta)$。
3. 优化（同MLE）。

设计损失函数。在MAP中，损失函数不仅仅是似然（如MSE或交叉熵），还额外包含了一个先验项$\log P(\theta)$。这个$\log P(\theta)$项通常充当了**正则化项 (Regularizer)**的角色，它会惩罚那些与先验信念偏差太大的$\theta$值。那么常见的正则化方式就是我们所说的**L1/L2正则化**。

::: tip

**从MAP视角解读正则化技术：**
以L2正则化为例，它在损失函数中增加了一项

$$
 \lambda ||\theta||^2
$$

这里$\theta$就是神经网络的参数，过去我们直观理解为什么L2正则化可以防止过拟合，因为它使得网络参数趋向于0，类似于dropout随机抑制某些神经元的输出，从而减小模型复杂度，从而减小过拟合风险。

实际上，这里的L2正则化表达等价于假设参数$\theta$的先验分布$P(\theta)$是一个均值为一个均值为0、方差为$\sigma_p^2$的高斯分布。

这个先验$P(\theta)$的概率密度函数可以写作：

$$
P(\theta) = \prod_{i=1}^{d} \frac{1}{\sqrt{2\pi\sigma_p^2}} \exp\left(-\frac{\theta_i^2}{2\sigma_p^2}\right) = C \cdot \exp\left(-\frac{||\theta||^2}{2\sigma_p^2}\right)
$$

（其中 $d$ 是参数 $\theta$ 的维度，$C$ 是一个不依赖于 $\theta$ 的归一化常数）

根据我们在贝叶斯学派笔记中推导的**最大后验估计 (MAP)** 公式，我们优化的目标是：

$$
\theta_{MAP} = \underset{\theta}{\arg \max } \left( \log P(x | \theta) + \log P(\theta) \right)
$$

现在，我们把高斯先验的对数 $\log P(\theta)$ 计算出来：

$$
\log P(\theta) = \log \left( C \cdot \exp\left(-\frac{||\theta||^2}{2\sigma_p^2}\right) \right) = \log C - \frac{1}{2\sigma_p^2}||\theta||^2
$$

将它代回MAP的优化目标中。由于 $\log C$ 是一个常数，它不影响最优化时 $\theta$ 的取值，所以可以被忽略：

$$
\theta_{MAP} = \underset{\theta}{\arg \max } \left( \log P(x | \theta) - \frac{1}{2\sigma_p^2}||\theta||^2 \right)
$$

上述表达中，最大化MAP就是最小化 $\frac{1}{2\sigma_p^2} ||\theta||^2$，对应了机器学习中最小化损失函数。

即：

$$
\theta_{MAP} = \underset{\theta}{\arg \min } \left( \text{Loss} + \frac{1}{2\sigma_p^2}||\theta||^2 \right)
$$


定义 $\lambda = \frac{1}{2\sigma_p^2}$，这个公式就变成了我们熟悉的L2正则化的形式：

$$\theta_{MAP} = \underset{\theta}{\arg \min } \left( \text{Loss} + \lambda ||\theta||^2 \right)$$

所以，L2正则化**在数学上就等价于**假设参数$\theta$满足**均值为0的高斯先验**的MAP估计，先当于向网络注入了一定先验知识，帮助网络向某个方向更新收敛。

:::

好，现在我们已经明白了机器学习中两大学派的基本观点，尤其是贝叶斯学派的基本思想，下一节，我们将要介绍贝叶斯预测及变分推断，进而引出VAE和Diffsuion model。