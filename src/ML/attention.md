---
icon: fangchahexiefangchafenxi-xuanzhong
ReadingTime: true
date: 2025-1-25
Word: true
PageView: true
category: ML
---

# Attention Mechanism

This article will introduce a powerful technique in machine learning called *Ateention Mechanism*. 

The core method of *attention mechanism* is to pay more attention to what we want. It allows model to weigh the importance of different parts of input dynamically rather than treating them equally. The model learns to assign higher weights to the most relevant elements.

Before stepping into the main text, we should first know some preliminary knowledge.

## preliminaries

1. RNNs and LSTM network

The content introduced below is mainly from this three blog/article.

\[1\] [Recurrent neural network From Wikipedia](https://en.wikipedia.org/wiki/Recurrent_neural_network)

\[2\] [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

\[3\] [如何从RNN起步，一步一步通俗理解LSTM](https://blog.csdn.net/v_JULY_v/article/details/89894058?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169296729516800211518875%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169296729516800211518875&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-89894058-null-null.142%5Ev93%5EchatgptT3_2&utm_term=RNN&spm=1018.2226.3001.4187)

RNNs, short for Recurrent Neural Networks, is a class of artificial nerural network used to process sequencial data. Unlike FFN(Forwardfeed Neural Network), RNNs process data across multiple times rather than in a single time, making them well-adapted for modelling and processing text, speech, and time series.

The following picture demonstrates the working flow of RNNs.

![An unrolled recurrent neural network from [2]]()

2. Encoder and Decoder



## Basic Components



## Working Flow







