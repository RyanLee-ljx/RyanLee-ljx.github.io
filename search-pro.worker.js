const V=Object.entries,et=Object.fromEntries,st="ENTRIES",L="KEYS",T="VALUES",_="";class D{set;_type;_path;constructor(t,s){const n=t._tree,o=Array.from(n.keys());this.set=t,this._type=s,this._path=o.length>0?[{node:n,keys:o}]:[]}next(){const t=this.dive();return this.backtrack(),t}dive(){if(this._path.length===0)return{done:!0,value:void 0};const{node:t,keys:s}=E(this._path);if(E(s)===_)return{done:!1,value:this.result()};const n=t.get(E(s));return this._path.push({node:n,keys:Array.from(n.keys())}),this.dive()}backtrack(){if(this._path.length===0)return;const t=E(this._path).keys;t.pop(),!(t.length>0)&&(this._path.pop(),this.backtrack())}key(){return this.set._prefix+this._path.map(({keys:t})=>E(t)).filter(t=>t!==_).join("")}value(){return E(this._path).node.get(_)}result(){switch(this._type){case T:return this.value();case L:return this.key();default:return[this.key(),this.value()]}}[Symbol.iterator](){return this}}const E=e=>e[e.length-1],nt=(e,t,s)=>{const n=new Map;if(t===void 0)return n;const o=t.length+1,u=o+s,i=new Uint8Array(u*o).fill(s+1);for(let r=0;r<o;++r)i[r]=r;for(let r=1;r<u;++r)i[r*o]=r;return R(e,t,s,n,i,1,o,""),n},R=(e,t,s,n,o,u,i,r)=>{const d=u*i;t:for(const c of e.keys())if(c===_){const a=o[d-1];a<=s&&n.set(r,[e.get(c),a])}else{let a=u;for(let h=0;h<c.length;++h,++a){const g=c[h],m=i*a,p=m-i;let l=o[m];const f=Math.max(0,a-s-1),y=Math.min(i-1,a+s);for(let F=f;F<y;++F){const v=g!==t[F],z=o[p+F]+ +v,A=o[p+F+1]+1,w=o[m+F]+1,j=o[m+F+1]=Math.min(z,A,w);j<l&&(l=j)}if(l>s)continue t}R(e.get(c),t,s,n,o,a,i,r+c)}};class C{_tree;_prefix;_size=void 0;constructor(t=new Map,s=""){this._tree=t,this._prefix=s}atPrefix(t){if(!t.startsWith(this._prefix))throw new Error("Mismatched prefix");const[s,n]=x(this._tree,t.slice(this._prefix.length));if(s===void 0){const[o,u]=O(n);for(const i of o.keys())if(i!==_&&i.startsWith(u)){const r=new Map;return r.set(i.slice(u.length),o.get(i)),new C(r,t)}}return new C(s,t)}clear(){this._size=void 0,this._tree.clear()}delete(t){return this._size=void 0,ot(this._tree,t)}entries(){return new D(this,st)}forEach(t){for(const[s,n]of this)t(s,n,this)}fuzzyGet(t,s){return nt(this._tree,t,s)}get(t){const s=k(this._tree,t);return s!==void 0?s.get(_):void 0}has(t){const s=k(this._tree,t);return s!==void 0&&s.has(_)}keys(){return new D(this,L)}set(t,s){if(typeof t!="string")throw new Error("key must be a string");return this._size=void 0,I(this._tree,t).set(_,s),this}get size(){if(this._size)return this._size;this._size=0;const t=this.entries();for(;!t.next().done;)this._size+=1;return this._size}update(t,s){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const n=I(this._tree,t);return n.set(_,s(n.get(_))),this}fetch(t,s){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const n=I(this._tree,t);let o=n.get(_);return o===void 0&&n.set(_,o=s()),o}values(){return new D(this,T)}[Symbol.iterator](){return this.entries()}static from(t){const s=new C;for(const[n,o]of t)s.set(n,o);return s}static fromObject(t){return C.from(Object.entries(t))}}const x=(e,t,s=[])=>{if(t.length===0||e==null)return[e,s];for(const n of e.keys())if(n!==_&&t.startsWith(n))return s.push([e,n]),x(e.get(n),t.slice(n.length),s);return s.push([e,t]),x(void 0,"",s)},k=(e,t)=>{if(t.length===0||e==null)return e;for(const s of e.keys())if(s!==_&&t.startsWith(s))return k(e.get(s),t.slice(s.length))},I=(e,t)=>{const s=t.length;t:for(let n=0;e&&n<s;){for(const u of e.keys())if(u!==_&&t[n]===u[0]){const i=Math.min(s-n,u.length);let r=1;for(;r<i&&t[n+r]===u[r];)++r;const d=e.get(u);if(r===u.length)e=d;else{const c=new Map;c.set(u.slice(r),d),e.set(t.slice(n,n+r),c),e.delete(u),e=c}n+=r;continue t}const o=new Map;return e.set(t.slice(n),o),o}return e},ot=(e,t)=>{const[s,n]=x(e,t);if(s!==void 0){if(s.delete(_),s.size===0)W(n);else if(s.size===1){const[o,u]=s.entries().next().value;q(n,o,u)}}},W=e=>{if(e.length===0)return;const[t,s]=O(e);if(t.delete(s),t.size===0)W(e.slice(0,-1));else if(t.size===1){const[n,o]=t.entries().next().value;n!==_&&q(e.slice(0,-1),n,o)}},q=(e,t,s)=>{if(e.length===0)return;const[n,o]=O(e);n.set(o+t,s),n.delete(o)},O=e=>e[e.length-1],ut=(e,t)=>{const s=e._idToShortId.get(t);if(s!=null)return e._storedFields.get(s)},it=/[\n\r -#%-*,-/:;?@[-\]_{}\u00A0\u00A1\u00A7\u00AB\u00B6\u00B7\u00BB\u00BF\u037E\u0387\u055A-\u055F\u0589\u058A\u05BE\u05C0\u05C3\u05C6\u05F3\u05F4\u0609\u060A\u060C\u060D\u061B\u061E\u061F\u066A-\u066D\u06D4\u0700-\u070D\u07F7-\u07F9\u0830-\u083E\u085E\u0964\u0965\u0970\u09FD\u0A76\u0AF0\u0C77\u0C84\u0DF4\u0E4F\u0E5A\u0E5B\u0F04-\u0F12\u0F14\u0F3A-\u0F3D\u0F85\u0FD0-\u0FD4\u0FD9\u0FDA\u104A-\u104F\u10FB\u1360-\u1368\u1400\u166E\u1680\u169B\u169C\u16EB-\u16ED\u1735\u1736\u17D4-\u17D6\u17D8-\u17DA\u1800-\u180A\u1944\u1945\u1A1E\u1A1F\u1AA0-\u1AA6\u1AA8-\u1AAD\u1B5A-\u1B60\u1BFC-\u1BFF\u1C3B-\u1C3F\u1C7E\u1C7F\u1CC0-\u1CC7\u1CD3\u2000-\u200A\u2010-\u2029\u202F-\u2043\u2045-\u2051\u2053-\u205F\u207D\u207E\u208D\u208E\u2308-\u230B\u2329\u232A\u2768-\u2775\u27C5\u27C6\u27E6-\u27EF\u2983-\u2998\u29D8-\u29DB\u29FC\u29FD\u2CF9-\u2CFC\u2CFE\u2CFF\u2D70\u2E00-\u2E2E\u2E30-\u2E4F\u3000-\u3003\u3008-\u3011\u3014-\u301F\u3030\u303D\u30A0\u30FB\uA4FE\uA4FF\uA60D-\uA60F\uA673\uA67E\uA6F2-\uA6F7\uA874-\uA877\uA8CE\uA8CF\uA8F8-\uA8FA\uA8FC\uA92E\uA92F\uA95F\uA9C1-\uA9CD\uA9DE\uA9DF\uAA5C-\uAA5F\uAADE\uAADF\uAAF0\uAAF1\uABEB\uFD3E\uFD3F\uFE10-\uFE19\uFE30-\uFE52\uFE54-\uFE61\uFE63\uFE68\uFE6A\uFE6B\uFF01-\uFF03\uFF05-\uFF0A\uFF0C-\uFF0F\uFF1A\uFF1B\uFF1F\uFF20\uFF3B-\uFF3D\uFF3F\uFF5B\uFF5D\uFF5F-\uFF65]+/u,M="or",$="and",rt="and_not",ct=(e,t)=>{e.includes(t)||e.push(t)},N=(e,t)=>{for(const s of t)e.includes(s)||e.push(s)},P=({score:e},{score:t})=>t-e,lt=()=>new Map,b=e=>{const t=new Map;for(const s of Object.keys(e))t.set(parseInt(s,10),e[s]);return t},G=(e,t)=>Object.prototype.hasOwnProperty.call(e,t)?e[t]:void 0,ht={[M]:(e,t)=>{for(const s of t.keys()){const n=e.get(s);if(n==null)e.set(s,t.get(s));else{const{score:o,terms:u,match:i}=t.get(s);n.score=n.score+o,n.match=Object.assign(n.match,i),N(n.terms,u)}}return e},[$]:(e,t)=>{const s=new Map;for(const n of t.keys()){const o=e.get(n);if(o==null)continue;const{score:u,terms:i,match:r}=t.get(n);N(o.terms,i),s.set(n,{score:o.score+u,terms:o.terms,match:Object.assign(o.match,r)})}return s},[rt]:(e,t)=>{for(const s of t.keys())e.delete(s);return e}},dt=(e,t,s,n,o,u)=>{const{k:i,b:r,d}=u;return Math.log(1+(s-t+.5)/(t+.5))*(d+e*(i+1)/(e+i*(1-r+r*n/o)))},at=e=>(t,s,n)=>{const o=typeof e.fuzzy=="function"?e.fuzzy(t,s,n):e.fuzzy||!1,u=typeof e.prefix=="function"?e.prefix(t,s,n):e.prefix===!0;return{term:t,fuzzy:o,prefix:u}},H=(e,t,s,n)=>{for(const o of Object.keys(e._fieldIds))if(e._fieldIds[o]===s){e._options.logger("warn",`SlimSearch: document with ID ${e._documentIds.get(t)} has changed before removal: term "${n}" was not present in field "${o}". Removing a document after it has changed can corrupt the index!`,"version_conflict");return}},ft=(e,t,s,n)=>{if(!e._index.has(n)){H(e,s,t,n);return}const o=e._index.fetch(n,lt),u=o.get(t);u==null||u.get(s)==null?H(e,s,t,n):u.get(s)<=1?u.size<=1?o.delete(t):u.delete(s):u.set(s,u.get(s)-1),e._index.get(n).size===0&&e._index.delete(n)},gt={k:1.2,b:.7,d:.5},mt={idField:"id",extractField:(e,t)=>e[t],tokenize:e=>e.split(it),processTerm:e=>e.toLowerCase(),fields:void 0,searchOptions:void 0,storeFields:[],logger:(e,t)=>{typeof console?.[e]=="function"&&console[e](t)},autoVacuum:!0},J={combineWith:M,prefix:!1,fuzzy:!1,maxFuzzy:6,boost:{},weights:{fuzzy:.45,prefix:.375},bm25:gt},pt={combineWith:$,prefix:(e,t,s)=>t===s.length-1},Ft={batchSize:1e3,batchWait:10},U={minDirtFactor:.1,minDirtCount:20},_t={...Ft,...U},K=Symbol("*"),yt=(e,t)=>{const s=new Map,n={...e._options.searchOptions,...t};for(const[o,u]of e._documentIds){const i=n.boostDocument?n.boostDocument(u,"",e._storedFields.get(o)):1;s.set(o,{score:i,terms:[],match:{}})}return s},X=(e,t=M)=>{if(e.length===0)return new Map;const s=t.toLowerCase(),n=ht[s];if(!n)throw new Error(`Invalid combination operator: ${t}`);return e.reduce(n)||new Map},S=(e,t,s,n,o,u,i,r,d=new Map)=>{if(o==null)return d;for(const c of Object.keys(u)){const a=u[c],h=e._fieldIds[c],g=o.get(h);if(g==null)continue;let m=g.size;const p=e._avgFieldLength[h];for(const l of g.keys()){if(!e._documentIds.has(l)){ft(e,h,l,s),m-=1;continue}const f=i?i(e._documentIds.get(l),s,e._storedFields.get(l)):1;if(!f)continue;const y=g.get(l),F=e._fieldLength.get(l)[h],v=dt(y,m,e._documentCount,F,p,r),z=n*a*f*v,A=d.get(l);if(A){A.score+=z,ct(A.terms,t);const w=G(A.match,s);w?w.push(c):A.match[s]=[c]}else d.set(l,{score:z,terms:[t],match:{[s]:[c]}})}}return d},At=(e,t,s)=>{const n={...e._options.searchOptions,...s},o=(n.fields||e._options.fields).reduce((l,f)=>({...l,[f]:G(n.boost,f)||1}),{}),{boostDocument:u,weights:i,maxFuzzy:r,bm25:d}=n,{fuzzy:c,prefix:a}={...J.weights,...i},h=e._index.get(t.term),g=S(e,t.term,t.term,1,h,o,u,d);let m,p;if(t.prefix&&(m=e._index.atPrefix(t.term)),t.fuzzy){const l=t.fuzzy===!0?.2:t.fuzzy,f=l<1?Math.min(r,Math.round(t.term.length*l)):l;f&&(p=e._index.fuzzyGet(t.term,f))}if(m)for(const[l,f]of m){const y=l.length-t.term.length;if(!y)continue;p?.delete(l);const F=a*l.length/(l.length+.3*y);S(e,t.term,l,F,f,o,u,d,g)}if(p)for(const l of p.keys()){const[f,y]=p.get(l);if(!y)continue;const F=c*l.length/(l.length+y);S(e,t.term,l,F,f,o,u,d,g)}return g},Y=(e,t,s={})=>{if(t===K)return yt(e,s);if(typeof t!="string"){const a={...s,...t,queries:void 0},h=t.queries.map(g=>Y(e,g,a));return X(h,a.combineWith)}const{tokenize:n,processTerm:o,searchOptions:u}=e._options,i={tokenize:n,processTerm:o,...u,...s},{tokenize:r,processTerm:d}=i,c=r(t).flatMap(a=>d(a)).filter(a=>!!a).map(at(i)).map(a=>At(e,a,i));return X(c,i.combineWith)},Q=(e,t,s={})=>{const n=Y(e,t,s),o=[];for(const[u,{score:i,terms:r,match:d}]of n){const c=r.length||1,a={id:e._documentIds.get(u),score:i*c,terms:Object.keys(d),queryTerms:r,match:d};Object.assign(a,e._storedFields.get(u)),(s.filter==null||s.filter(a))&&o.push(a)}return t===K&&s.boostDocument==null&&e._options.searchOptions.boostDocument==null||o.sort(P),o},Ct=(e,t,s={})=>{s={...e._options.autoSuggestOptions,...s};const n=new Map;for(const{score:u,terms:i}of Q(e,t,s)){const r=i.join(" "),d=n.get(r);d!=null?(d.score+=u,d.count+=1):n.set(r,{score:u,terms:i,count:1})}const o=[];for(const[u,{score:i,terms:r,count:d}]of n)o.push({suggestion:u,terms:r,score:i/d});return o.sort(P),o};class Et{_options;_index;_documentCount;_documentIds;_idToShortId;_fieldIds;_fieldLength;_avgFieldLength;_nextId;_storedFields;_dirtCount;_currentVacuum;_enqueuedVacuum;_enqueuedVacuumConditions;constructor(t){if(t?.fields==null)throw new Error('SlimSearch: option "fields" must be provided');const s=t.autoVacuum==null||t.autoVacuum===!0?_t:t.autoVacuum;this._options={...mt,...t,autoVacuum:s,searchOptions:{...J,...t.searchOptions||{}},autoSuggestOptions:{...pt,...t.autoSuggestOptions||{}}},this._index=new C,this._documentCount=0,this._documentIds=new Map,this._idToShortId=new Map,this._fieldIds={},this._fieldLength=new Map,this._avgFieldLength=[],this._nextId=0,this._storedFields=new Map,this._dirtCount=0,this._currentVacuum=null,this._enqueuedVacuum=null,this._enqueuedVacuumConditions=U,this.addFields(this._options.fields)}get isVacuuming(){return this._currentVacuum!=null}get dirtCount(){return this._dirtCount}get dirtFactor(){return this._dirtCount/(1+this._documentCount+this._dirtCount)}get documentCount(){return this._documentCount}get termCount(){return this._index.size}toJSON(){const t=[];for(const[s,n]of this._index){const o={};for(const[u,i]of n)o[u]=Object.fromEntries(i);t.push([s,o])}return{documentCount:this._documentCount,nextId:this._nextId,documentIds:Object.fromEntries(this._documentIds),fieldIds:this._fieldIds,fieldLength:Object.fromEntries(this._fieldLength),averageFieldLength:this._avgFieldLength,storedFields:Object.fromEntries(this._storedFields),dirtCount:this._dirtCount,index:t,serializationVersion:2}}addFields(t){for(let s=0;s<t.length;s++)this._fieldIds[t[s]]=s}}const zt=({index:e,documentCount:t,nextId:s,documentIds:n,fieldIds:o,fieldLength:u,averageFieldLength:i,storedFields:r,dirtCount:d,serializationVersion:c},a)=>{if(c!==1&&c!==2)throw new Error("SlimSearch: cannot deserialize an index created with an incompatible version");const h=new Et(a);h._documentCount=t,h._nextId=s,h._documentIds=b(n),h._idToShortId=new Map,h._fieldIds=o,h._fieldLength=b(u),h._avgFieldLength=i,h._storedFields=b(r),h._dirtCount=d||0,h._index=new C;for(const[g,m]of h._documentIds)h._idToShortId.set(m,g);for(const[g,m]of e){const p=new Map;for(const l of Object.keys(m)){let f=m[l];c===1&&(f=f.ds),p.set(parseInt(l,10),b(f))}h._index.set(g,p)}return h},B=(e,t)=>{const s=e.toLowerCase(),n=t.toLowerCase(),o=[];let u=0,i=0;const r=(c,a=!1)=>{let h="";i===0?h=c.length>20?`… ${c.slice(-20)}`:c:a?h=c.length+i>100?`${c.slice(0,100-i)}… `:c:h=c.length>20?`${c.slice(0,20)} … ${c.slice(-20)}`:c,h&&o.push(h),i+=h.length,a||(o.push(["mark",t]),i+=t.length,i>=100&&o.push(" …"))};let d=s.indexOf(n,u);if(d===-1)return null;for(;d>=0;){const c=d+n.length;if(r(e.slice(u,d)),u=c,i>100)break;d=s.indexOf(n,u)}return i<100&&r(e.slice(u),!0),o},wt=(e,t)=>t.contents.reduce((s,[,n])=>s+n,0)-e.contents.reduce((s,[,n])=>s+n,0),xt=(e,t)=>Math.max(...t.contents.map(([,s])=>s))-Math.max(...e.contents.map(([,s])=>s)),Z=(e,t,s={})=>{const n={};return Q(t,e,{boost:{h:2,t:1,c:4},prefix:!0,...s}).forEach(o=>{const{id:u,terms:i,score:r}=o,d=u.includes("@"),c=u.includes("#"),[a,h]=u.split(/[#@]/),g=Number(a),m=i.sort((l,f)=>l.length-f.length).filter((l,f)=>i.slice(f+1).every(y=>!y.includes(l))),{contents:p}=n[g]??={title:"",contents:[]};if(d)p.push([{type:"customField",id:g,index:h,display:m.map(l=>o.c.map(f=>B(f,l))).flat().filter(l=>l!==null)},r]);else{const l=m.map(f=>B(o.h,f)).filter(f=>f!==null);if(l.length&&p.push([{type:c?"heading":"title",id:g,...c&&{anchor:h},display:l},r]),"t"in o)for(const f of o.t){const y=m.map(F=>B(f,F)).filter(F=>F!==null);y.length&&p.push([{type:"text",id:g,...c&&{anchor:h},display:y},r])}}}),V(n).sort(([,o],[,u])=>"max"==="total"?wt(o,u):xt(o,u)).map(([o,{title:u,contents:i}])=>{if(!u){const r=ut(t,o);r&&(u=r.h)}return{title:u,contents:i.map(([r])=>r)}})},tt=(e,t,s={})=>{const n=Ct(t,e,{fuzzy:.2,maxFuzzy:3,...s}).map(({suggestion:o})=>o);return e.includes(" ")?n:n.filter(o=>!o.includes(" "))},bt=et(V(JSON.parse("{\"/\":{\"documentCount\":240,\"nextId\":240,\"documentIds\":{\"0\":\"1\",\"1\":\"1#写在前面-prologue\",\"2\":\"1#缘由-reason\",\"3\":\"1#我-myself\",\"4\":\"1#指南-navigation\",\"5\":\"2\",\"6\":\"2@0\",\"7\":\"3\",\"8\":\"3#basic-concepts-from-the-perspective-of-markov-decision-process\",\"9\":\"3#state\",\"10\":\"3#action\",\"11\":\"3#policy\",\"12\":\"3#reward\",\"13\":\"3#probability-distribution\",\"14\":\"3#markov-property\",\"15\":\"3#other-concepts\",\"16\":\"3#trajectory-and-return\",\"17\":\"3#discounted-rate\",\"18\":\"3#episode\",\"19\":\"3@0\",\"20\":\"4\",\"21\":\"4#revision\",\"22\":\"4#state-value\",\"23\":\"4#bellman-equation\",\"24\":\"4#ways-to-solve-state-value\",\"25\":\"4#action-value\",\"26\":\"4@0\",\"27\":\"5\",\"28\":\"5#optimal-policy\",\"29\":\"5#bellman-optimality-equation-boe\",\"30\":\"5#motivating-examples\",\"31\":\"5#solve-the-bellman-optimality-equation\",\"32\":\"5#preliminaries\",\"33\":\"5#contraction-property-of-boe\",\"34\":\"5#iterative-algorithm\",\"35\":\"5#factors-determine-the-optimal-policy\",\"36\":\"5@0\",\"37\":\"6\",\"38\":\"6#value-iteration\",\"39\":\"6#policy-iteration\",\"40\":\"6#differences-between-value-iteration-and-policy-iteration\",\"41\":\"6#truncated-policy-iteration-algorithm\",\"42\":\"6@0\",\"43\":\"7\",\"44\":\"7#monte-carlo-mc-basic-algorithm\",\"45\":\"7#monte-carlo-mc-exploring-starts-algorithm\",\"46\":\"7#monte-carlo-mc-greedy-algorithm\",\"47\":\"7@0\",\"48\":\"8\",\"49\":\"8#preliminaries\",\"50\":\"8#basic-components\",\"51\":\"8#key-components-of-attention\",\"52\":\"8#query-key-and-value\",\"53\":\"8#attention-score\",\"54\":\"8#softmax\",\"55\":\"8#weighted-sum\",\"56\":\"8#working-flow\",\"57\":\"8#encoding-phase\",\"58\":\"8#attention-calculation\",\"59\":\"8#decoding-phase\",\"60\":\"8#summary\",\"61\":\"8#example-code\",\"62\":\"8@0\",\"63\":\"9\",\"64\":\"9#layout-slidesidebar-falsebreadcrumb-falsepageinfo-false\",\"65\":\"9#target\",\"66\":\"9#method\",\"67\":\"9#approach-and-proof\",\"68\":\"9#control-variate-in-imtsp\",\"69\":\"9#problem-formulation\",\"70\":\"9#gradient-estimation\",\"71\":\"9#challenge-1-non-differentiability\",\"72\":\"9#proof-of-gradient-interchange\",\"73\":\"9#variance-reduction\",\"74\":\"9#challenge-2-high-variance\",\"75\":\"9#surrogate-network-design\",\"76\":\"9@0\",\"77\":\"10\",\"78\":\"10#基本概念\",\"79\":\"10@0\",\"80\":\"11\",\"81\":\"11#_1-torch-optim-————-scheduler\",\"82\":\"11#_2-torch-autograd-grad\",\"83\":\"11#learning-rate\",\"84\":\"11#manually-optimize-parameters\",\"85\":\"11@0\",\"86\":\"12\",\"87\":\"12#graph\",\"88\":\"12#task\",\"89\":\"12#message-passing\",\"90\":\"12#key-gnn-architectures\",\"91\":\"12#gcn\",\"92\":\"12#key-components\",\"93\":\"12#layer-wise-propagation-rule\",\"94\":\"12#gat\",\"95\":\"12#key-components-1\",\"96\":\"12#attention-mechanism\",\"97\":\"12#normalized-attention-coefficients\",\"98\":\"12#feature-aggregation\",\"99\":\"12#graphsage\",\"100\":\"12#key-components-2\",\"101\":\"12#layer-wise-propagation-rule-1\",\"102\":\"12#output-embedding\",\"103\":\"12#summary\",\"104\":\"12#code\",\"105\":\"12@0\",\"106\":\"13\",\"107\":\"13@0\",\"108\":\"14\",\"109\":\"14#基本概念\",\"110\":\"14#一、绪论\",\"111\":\"14#二、模型评估与选择\",\"112\":\"14#三、线型模型\",\"113\":\"14#四、决策树\",\"114\":\"14#五、神经网络\",\"115\":\"14#六、支持向量机-svm\",\"116\":\"14#七、聚类-cluster\",\"117\":\"14#八、降维\",\"118\":\"14#further-reading-questions-in-english\",\"119\":\"14#_1-what-is-regularization\",\"120\":\"14#_2-what-is-cross-validation\",\"121\":\"14#_3-pca-principle\",\"122\":\"14#_4-k-means-principle\",\"123\":\"14#_5-support-vector-machine-principle\",\"124\":\"14#_6-decision-tree-principle\",\"125\":\"14@0\",\"126\":\"15\",\"127\":\"15#定义-definition\",\"128\":\"15#元胞-cell\",\"129\":\"15#元胞空间-space\",\"130\":\"15#元胞邻居-neighbour\",\"131\":\"15#边界条件-boundary\",\"132\":\"15#元胞规则-rule\",\"133\":\"15#生命游戏-the-game-of-life\",\"134\":\"15#应用-application\",\"135\":\"15@0\",\"136\":\"16\",\"137\":\"16#_1-基本概念\",\"138\":\"16#_1-1-nagel–schreckenberg-model-ns模型\",\"139\":\"16#_1-2-rule-184\",\"140\":\"16#_1-3-phantom-traffic-jam-幽灵拥堵\",\"141\":\"16#_1-4-基本图-fundamental-diagram\",\"142\":\"16#_2-model-description-模型描述\",\"143\":\"16#_2-1-model-information-模型说明\",\"144\":\"16#_2-2-model-step-更新规则\",\"145\":\"16#_3-改进ns模型-ns-model-for-inhomogenous-traffic-flow-in-a-single-lane\",\"146\":\"16#_3-1-改进点\",\"147\":\"16#_3-2-模型信息\",\"148\":\"16#_3-3-结果\",\"149\":\"16#_3-4-代码\",\"150\":\"16#_3-5-进一步的改进点\",\"151\":\"16@0\",\"152\":\"16@1\",\"153\":\"17\",\"154\":\"17#概述\",\"155\":\"17#问题描述\",\"156\":\"17#模型设置\",\"157\":\"17#基本设置\",\"158\":\"17#更新规则\",\"159\":\"17#部分代码解释\",\"160\":\"17#参数设置\",\"161\":\"17#参数计算\",\"162\":\"17#结果\",\"163\":\"17@0\",\"164\":\"17@1\",\"165\":\"18\",\"166\":\"18#大语言模型-foudation-model\",\"167\":\"18#sora-模型对于交通行业影响\",\"168\":\"19\",\"169\":\"20\",\"170\":\"20@0\",\"171\":\"21\",\"172\":\"21#五线谱\",\"173\":\"21#简谱\",\"174\":\"21#六线谱\",\"175\":\"21@0\",\"176\":\"22\",\"177\":\"22#拨片\",\"178\":\"22#五种音阶\",\"179\":\"22@0\",\"180\":\"23\",\"181\":\"23#音符\",\"182\":\"23#休止符\",\"183\":\"23@0\",\"184\":\"24\",\"185\":\"24#abstract\",\"186\":\"24#key-points-in-this-paper\",\"187\":\"24#problem-statement\",\"188\":\"24#_1-workspace-and-agents\",\"189\":\"24#_2-agent-paths-and-costs\",\"190\":\"24#_3-teams-and-objectives\",\"191\":\"24#_4-pareto-optimality\",\"192\":\"24#_5-general-and-fully-cooperative-tcpf\",\"193\":\"24#dominance-and-pareto-optimality\",\"194\":\"24#_1-pareto-optimality-and-trade-offs\",\"195\":\"24#_2-definition-of-dominance\",\"196\":\"24#_3-why-dominance-is-necessary\",\"197\":\"24#_4-how-to-choose-the-final-or-best-solution-when-finding-the-pareto-optimal-front\",\"198\":\"24#conflict-based-search-cbs\",\"199\":\"24#teamwise-cooperative-cbs-tc-cbs\",\"200\":\"24#tc-cbs-t-algorithm\",\"201\":\"24#experimental-results-and-conclusion\",\"202\":\"24@0\",\"203\":\"25\",\"204\":\"25@0\",\"205\":\"26\",\"206\":\"26#_1-1-what-is-planning\",\"207\":\"26#_1-2-basic-ingredients-of-planning\",\"208\":\"26#_1-3-organization-of-this-book\",\"209\":\"26@0\",\"210\":\"27\",\"211\":\"27#_2-1-introduction-to-discrete-feasible-planning\",\"212\":\"27#_2-1-1-problem-formulation\",\"213\":\"27#_2-1-2-examples-of-discrete-planning\",\"214\":\"27#_2-2-searching-for-feasible-plans\",\"215\":\"27#_2-2-1-general-forward-search\",\"216\":\"27#_2-2-2-particular-forward-search-methods\",\"217\":\"27#_2-2-3-other-general-search-schemes\",\"218\":\"27#_2-2-4-a-unified-view-of-the-search-methods\",\"219\":\"27#_2-3-discrete-optimal-planning\",\"220\":\"27#_2-3-1-optimal-fixed-length-plans\",\"221\":\"27#_2-3-1-1-backward-value-iteration\",\"222\":\"27#_2-3-1-2-forward-value-iteration\",\"223\":\"27#_2-3-2-optimal-plans-of-unspecified-lengths\",\"224\":\"27#_2-3-3-dijkstra-revisited\",\"225\":\"27#_2-4-using-logic-to-formulate-discrete-planning\",\"226\":\"27#_2-4-1-a-strips-like-representation\",\"227\":\"27#_2-4-2-converting-to-the-state-space-representation\",\"228\":\"27#_2-5-logic-based-planning-methods\",\"229\":\"27#_2-5-1-searching-in-a-space-of-partial-plans\",\"230\":\"27@0\",\"231\":\"28\",\"232\":\"29\",\"233\":\"30\",\"234\":\"31\",\"235\":\"32\",\"236\":\"33\",\"237\":\"34\",\"238\":\"35\",\"239\":\"36\"},\"fieldIds\":{\"h\":0,\"t\":1,\"c\":2},\"fieldLength\":{\"0\":[2,3],\"1\":[1,30],\"2\":[1,21],\"3\":[1,65],\"4\":[1,25],\"5\":[2,33],\"6\":[null,null,1],\"7\":[7,54],\"8\":[9,57],\"9\":[1,20],\"10\":[1,32],\"11\":[1,40],\"12\":[1,31],\"13\":[2,24],\"14\":[2,17],\"15\":[2],\"16\":[3,35],\"17\":[2,33],\"18\":[1,86],\"19\":[null,null,1],\"20\":[4,13],\"21\":[1,107],\"22\":[2,66],\"23\":[2,139],\"24\":[5,81],\"25\":[2,111],\"26\":[null,null,1],\"27\":[8,26],\"28\":[2,52],\"29\":[5,70],\"30\":[2,78],\"31\":[5],\"32\":[1,45],\"33\":[4,32],\"34\":[2,50],\"35\":[5,32],\"36\":[null,null,1],\"37\":[4,41],\"38\":[2,85],\"39\":[2,89],\"40\":[6,42],\"41\":[4,35],\"42\":[null,null,1],\"43\":[5,121],\"44\":[5,127],\"45\":[6,153],\"46\":[5,138],\"47\":[null,null,1],\"48\":[2,65],\"49\":[1,273],\"50\":[2],\"51\":[4,10],\"52\":[4,38],\"53\":[2,76],\"54\":[1,32],\"55\":[2,22],\"56\":[2],\"57\":[2,29],\"58\":[2,52],\"59\":[2,127],\"60\":[1,43],\"61\":[2,112],\"62\":[null,null,1],\"63\":[2],\"64\":[6],\"65\":[1,9],\"66\":[1,14],\"67\":[3,44],\"68\":[4],\"69\":[2,43],\"70\":[2],\"71\":[4,23],\"72\":[4,15],\"73\":[2],\"74\":[4,19],\"75\":[3,15],\"76\":[null,null,1],\"77\":[1,1],\"78\":[1,152],\"79\":[null,null,1],\"80\":[5],\"81\":[4,92],\"82\":[4,157],\"83\":[2,82],\"84\":[3,37],\"85\":[null,null,1],\"86\":[3,62],\"87\":[1,125],\"88\":[1,101],\"89\":[2,141],\"90\":[3],\"91\":[1,42],\"92\":[3,41],\"93\":[4,79],\"94\":[1,48],\"95\":[3,30],\"96\":[2,44],\"97\":[3,22],\"98\":[2,71],\"99\":[1,43],\"100\":[3,47],\"101\":[4,52],\"102\":[2,13],\"103\":[1,20],\"104\":[1,160],\"105\":[null,null,1],\"106\":[3],\"107\":[null,null,1],\"108\":[1,1],\"109\":[1,21],\"110\":[2,213],\"111\":[2,178],\"112\":[2,31],\"113\":[2,39],\"114\":[2,57],\"115\":[3,123],\"116\":[3,83],\"117\":[2,26],\"118\":[5],\"119\":[5,55],\"120\":[6,62],\"121\":[3,64],\"122\":[4,62],\"123\":[5,67],\"124\":[4,76],\"125\":[null,null,1],\"126\":[3,6],\"127\":[1,28],\"128\":[1,12],\"129\":[1,10],\"130\":[1,26],\"131\":[1,17],\"132\":[1,22],\"133\":[1,40],\"134\":[1,63],\"135\":[null,null,1],\"136\":[1,3],\"137\":[2],\"138\":[5,26],\"139\":[4,9],\"140\":[6,78],\"141\":[5,21],\"142\":[4],\"143\":[5,55],\"144\":[4,127],\"145\":[12],\"146\":[4,43],\"147\":[3,8],\"148\":[2,100],\"149\":[3,3],\"150\":[3,43],\"151\":[null,null,1],\"152\":[null,null,1],\"153\":[3,7],\"154\":[1,160],\"155\":[1,21],\"156\":[1],\"157\":[1,15],\"158\":[1,69],\"159\":[1,3],\"160\":[1,111],\"161\":[1,138],\"162\":[1,6],\"163\":[null,null,1],\"164\":[null,null,1],\"165\":[4,2],\"166\":[1,59],\"167\":[2,76],\"168\":[4,80],\"169\":[2,36],\"170\":[null,null,1],\"171\":[3],\"172\":[1,17],\"173\":[1,3],\"174\":[1,14],\"175\":[null,null,1],\"176\":[2],\"177\":[1,40],\"178\":[1,5],\"179\":[null,null,1],\"180\":[1,4],\"181\":[1,39],\"182\":[1,4],\"183\":[null,null,1],\"184\":[10,17],\"185\":[1,122],\"186\":[5],\"187\":[2],\"188\":[4,41],\"189\":[5,65],\"190\":[4,75],\"191\":[3,69],\"192\":[6,55],\"193\":[4,84],\"194\":[6,37],\"195\":[4,74],\"196\":[5,109],\"197\":[14,87],\"198\":[5,128],\"199\":[5,181],\"200\":[4,89],\"201\":[4,5],\"202\":[null,null,1],\"203\":[11,149],\"204\":[null,null,1],\"205\":[3,19],\"206\":[5,23],\"207\":[6,204],\"208\":[6,20],\"209\":[null,null,1],\"210\":[4],\"211\":[7],\"212\":[4,33],\"213\":[6,121],\"214\":[5,43],\"215\":[5,123],\"216\":[5,143],\"217\":[6,94],\"218\":[9,108],\"219\":[5,123],\"220\":[7,42],\"221\":[6,198],\"222\":[6,142],\"223\":[7,276],\"224\":[4,239],\"225\":[8,50],\"226\":[7,282],\"227\":[8,217],\"228\":[6,19],\"229\":[10,205],\"230\":[null,null,1],\"231\":[1,3],\"232\":[1],\"233\":[1],\"234\":[1],\"235\":[1],\"236\":[1],\"237\":[1],\"238\":[1],\"239\":[1]},\"averageFieldLength\":[3.123194357942184,63.30543478967117,0.37469824438889093],\"storedFields\":{\"0\":{\"h\":\"简介 Introduction\",\"t\":[\"👋 欢迎来到我的博客！\"]},\"1\":{\"h\":\"写在前面\",\"t\":[\"其实很早就萌生过写 blog 的想法，但总感觉很难，很麻烦，不想走出舒适圈，所以迟迟不肯行动。\",\"但痛定思痛，觉得还是要尝试些新东西，所以便有了现在这个页面。\",\"刚开始的过程确实很难，很多新东西从未见过，一时难以消化吸收，过程中还遇到很多莫名其妙的小 bug。\",\"但还好有dream-oyh的帮助，让我顺利解决了很多问题，在此表示十分感谢！💖\",\"目前还是处于入门小白阶段，还有很多排版、布局、内容方面有待优化，我会逐步去完善。\",\"下面来 talk about myself。\"]},\"2\":{\"h\":\"缘由\",\"t\":[\"👐 决定写 blog 原因大致有以下几点：\",\"很早就有的想法（想当一个知识区博主、音乐区 up 主），目前先尝试一个。\",\"准备夏令营过程中，发现自己之前做过的很多小项目、写过的很多代码、学过的知识都记不清了，如果之前有所记录就很便于回忆，而且复习过程中也苦于没有地方整理，blog 就显得非常合适。\",\"周边盆友的影响👬。\",\"走出舒适圈，尝试新的记录生活的方式。\"]},\"3\":{\"h\":\"我\",\"t\":[\"职业： 一名在读 CSUer 🏫，对，就是那个California State University(bushi)。即将变为SJTUer.\",\"星座： 水瓶座 \",\"成分： Swiftie 🍓 一枚。\",\"人格： INFJ 绿老头一枚。(有点变化)\",\"爱好： LOL、听歌 🎧、唱歌 🎤、吉他 🎸、足球 ⚽。\",\"自我评价：自我感觉是一个矛盾体，看待问题十分的现实，但同时也是一个理想的完美主义者；绝对的宿命论主义倾向；梦想很多，却也逐渐看清了生活的本质，但还是希望自己未来能成为一个有用之人，探究生命的意义，世界之本质 💭 。\",\"📝 目前小小的愿望清单\",\"[✓] 进入到梦中情组\",\" 现场看一场 LOL 比赛(S 赛/MSI/EDG 比赛)\",\" 看一次霉妈的演唱会\",\" 现场看一次球赛（世界杯/国家队比赛/亚冠/欧冠/欧洲杯......）\",\" 冰岛/欧洲游\",\"twin flame who？\",\" 统一\",\" 国足进世界杯\"]},\"4\":{\"h\":\"指南\",\"t\":[\"目前博客内容主要为过去做过的一些项目、小研究、自学内容的整理等，有时间也会加入书评、乐评、随笔等其他内容。\",\"本科 主要记录本科专业一些相关学习内容。\",\"机器学习 主要为自学 ML 笔记及代码。\",\"路径规划 纪录读研阶段学习内容。\",\"日常 记录日常生活，e.g., 吉他学习/英语学习/随笔等\",\"Myself 博主的自我介绍。\",\"❤️ 感谢你看到这里 ❤️\"]},\"5\":{\"h\":\"Before reading\",\"t\":[\"This blog is mainly a notebook of Mathematical Foundations of Reinforcement Learning by Shiyu Zhao from Westlake University WindyLab.\",\"You can find more about the book and related tutorial videos at this link.\"]},\"6\":{\"c\":[\"RL\"]},\"7\":{\"h\":\"Chapter 1 Basic Concepts of Reinforcement Learning\",\"t\":[\"Reinforcement Learning (RL) can be described by the grid world example.\",\"We place one agent in an environment, the goal of the agent is to find a good route to the target. Every cell/grid the agent placed can be seen as a state. Agent can take one action at each state according to a certain policy. The goal of RL is to find a good policy to guide the agent taking a sequence of acitons, travelling from the start place, moving from one state to another, and finally reach the target.\",\"grid world\"]},\"8\":{\"h\":\"Basic Concepts from the perspective of Markov decision process\",\"t\":[\"Reinforcement learning utilizes the MDP framework to model the interaction between a learning agent and its environment. Actually the above example is just one simple description of the Markov decision preocess. It reflects how the agent interacts with the environment. The process directly involves three basic concepts state, action, policy and one transition state transition. It also implictly includes another concept reward.\",\"We will introduce the key concepts one by one in the following.\"]},\"9\":{\"h\":\"State\",\"t\":[\"State is the status of the agent with respect to the environment. Its set is the state space $ S = {s_i} $.\"]},\"10\":{\"h\":\"Action\",\"t\":[\"Action is what the agent do at a certain state. The agent will obtain a new state after taking one aciton. Similarly, its set is the action space of a state denoted as $ A(s_i) = {a_i} $.\"]},\"11\":{\"h\":\"Policy\",\"t\":[\"Policy denoted as $ \\\\pi $, tells the agent what actions to take at a state. It gives the probability of each action to be taken at a certain state denoted as π(at​∣st​). In mathematical form ,we use tabular representation to display one policy. In programming, we use one array, matrix to represent a policy.\",\"tabular representation\"]},\"12\":{\"h\":\"Reward\",\"t\":[\"Reward guides the agent to our target. Agent wants more reward, which means the agent will minimize (the reward is negative) or maximum (the reward is postive) the reward in the process.\",\"Reward depends on the current state and action not the next state.\"]},\"13\":{\"h\":\"Probability Distribution\",\"t\":[\"Involve two probability form:\",\"State transition probability: at state s, taking action a, the probability to transit to state s′ is p(s′∣s,a)\",\"Reward probability: at state s, taking action a, the probability to get reward r is p(r∣s,a)\"]},\"14\":{\"h\":\"Markov Property\",\"t\":[\"Memoryless property: The state transiting to the next depends on current state and action rather than previous.\"]},\"15\":{\"h\":\"Other concepts\"},\"16\":{\"h\":\"Trajectory and Return\",\"t\":[\"A trajectory is a state-action-reward chain:\",\"chain\",\"The return of this trajectory is the sum of all the rewards collected along the trajectory. It can be finite, e.g., transit from target to target s1​→s2​→s3​→s5​→s5​→s5​....\",\"return=0+0+0+1=1\",\"Return can be used to evaluate a policy.\"]},\"17\":{\"h\":\"Discounted Rate\",\"t\":[\"discount rateγ∈[0,1)\",\"Roles: 1) the sum of the return becomes finite instead of infinite; 2) balance the far and near future rewards:\",\"If γ is close to 0, the value of the discounted return is dominated by the rewards obtained in the near future.\",\"If γ is close to 1, the value of the discounted return is dominated by the rewards obtained in the far future\",\"return=0×γ+0×γ2+0×γ3+1×γ4+1×γ5.....=1−γγ​\"]},\"18\":{\"h\":\"Episode\",\"t\":[\"When interacting with the environment following a policy, the agent may stop at some terminal states. The resulting trajectory is called an episode (or a trial), e.g. s1​→s2​→s3​→s5​,\",\"An episode is usually assumed to be a finite trajectory. Tasks with episodes are called episodic tasks.\",\"We can treat episodic and continuing tasks in a unified mathematical way by converting episodic tasks to continuing tasks:\",\"Option 1: Treat the target state as a special absorbing state. Once the agent reaches an absorbing state, it will never leave. The consequent rewards r = 0.\",\"Option 2: Treat the target state as a normal state with a policy. The agent can still leave the target state and gain r = +1 when entering the target state.\",\"This tutorial course considers option 2 so as to not distinguish the target state from the others and can treat it as a normal state.\"]},\"19\":{\"c\":[\"RL\"]},\"20\":{\"h\":\"Chapter 2 Bellman Equation\",\"t\":[\"This chapter we will introduce two key concepts and one important formula.\"]},\"21\":{\"h\":\"Revision\",\"t\":[\"I recommand you reading the motivating examples in the tutorial. Here I will skip this part and directly introduce the concepts.\",\"Before delving into the context, we need to do a revision about previous key concepts.\",\"We learn four key components in RL, mamely stateS, actionA, policyπ and rewardR.\",\"These four components are comprised of the following process:\",\"St​→At​Rt+1​,St+1​\",\"This step is governed by the following probability distributions:\",\"With the policy, take what action under current state\",\"St​→At​At​ is governed by π(At​∣St​).\",\"Which state will be transited after taking action\",\"St​,At​→At​St+1​ is governed by p(St+1​∣St​,At​).\",\"Receive how much reward\",\"St​,At​→At​Rt+1​ is governed by p(Rt+1​∣St​,At​).\",\"Notice that $St, A_t, R are all random variables. Thus, We can calculate their expectation.\",\"Understand the mentioned three distribution is crucial for learning the Bellman Equation.\"]},\"22\":{\"h\":\"State Value\",\"t\":[\"Definition: The expectation of one specific state's discounted return.\",\"Denoted as E[Gt​∣St​=s]\",\"Remarks:\",\"State value is a function of s. It is a conditional expectation with the condition that the state starts from s.\",\"It is based on the policy π. For a different policy, the state value may be different.\",\"It represents the “value” of a state. If the state value is greater, then the policy is better because greater cumulative rewards can be obtained.\",\"Q: What is the relationship between return and state value?\",\"A: The state value is the mean of all possible returns that can be obtained starting from a state. If everything like π(a∣s),p(r∣s,a),p(s0​∣s,a) is deterministic, then state value is the same as return.\"]},\"23\":{\"h\":\"Bellman Equation\",\"t\":[\"We have already known the state value that is used to describe the expectation of discounted return gained from current state. Actually from the literal sense, we can learn that the state value is the expectation of reward with all possible actions the agent taken in current state. When taking one aciton, the agent will obtain a reward immediately and go into anther new postion (state). And in that state, the agent will 'benefit' from that state after taking another action and get reward.\",\"So the state value consists of two part, namely the immediate reward and future reward.\",\"The proof is omitted here. I recommand you looking it up in the tutorial.\",\"The Bellman Equation is:\",\"vπ​(s)=meanofimmediaterewarda∑​π(a∣s)r∑​p(r∣s,a)r​​+meanoffuturerewardγa∑​π(a∣s)s′∑​p(s′∣s,a)vπ​(s)​​,∀s∈S\",\"A brief version:\",\"vπ​(s)=a∑​π(a∣s)[r∑​p(r∣s,a)r+γs′∑​p(s′∣s,a)vπ​(s)]∀s∈S\",\"vπ​(s)=a∑​π(a∣s)[E[Rt+1​∣St​=s,At​=a]+γE[Gt+1​∣St​=s,At​=a]]∀s∈S\",\"A briefer version:\",\"vπ​(s)=rπ​(s)+γs′∑​pπ​(s′∣s)vπ​(s′)∀s∈S\",\"We can write it into matrix form:\",\"vπ​=rπ​+γPπ​vpi​∀s∈S\",\"where,\",\"vπ​=[vπ​(s1​),…,vπ​(sn​)]⊤∈Rn\",\"rπ​=[rπ​(s1​),…,rπ​(sn​)]⊤∈Rn\",\"Pπ​∈Rn×n, where ]Pπ​]ij​=pπ​(sj​∣si​),is the state‐transition matrix.\",\"Assume there are four states, the matrix form goes:\",\"matrix form\",\"Two examples:\",\"illustrative example 1\",\"illustrative example 2\",\"Bellman Equation reflect the relationship between state values. It is a set of equations, not just one.\",\"We can derive state value by solving it.\"]},\"24\":{\"h\":\"Ways to solve state value\",\"t\":[\"Given a policy, finding out the corresponding state values is called policy evaluation! It is a fundamental problem in RL. It is the foundation to find better policies. Thus, it is important to understand how to solve the Bellman equation.\",\"closed-form solution\",\"from the matrix form:\",\"vπ​(s)=rπ​+γPπ​vπ​\",\"we can get:\",\"vπ​(s)=(I−γPπ​)−1rπ​\",\"Closed-form solution is less used due to the existence of inverse matrix.\",\"iterative solution\",\"An iterative solution is:\",\"vπ​(k+1)=rπ​+γPπ​vk​\",\"This algorithm leads to a sequence v0​,v1​,v2​,.... We can show that\",\"vk​→vπ​=(I−γPπ​)−1rπ​,k→∞\",\"We will introduce it in detail in the following chapter.\",\"Here are two examples showing that how state value is leveraged to evaluate policy.\",\"better policy\",\"worse policy\"]},\"25\":{\"h\":\"Action value\",\"t\":[\"From state value to action value:\",\"State value: the average return the agent can get starting from a state.\",\"Action value: the average return the agent can get starting from a state and taking an action.\",\"Why do we care action value? Because we want to know which action is better. This point will be clearer in the following lectures. We will frequently use action values.\",\"Action value is determined by action and state, so the definition is:\",\"qπ​(s,a)=E[Gt​∣St​=s,At​=a]\",\"q depends on π.\",\"qπ​(s,a)=r∑​p(r∣s,a)r+s′∑​vπ​(s′)p(s′∣s,a)\",\"Action value is also composed of the immediate reward and future reward. The difference between action value and state value is that action value is value evaluation after taking action. It depends on specific action and state while state value is only based on state. It needs to consider all the possible action and the corresponding state transition brought by the action and its following reward.\",\"State value can also be thought as all possible actions time corresponding action value:\",\"vπ​(s)=a∑​π(a∣s)actionvalueqπ​(s,a)[r∑​p(r∣s,a)r+γs′∑​p(s′∣s,a)vπ​(s)]​​\",\"State value and action value can be derived from each other according to the above formula.\",\"Example for action value:\",\"action value\",\"Note that like qπ​(s1​,a3​)=0.\"]},\"26\":{\"c\":[\"RL\"]},\"27\":{\"h\":\"Chapter 3 Optimal Policy and Bellman Optimality Equation\",\"t\":[\"We know that RL's ultimate goal is to find the optimal policy. In this chapter we will show how we obtain optimal policy through Bellman Optimality Equation.\"]},\"28\":{\"h\":\"Optimal Policy\",\"t\":[\"The state value could be used to evaluate if a policy is good or not: if\",\"vπ1​​(s)≥vπ2​​(s),∀s∈S\",\"We say policy π1​ is 'better' than π2​.\",\"If\",\"vπ∗​(s)≥vπ​(s),∀s∈Sunder∀π\",\"We say policy π∗ is the optimal policy.\",\"Here comes the questions:\",\"Does the optimal policy exist?\",\"Is the optimal policy unique?\",\"Is the optimal policy stochastic or deterministic?\",\"How to obtain the optimal policy?\",\"Bellman Optimality Equation (BOE) will give you the answers.\"]},\"29\":{\"h\":\"Bellman optimality equation (BOE)\",\"t\":[\"Bellman optimality equation (elementwise form):\",\"v(s)​=πmax​a∑​π(a∣s)(r∑​p(r∣s,a)r+γs′∑​p(s′∣s,a)v(s′)),∀s∈S,=πmax​a∑​π(a∣s)q(s,a),s∈S.​​\",\"Notes:\",\"p(r∣s,a),p(s0​∣s,a) are known.\",\"v(s),v(s0​) are unknown and to be calculated.\",\"π(s) can be written with other π(s).\",\"Bellman optimality equation (matrix-vector form):\",\"v=πmax​(rπ​+γPπ​v)\",\"The expression contains two unknown elements, namely the policy π and state value v. So we need find an approach to solve it. But before introducing the solving algorithm, we need to learn some preliminaries through some interesting exapmles.\"]},\"30\":{\"h\":\"Motivating examples\",\"t\":[\"As mentioned above, BOE has two unknowns from one equation. How to solve problems like this? See the following exapmle:\",\"提示\",\"Example (How to solve two unknowns from one equation\",\"Okay, we know that the way is to fix one unknown and solve the equation. Suppose we fix v(s′) on the rightside of the equation.\",\"v(s)​=πmax​a∑​π(a∣s)(r∑​p(r∣s,a)r+γs′∑​p(s′∣s,a)v(s′)),∀s∈S,=πmax​a∑​π(a∣s)q(s,a),s∈S.​​\",\"We know that maxπ​∑a​=1. We will need to solve is the maximum with different probability assigned to each action value. So a similar exapmle goes:\",\"提示\",\"Example (How to solve max)\",\"So through that example, we will know how to gain optimal policy. That is to adopt the action with largest action value in all state.\",\"gain\"]},\"31\":{\"h\":\"Solve the Bellman optimality equation\"},\"32\":{\"h\":\"Preliminaries\",\"t\":[\"Fixed point: x∈X is a fixed point of f:x→X if :\",\"x=f(x)\",\"Contraction mapping: f is a contraction mapping if:\",\"∣∣f(x1​)−f(x2​)∣∣≤γ∣∣x1​−x2​∣∣,γ<1\",\"提示\",\"example\",\"So here we can introduce the important theorem:\",\"重要\",\"Contraction mapping theorem\",\"Examples like: x=0.5x ,xk+1​=0.5xk​ . Suppose that x0​=10 So x1​=5,x2​=2.5......x→0\"]},\"33\":{\"h\":\"Contraction property of BOE\",\"t\":[\"The Bellman Equation:\",\"v=πmax​(rπ​+γPπ​v)\",\"can be regarded as the function of v. So it can write like:\",\"v=f(v)\",\"And f(v) is a contraction mapping.\",\"So we can utilize the Contraction mapping theorem to solve BOE.\",\"Applying the contraction mapping theorem to solve BOE\"]},\"34\":{\"h\":\"Iterative algorithm\",\"t\":[\"Matrix vector form:\",\"vk+1​=f(vk​)=πmax​(rπ​+γPπ​vk​)\",\"Elementwise form:\",\"vk+1​(s)​=πmax​a∑​π(a∣s)(r∑​p(r∣s,a)r+γs′∑​p(s′∣s,a)vk​(s′)),=πmax​a∑​π(a∣s)qk​(s,a),=amax​qk​(s,a).​\",\"The procedure goes:\",\"procedure\",\"提示\",\"example\",\"Actions: al​,a0​,ar​ represent go left, stay unchanged, and go right.\",\"Reward: entering the target area: +1; try to go out of boundary -1.\",\"example\",\"example\",\"example\"]},\"35\":{\"h\":\"Factors determine the optimal policy\",\"t\":[\"Reward design: r\",\"System model: p(s0​∣s,a),p(r∣s,a)\",\"Discount rate: γ, affecting whether the model is short-sighted or not.\",\"$v(s), v(s_0), π(a|s) are unknowns to be calculate.\"]},\"36\":{\"c\":[\"RL\"]},\"37\":{\"h\":\"Value Iteration and Policy Iteration\",\"t\":[\"In the last chapter, we study the Bellman Optimality Equation and introduce the iterative algorithm. This chapter we will introduce three model-based approach for deriving optimal policy. I recommand read the pdf tutorial by yourself. In this blog I will mainly focus on the difference between value iteration, policy iteration and truncated policy iteration.\"]},\"38\":{\"h\":\"Value Iteration\",\"t\":[\"v=f(v)=πmax​rπ​+γPπ​v\",\"In the last chapter, we know that the contraction mapping theorem suggest an iterative algorithm:\",\"vk+1​=f(vK​)=πmax​rπ​+γPπ​vk​,k=1,2,3......\",\"v0​ can be arbitrary. We know we solve this equation with the value iteration.\",\"The algorithm goes two steps:\",\"Step 1 Policy Update (PU)\",\"Given initial value v0​(s), calculate action value q. For any state, choose largest action value obtaining the updated policy.\",\"Note that this value is not state value. It is just a interimmediate value.\",\"Step 2 Value Update (VU)\",\"Given the new policy, calculate new iterative value v1​(s). Since the policy is deterministic, the new value is equal to the largest action value.\",\"Value Iteration\",\"Value Iteration\",\"The procedure can be summarized as:\",\"vk​(s)→q(vk​(s),a)→π(k+1)(a∣s)=argamax​q(vk​(s),a)→vk+1​(s)=amax​q(vk​(s),a)\",\"Pseudocode: Value iteration algorithm\"]},\"39\":{\"h\":\"Policy Iteration\",\"t\":[\"Step 1 Policy Evaluation (PE)\",\"Namely the calculation of state value through iterative algorithm.\",\"vπk​​=rπk​​+γPπk​​vπk​​\",\"Note that this step is just an iteration in the policy iteration, namaly the nested iteration in the policy iteration.\",\"Step 1 Policy Improvement (PI)\",\"Now we gain the state value of current policy. We can obtain action value through\",\"qπ​(s,a)=r∑​p(r∣s,a)r+s′∑​vπ​(s′)p(s′∣s,a)\",\"Then improved policy is\",\"π(k+1)(a∣s)=argamax​q(vk​(s),a)\",\"Or in one equation\",\"πk+1​=argπmax​rπ​+γPπ​vπk​​\",\"Which is the same as the formula before since acquiring maximum action value is just acquiring current optimal policy.\",\"Note that here P is Pπ​ not Pπk​​ in the value update in value iteration since π is what we calculate now. It is unknown. We need to derive it through maximum the equation, which is finding the maximum action value.\",\"Policy Iteration Process\",\"Q1\",\"Q2\",\"Q3\",\"Policy Iteration\",\"Policy Iteration\",\"Pseudocode: Policy iteration algorithm\"]},\"40\":{\"h\":\"Differences between Value Iteration and Policy Iteration\",\"t\":[\"The key differences between Value Iteration and Policy Iteration is\",\"Value iteration has one iterative process while Policy Iteration has two.\",\"Value iteration do one iteration, update policy according to the interimmediate value at once and then continue the value iterative process and finally obtain the policy through this process. Policy iteration do a full iterative process to obtain real state value and then derive a new policy according to the state value and finnaly derive the policy.\",\"Comparison between VI and PI\",\"Comparison between VI and PI\",\"Comparison between VI and PI\"]},\"41\":{\"h\":\"Truncated policy iteration algorithm\",\"t\":[\"The truncated policy iteration is just one combination of the two. Or in other words, the policy iteration and value iteration are just one extreme example of truncated policy iteration.\",\"truncated policy iteration\",\"Pseudocode: Truncated policy iteration algorithm\",\"For the convergence:\",\"Proposition (Value Improvement)\",\"llustration\",\"So truncated policy iteration is actually a trade-off.\"]},\"42\":{\"c\":[\"RL\"]},\"43\":{\"h\":\"Chapter 5 Monte Carlo Learning\",\"t\":[\"This chapter we will introduce a model-free approach for deriving optimal policy.\",\"Here, model-free refers that we do not rely on a specific mathematical model to obtain state value or action value. Like, in the policy evaluation, we use BOE to obtain state value, which is just model-based. For model-free, we do not use that equation anymore. Instead, we leverage the mean estimation methods.\",\"Probably the following example can better illustrate\",\"Example Flip a coin\",\"The result (either head or tail) is denoted as a random variable X.\",\"if the result is head, then X=+1.\",\"if the result is tail, then X=−1.\",\"The aim is to compute E[X].\",\"The model-based approach is to calculate the expectation through the definition.\",\"E[X]=x∑​p(x)x=0.5×1+0.5×(−1)=0.5\",\"Problem: it may be impossible to know the precise distribution!!\",\"The model-free approach is based on sampling.\",\"We flip the coin many times, and then calculate the average of the outcomes.\",\"Suppose we get a sample sequence: x1​,x2​,...,xN​. Then, the mean can be approximated as:\",\"E[X]≃xˉ=j=1∑N​Nxj​​\",\"This is the idea of Monte Carlo estimation and it is supported by the Law of Large Numbers to be accurate.\"]},\"44\":{\"h\":\"Monte Carlo (MC) Basic Algorithm\",\"t\":[\"The policy iteration involves two processes, namely policy evaluation and policy improvement.\",\"As mentioned before, the differences between model-based and model-free is policy evaluation in which how to gain the action value is of paramount importance.\",\"model-based and model-free approach for calculating action value\",\"Here we use the expression 2, namely estimate the expectation of every state-action pair as their real value.\",\"Here is a thorough statement and the corresponding pseudocode.\",\"Given an initial policy π0​, there are two steps at the kth iteration.\",\"Step 1 Policy Evaluation: This step is to obtain qπk​​(s,a) for all (s,a). Specifically, for each action-state pair (s,a), run an infinite number of (or sufficiently many) episodes. The average of their returns is used to approximate qπk​​(s,a).\",\"Step 2 Policy Improvement: Just like the policy improvement in the policy iteration. Acquire the maximum action value in every state.\",\"Exactly the same as the policy iteration algorithm, except that we estimate $q_{\\\\pi_{k}}(s, a) $ directly, instead of solving vπk​​(s).\",\"MC Basic (a model-free variant of policy iteration)\",\"Q\",\"Why does MC Basic estimate action values instead of state values?\",\"That is because state values cannot be used to improve policies directly. When models are not available, we should directly estimate action values.\",\"Since policy iteration is convergent, the convergence of MC Basic is also guaranteed to be convergent given sufficient episodes.\"]},\"45\":{\"h\":\"Monte Carlo (MC) Exploring Starts Algorithm\",\"t\":[\"In MC Basic Algorithm, we have to start from every state-action pair and do many samplings to estimate, which is less efficient. In detail, the episode also visits many other state-action pairs such as (s2​,a4​),(s2​,a3​),and(s5​,a1​). These visits can also be used to estimate the corresponding action values. In particular, we can decompose the episode into multiple subepisodes:\",\"subepisodes\",\"Compare with MC Basic, MC Exploring Starts sufficiently utilize data. The method goes:\",\"Given a episode, it also focuses on other state-action pairs. Each can be regarded as a start and can be used to do a self-estimation.\",\"For data appears in one episode, there are two methods:\",\"first-visit: only the one first appear in the episode will be leveraged to do average.\",\"every-visit: no matter how many times it appear, all will be used to do average.\",\"For when to update the policy. Also, there are two methods:\",\"The first method is, in the policy evaluation step, to collect all the episodes starting from a state-action pair and then use the average return to approximate the action value.\",\"This is the one adopted by the MC Basic algorithm.\",\"The problem of this method is that the agent has to wait until all episodes have been collected.\",\"The second method uses the return of a single episode to approximate the action value. That means we improve the policy right after one episode not all the episodes. We do not care about the accuracy but should make sure all state-action pairs are considered.(by starting from every state-action pair).\",\"In this way, we can improve the policy episode-by-episode.\",\"In fact, this strategy falls into the scope of generalized policy iteration introduced in the last chapter. That is, we can still update the policy even if the value estimate is not sufficiently accurate.\",\"MC Exploring Starts (an efficient variant of MC Basic)\",\"Q: if a state-action pair does not appear in the first episode. How to calculate the average action value?\"]},\"46\":{\"h\":\"Monte Carlo (MC) -greedy Algorithm\",\"t\":[\"A policy is called soft if the probability to take any action is positive.\",\"With a soft policy, a few episodes that are sufficiently long can visit every state-action pair for sufficiently many times. Then, we do not need to have a large number of episodes starting from every state-action pair. Hence, the requirement of exploring starts (start from every state-action) can thus be removed.\",\"The difference between exploring starts and ε-greedy is just the policy turned from deterministic to stochastic. That is, to integrate ε-greedy policies into MC learning, we only need to change the policy improvement step from greedy to ε-greedy.\",\"ε-greedy policy\",\"ε-greedy algorithm\",\"It does not require exploring starts, but still requires to visit all state-action pairs in a different form.\",\"There are two parameters, namely the ε and the step length of the episode. All of them will affect the algorithm performance.\",\"We can see from below examples:\",\"example\",\"example\",\"The feature or advantage of ε-greedy is that it has stronger exploration ability so that the exploring starts condition is not required. But it cannot guarantee policy optimalitly in general since its stochastic characteristic. We can only prove there exists the optimal policy and if we take every action with the largest probability, the policy is the same as the optimal. We can say the ε-greedy policy is consistent with the optimal policy.\",\"Iteration\",\"consistency\",\"We can see when ε is large, the policy is not consistent with optimal policy.\",\"So in practice, the set of ε can not be too large. And after we gain the consistent ε-greedy policy, we turn it into deterministic (fully greedy).\"]},\"47\":{\"c\":[\"RL\"]},\"48\":{\"h\":\"Attention Mechanism\",\"t\":[\"This article will introduce a powerful technique in machine learning called Ateention Mechanism.\",\"The core method of attention mechanism is to pay more attention to what we want. It allows model to weigh the importance of different parts of input dynamically rather than treating them equally. The model learns to assign higher weights to the most relevant elements.\",\"Before stepping into the main text, we should first know some preliminary knowledge(Understand hidden states, encoder, decoder).\"]},\"49\":{\"h\":\"Preliminaries\",\"t\":[\"1. RNNs and LSTM network\",\"The content introduced below is mainly from this three blog/article. For details, click the following link to learn more.\",\"[1] Recurrent neural network From Wikipedia\",\"[2] Understanding LSTM Networks\",\"[3] 如何从RNN起步，一步一步通俗理解LSTM\",\"RNNs, short for Recurrent Neural Networks, is a class of artificial nerural network used to process sequencial data. Unlike FFN(Forwardfeed Neural Network), RNNs process data across multiple times rather than in a single time, making them well-adapted for modelling and processing text, speech, and time series.\",\"The following picture demonstrates the working flow of RNNs.\",\"An unrolled recurrent neural network from [2]\",\"RNNs are made up of many units or a loop. Each unit are fed with the previous unit's state————the hidden state ht−1​ and the input element xt​. The hidden state is the extraction of the input's feature and can be calculated to obtain the output. We can use this formula to express:\",\"ht​=f(Uht−1​,Wxt​,b)\",\"Here f are activation function like sigmoid, ReLu.\",\"By applying another function/transformation like Softmax to h, we can obtain the output yt​ at each time step.\",\"Out put of RNNs from [2]\",\"However, RNNs fall short in Long-TermDependencies, causing vanishing gradient problem. So here we introduce LSTM, short for Long Short Term Memory networks.\",\"LSTM follows the main structure of RNNs, adapting the inner structure of each recurrent unit. Each unit comprise of one cell state and three 'gates', the 'forget gate layer', 'input gate layer' and 'output gate layer'\",\"The key to LSTMs is the cell state, the horizontal line running through the top of the diagram.The cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. It’s very easy for information to just flow along it unchanged.\",\"cell state from [2]\",\"It receives information from the 'forget gate layer' and 'input gate layer'.\",\"Forget gate layer from [2]\",\"Input gate layer from [2]\",\"Output gate layer from [2]\",\"Calculat new cell state\",\"Overview of LSTM\",\"2. Encoder and Decoder\",\"[4] 从Encoder-Decoder(Seq2Seq)理解Attention的本质\",\"[5] What are Attention Mechanisms in Deep Learning?\",\"The Encoder-Decoder framework, also called Seq2Seq framework, is a widely used design pattern in ML.\",\"The encoder’s job is to capture the context and important information from the input sequence. Leveraging RNNs, LSTM, GRUs, it transforms the input sequence into a high-dimensional representation that can be used by the decoder to generate the output sequence.\",\"The decoder’s job is to generate the output sequence one token at a time. It uses the information from the encoder as well as its own hidden states to produce coherent and contextually accurate outputs.\",\"For example, here is a sentence pair (Source,Target)\",\"Source=(x1​,x2​,x3​,...,xn​)\",\"Target=(y1​,y2​,y3​,...,yn​)\",\"The encoder, as its name suggests, encodes the input sentence Source, transforming the input sentence into an intermediate semantic representation C:\",\"C=f(x1​,x2​,x3​,...,xn​)\",\"For the decoder, its task is to generate the target sentence based on the intermediate semantic representation C of the sentence Source and the historical information generated previously.\",\"yi​=g(C,y1​,y2​,y3​,...,yi−1​)\"]},\"50\":{\"h\":\"Basic Components\"},\"51\":{\"h\":\"Key Components of Attention\",\"t\":[\"The attention mechanism typically involves the following key elements:\"]},\"52\":{\"h\":\"Query, Key, and Value\",\"t\":[\"Query (Q): Represents what the model is currently focusing on (e.g., the word or token for which attention is being computed).\",\"Key (K): Represents the features of the input elements that the model compares against the query.\",\"Value (V): Represents the actual information or content of the input elements that the model uses for the output.\"]},\"53\":{\"h\":\"Attention Score\",\"t\":[\"The attention score measures the similarity between the query and each key. A common approach is to compute the dot product between the query and the keys, optionally scaled by a factor to stabilize training:\",\"Score(Q,K)=dk​​QKT​(1)\",\"where dk​ is the dimension of the key vector.\",\"The implementation of attention algorithms differs based on the way it measures the similarity.\",\"Bahdanau Attention:\",\"e(hi​,sj​)=Utanh(Vh​+Ws​)\",\"where U, V, and W are model parameters, and e(h,s) represents a fully connected layer.\",\"Luong Attention:\",\"e(h,s)=hTWs\",\"However, both Bahdanau and Luong attention are soft attention mechanisms, calculating αi,j using a softmax function.\"]},\"54\":{\"h\":\"Softmax\",\"t\":[\"The scores are passed through a softmax function to produce normalized attention weights. These weights sum to 1, indicating the importance of each input element relative to the others:\",\"Attention Weights=softmax(Score(Q,K))(2)\"]},\"55\":{\"h\":\"Weighted Sum\",\"t\":[\"The attention weights are applied to the values (V) to compute a weighted sum, which becomes the output of the attention mechanism:\",\"Output=Attention Weights⋅V(3)\"]},\"56\":{\"h\":\"Working Flow\"},\"57\":{\"h\":\"Encoding Phase\",\"t\":[\"The encoder processes the input(namely the source), transforming them into hidden state [s1​,s2​,...,sk​,...,sT​](namely the key). T is the length of the input.\",\"This process is the content we introduce in the preliminaries.\"]},\"58\":{\"h\":\"Attention Calculation\",\"t\":[\"Calculate the similarity, namely the attention score, between decoder's current hidden state ht​(namely the query) and each element of encoder's output(si​,i=1,2...T) with (1).\",\"Derive attention score of si​ using (2).\",\"Calculate weighted sum using (3). The result is called the context vector, which is passed to the decoder to generate the its next hidden state and the output.\",\"The context vector computed by the attention mechanism, which contains weighted information from the Encoder's hidden states.\"]},\"59\":{\"h\":\"Decoding Phase\",\"t\":[\"The next hidden state of the Decoder is determined by three main components:current hidden state, context vector, decoder input(e.g., the previously generated word in machine translation).\",\"Specifically, the next hidden state ht​ is generated by feeding the current hidden state ht−1​, the Context Vector ct​, and the Decoder input yt−1​ into an RNNs, LSTM, GRUs. This can be expressed as:\",\"ht​=RNN(ht−1​,[ct​,yt−1​])\",\"Where:\",\"ht​ is the hidden state at the current time step,\",\"ht−1​ is the hidden state at the previous time step,\",\"ct​ is the Context Vector at the current time step,\",\"yt−1​ is the output from the previous time step (Decoder input).\",\"The output of the Decoder is typically generated through the following steps:\",\"Compute Logits Using Hidden State and Context Vector: The hidden state ht​, the Context Vector ct​ and the output from the previous time step yt−1​ are combined and passed through a fully connected layer (often a linear transformation followed by an activation function) to produce unnormalized scores (logits).\",\"logits=Wo​[ht​;ct​;yt−1​]+bo​\",\"Where:\",\"Wo​ and bo​ are learnable parameters,\",\"[ht​;ct​] represents the concatenation of the hidden state and the Context Vector.\",\"Generate Probability Distribution via Softmax: The logits are passed through a Softmax function to produce a probability distribution over possible outputs.\",\"p(yt​∣y<t​,x)=Softmax(logits)\",\"Select the Output: Based on the probability distribution, the word with the highest probability is selected as the output yt​ for the current time step. Alternatively, sampling methods can be used to generate the output.\",\"The following pictures can clearly illustrate this process.\",\"illustration of Attention Mechanism(1)\",\"illustration of Attention Mechanism(2)\"]},\"60\":{\"h\":\"Summary\",\"t\":[\"The essence of attention mechanism is that we select important information from the input,giving them more weigths so as to omit redundant information.\",\"The function of attention mechanism is to calculate the weighted sum between query and value. The weights are measured by the similarity between query and key.\",\"Attention(Source,Queryt​)=i=1∑N​Similarity(Queryt​,Keyi​)⋅Valuei​\"]},\"61\":{\"h\":\"Example & code\",\"t\":[\"提示\",\"To better understand the matrix calculation, we can see the following illustration.\",\"illustration\",\"Assume:\",\"Key (K): [3, 2]（three itmes with the dimension of 2）\",\"Value (V): [3, 2]（three itmes with the dimension of 2）\",\"Query (Q): [3, 2]（three itmes with the dimension of 2）\",\"Q=​135​246​​,K=​0.51.52.5​123​​,v=​103050​204060​​\",\"Code:\",\"import torch import torch.nn as nn class Attention(nn.Module): def __init__(self, dim): super(Attention, self).__init__() ''' if you want to apply a linear transformation to the input(q, k, v), you can add the following code to your program. self.wk = torch.nn.Linear(dim, dim) self.wv = torch.nn.Linear(dim, dim) self.wq = torch.nn.Linear(dim, dim) ''' def forward(self, query, key, value): ''' if the input is not in batch, we need to add a dimension(batch dimension) to the input. ''' if len(key.shape) == 2: key = torch.unsqueeze(key, dim=0) if len(query.shape) == 2: query = torch.unsqueeze(query, dim=0) if len(value.shape) == 2: value = torch.unsqueeze(value, dim=0) # make sure the input is float type. key = key.float() query = query.float() value = value.float() ''' if apply linear transformation, add the following code. key = self.wk(key) value = self.wv(value) query = self.wv(query) ''' d_k = key.shape[-1] score = torch.bmm(query, key.transpose(-2, -1))/(d_k ** 0.5) print(f\\\"score: \\\\n {score} \\\\n\\\") attention_weights = torch.nn.functional.softmax(score, dim=-1) print(f\\\"attention weights: \\\\n {attention_weights} \\\\n\\\") output = torch.bmm(attention_weights, value) return output # input Q = torch.tensor([[1, 2], [3, 4], [5, 6]]) K = torch.tensor([[0.5, 1], [1.5, 2], [2.5, 3]]) V = torch.tensor([[10, 20], [30, 40], [50, 60]]) attention_layer = Attention(2) output = attention_layer(Q, K, V) print(f\\\"output: \\\\n{output}\\\\n\\\") \",\"Results:\",\"Results\"]},\"62\":{\"c\":[\"ML\"]},\"63\":{\"h\":\"Control Variate\"},\"64\":{\"h\":\"layout: Slide sidebar: false breadcrumb: false pageInfo: false\"},\"65\":{\"h\":\"Target\",\"t\":[\"Reduce the variance of a random variable X.\"]},\"66\":{\"h\":\"Method\",\"t\":[\"Generate an alternative random variable Y such that:\",\"E(Y)=E(X)\",\"Var(Y)<Var(X)\"]},\"67\":{\"h\":\"Approach and Proof\",\"t\":[\"The control variate Y is defined as:\",\"Y=X+b(C−E(C))(1)\",\"Where:\",\"C is any other random variable (different from X) with known E(C)\",\"b∈R is a constant\",\"Key Requirements\",\"C must be such that E(C) is known a priori\",\"C should be an inherent part of the simulation output for X (generated \\\"for free\\\" with X)\"]},\"68\":{\"h\":\"Control Variate in IMTSP\"},\"69\":{\"h\":\"Problem Formulation\",\"t\":[\"View MTSP as a bilevel optimization problem:\",\"Upper Level:\",\"Optimize allocation network f(θ) (city-agent assignment)\",\"Optimize surrogate network s(γ)\",\"Lower Level:\",\"Optimize TSP solver g(μ∗) (single-agent routing)\",\"minL=maxD(g(h(f(θ))),μ)\",\"Notation Breakdown\",\"Component\",\"Description\",\"f(θ)\",\"Allocation network (parameters θ)\",\"h(⋅)\",\"Sampling function\",\"g(⋅)\",\"TSP solver\",\"μ\",\"TSP solver parameters\",\"D(⋅)\",\"Euclidean distance cost function\"]},\"70\":{\"h\":\"Gradient Estimation\"},\"71\":{\"h\":\"Challenge 1: Non-Differentiability\",\"t\":[\"Solution: Log-Derivative Trick\",\"Implement gradient computation through:\",\"Allocation network f\",\"TSP solver g and parameters μ\",\"Compact form:\",\"minL=maxD(g(h(f(θ))),μ)\"]},\"72\":{\"h\":\"Proof of Gradient Interchange\",\"t\":[\"Under regularity conditions:\",\"∇θ​L=∇θ​E[D(g(h(f(θ))),μ)]\",\"Rewritten as:\",\"∇θ​L=E[∇θ​D(g(h(f(θ))),μ)]\"]},\"73\":{\"h\":\"Variance Reduction\"},\"74\":{\"h\":\"Challenge 2: High Variance\",\"t\":[\"::: success Solution: Control Variate Introduce surrogate network to:\",\"Provide control variate for allocation network\",\"Minimize single-sample gradient variance :::\"]},\"75\":{\"h\":\"Surrogate Network Design\",\"t\":[\"Input: Allocation matrix P(θ)Output: Maximum tour length L′\",\"E(C)=E[∇θ​logP(θ)]\"]},\"76\":{\"c\":[\"ML\"]},\"77\":{\"h\":\"卷积\",\"t\":[\"本节整理卷积方面基本概念\"]},\"78\":{\"h\":\"基本概念\",\"t\":[\"卷积：卷积就是用一个可移动的窗口（卷积核），按一定步长，与图像对应元素进行点乘相加的操作。卷积本质上也是一种对数据维度的变换，提取图像的特征，相较于全连接层直接把图像展开成一个行向量，其能更好地捕获图像的空间特征，当然通过改变参数的形状，任何全连接层都能被转换为一个等价卷积层。\",\"Convolution: Convolution is to use a movable window (convolution kernel) to perform a dot multiplication and addition operation with the corresponding elements of the image at a certain step size. Convolution is essentially a transformation of the data dimension to extract the features of the image. Compared with the fully connected layer that directly expands the image into a row vector, it can better capture the spatial features of the image. Of course, by changing the shape of the parameters, any fully connected layer can be converted into an equivalent convolution layer.\",\"池化：一种下采样方式，池化层的引入是仿照人的视觉系统对视觉输入对象进行降维（reduce demention）和抽象（abstract）。主要有三个功效：\",\"特征不变性：池化操作是模型更加关注是否存在某些特征而不是特征具体的位置。其中不变形性包括，平移不变性、旋转不变性和尺度不变性。平移不变性是指输出结果对输入对小量平移基本保持不变，例如，输入为(1, 5, 3), 最大池化将会取 5，如果将输入右移一位得到(0, 1, 5)，输出的结果仍将为 5。对伸缩的不变形，如果原先的神经元在最大池化操作后输出 5，那么经过伸缩（尺度变换）后，最大池化操作在该神经元上很大概率的输出仍是 5；\",\"特征降维（下采样）：池化相当于在空间范围内做了维度约减，从而使模型可以抽取更加广范围的特征。同时减小了下一层的输入大小，进而减少计算量和参数个数。\",\"在一定程度上防止过拟合，更方便优化。\",\"实现非线性（类似 relu）。\",\"扩大感受野。\",\"上采样：放大图像，反卷积/转置卷积。\",\"下采样：缩小图像，如池化与步长为 2 的卷积。\",\"relu 激活函数：负为 0，正 y=x\",\"采用原因：\",\"第一，采用 sigmoid 等函数，算激活函数时（指数运算），计算量大，反向传播求误差梯度时，求导涉及除法，计算量相对大，而采用 Relu 激活函数，整个过程的计算量节省很多。\",\"第二，对于深层网络，sigmoid 函数反向传播时，很容易就会出现梯度消失的情况（在 sigmoid 接近饱和区时，变换太缓慢，导数趋于 0，这种情况会造成信息丢失），从而无法完成深层网络的训练。\",\"第三，ReLu 会使一部分神经元的输出为 0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生。\"]},\"79\":{\"c\":[\"ML\"]},\"80\":{\"h\":\"Problem record of learning pytorch\"},\"81\":{\"h\":\"1. torch.optim ———— scheduler\",\"t\":[\"Scheduler is used to adjust learning rate.\",\"torch.optim.lr_scheduler.LRScheduler provides several methods to adjust the learning rate based on the number of epochs. torch.optim.lr_scheduler.ReduceLROnPlateau allows dynamic learning rate reducing based on some validation measurements.\",\"ReduceLROnPlateau expects a scalar value (usually a summed loss or cost) that reflects the training progress. This scalar value is used to determine if the learning rate should be reduced.\",\"like:\",\"scheduler_p.step(torch.tensor(cost, device=device).sum()) \",\"It updates the learning rate based on the sum of the cost across the batch, reflecting the total loss over the batch.\",\"The ReduceLROnPlateau scheduler observes this summed cost over several iterations.\",\"If the cost is improving (i.e., decreasing over time), the scheduler will keep the learning rate as it is or reduce it slightly (depending on the parameters).\",\"If the cost plateaus (i.e., stops improving or starts increasing), the scheduler will reduce the learning rate. This helps in fine-tuning the model when the updates become small or stagnant.\"]},\"82\":{\"h\":\"2. torch.autograd.grad\",\"t\":[\"torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=None, is_grads_batched=False, materialize_grads=False)[source] \",\"Compute and return the sum of gradients of outputs with respect to the inputs.\",\"grad_outputs should be a sequence of length matching output containing the “vector” in vector-Jacobian product, usually the pre-computed gradients w.r.t. each of the outputs. If an output doesn’t require_grad, then the gradient can be None).\",\"That means grad_outputs is essentially the gradient of the loss with respect to the outputs you're interested in. It allows you to specify how much each output contributes to the overall loss, which is necessary when you manually compute gradients using torch.autograd.grad.\",\"grad_outputs should match the shape of the output tensor you are differentiating with respect to.\",\"For example, if loss = a + b, then:\",\"d(loss)/d(a) = 1 and d(loss)/d(b) = 1.\",\"By passing grad_outputs=torch.ones_like(loss), you're telling PyTorch to propagate the gradient of 1 through the graph to compute the other gradients.\",\"create_graph=True: This argument allows you to compute the gradient of the gradient (i.e., second-order gradients). By setting it to True, you're telling PyTorch to track the computation for further differentiation. This is particularly useful when you're performing optimization on higher-order derivatives (like the variance of the gradients). Without create_graph=True, the computation graph would be discarded after the first backward pass, preventing higher-order gradients from being computed.\",\"retain_graph=True: Normally, after calling backward(), PyTorch releases the computation graph to save memory. Setting retain_graph=True prevents this, which is important when you want to compute more than one gradient with respect to the same graph (which seems to be the case here, as you're calculating the gradients for the surrogate network afterward).\"]},\"83\":{\"h\":\"Learning rate\",\"t\":[\"When the cost stops improving (or plateaus), it often means that the model is near a local minimum or has reached a point where the gradient is very small, making it hard to improve further.\",\"In such situations, reducing the learning rate encourages the optimizer to take smaller steps and refine the model's parameters more precisely.\",\"If the learning rate is too high, the model might quickly converge to a suboptimal solution. It might not have fully explored the parameter space to find the true minimum of the cost function.\",\"By gradually reducing the learning rate, the optimizer can continue exploring the space without making drastic changes to the parameters. This allows the model to refine its solution and potentially find better optima over time.\"]},\"84\":{\"h\":\"Manually optimize parameters\",\"t\":[\"grad_ps = torch.square(grad_temp).mean(0) # variance grad_s = torch.autograd.grad(grad_ps, surrogate.parameters(), grad_outputs=torch.ones_like(grad_ps), retain_graph=True, allow_unused=True) for params, grad in zip(surrogate.parameters(), grad_s): params.grad = grad \",\"We manually replace the derivative in updataing funciton p = p - a*(derivative) with the derivative we define.\"]},\"85\":{\"c\":[\"ML\"]},\"86\":{\"h\":\"Graph Neural Network\",\"t\":[\"[1] A Gentle Introduction to Graph Neural Networks\",\"[2] Graph neural networks: A review of methods and applications\",\"[3] 【图神经网络】10分钟掌握图神经网络及其经典模型\",\"Graph Neural Networks (GNNs) are a class of deep learning models designed to process graph-structured data. Unlike traditional neural networks (e.g., CNNs for grids, RNNs for sequences), GNNs explicitly model relationships between entities by propagating information through nodes and edges. They have become essential tools for tasks involving relational or topological data.\"]},\"87\":{\"h\":\"Graph\",\"t\":[\"Graph Data: A graph G=(V,E) consists of:\",\"Nodes/Vertices (V): Represent entities (e.g., users, molecules, or cities).\",\"Edges (E): Represent relationships or interactions between nodes.\",\"Graph (G): The entire graph structure.\",\"Node/Edge/Graph Features: Optional attributes associated with nodes, edges, and the graph (global feature or master node).\",\"We can additionally specialize graphs by associating directionality to edges (directed, undirected).\",\"Graph representation\",\"We use an adjacency matrix to describe a graph. However, it can be inefficient when the scale of the graph is huge (e.g., for n nodes, the scale of the matrix is (n2,n2)). So, an adjacency list can be a great option. We avoid computation and storage on the disconnected parts of the graph.\",\"Graph attributes representation\",\"Each node and edge has its feature vector.\",\"The key feature of a graph is its permutation invariance. Permutation invariance refers to a property where the output of a function remains unchanged when the input's order is rearranged. For example, summation is permutation invariant because the order of elements doesn't affect the result (e.g., 1+2+3=3+1+2=2+1+3). This requires GNNs to perform optimizable transformations on all attributes of the graph (nodes, edges, global-context) that preserve graph symmetries (permutation invariances).\"]},\"88\":{\"h\":\"Task\",\"t\":[\"GNNs have three levels of tasks: node-level, edge-level, and graph-level.\",\"Node-level prediction problems are analogous to image segmentation, where we are trying to label the role of each pixel in an image. With text, a similar task would be predicting the parts-of-speech of each word in a sentence (e.g., noun, verb, adverb, etc.).\",\"Edge-level problems predict which of these nodes share an edge or what the value of that edge is.\",\"Graph-level tasks are analogous to image classification problems with MNIST and CIFAR, where we want to associate a label to an entire image. With text, a similar problem is sentiment analysis, where we want to identify the mood or emotion of an entire sentence at once.\",\"Graphs have up to four types of information that we will potentially want to use to make predictions: nodes, edges, global-context, and connectivity. After several iterations, we can apply a classification layer to each of its information (feature vector) to predict.\",\"An end-to-end prediction task with a GNN model\"]},\"89\":{\"h\":\"Message Passing\",\"t\":[\"Neighboring nodes or edges exchange information and influence each other’s updated embeddings.\",\"Message passing works in three steps:\",\"For each node in the graph, gather (concatenate) all the neighboring node embeddings (or messages).\",\"Aggregate all messages via an aggregate function (like sum).\",\"All pooled messages are passed through an update function, usually a learned neural network.\",\"This is the node-node message passing process.\",\"Node-node message passing\",\"Node-node message passing\",\"In the picture, ρ is the pooling process (step 1 and step 2 in message passing).\",\"We also have node-level, node-graph, edge-graph message passing patterns and vice versa.\",\"Node-edge-node\",\"Which graph attributes we update and in which order we update them is one design decision when constructing GNNs. We could choose whether to update node embeddings before edge embeddings or the other way around. This is an open area of research with a variety of solutions–for example, we could update in a ‘weave’ fashion, where we have four updated representations that get combined into new node and edge representations: node to node (linear), edge to edge (linear), node to edge (edge layer), edge to node (node layer).\",\"Weave node-edge-node, edge-node-edge\",\"When we want to add a global representation (graph feature), one solution to this problem is to add a master node or context vector, which is virtually proposed. This global context vector is connected to all other nodes and edges in the network and can act as a bridge between them to pass information, building up a representation for the graph as a whole. This creates a richer and more complex representation of the graph than could have otherwise been learned.\",\"So, we have node, edge, and global representations. We can choose which of them to aggregate and update in the iterations.\",\"Conditioning information\"]},\"90\":{\"h\":\"Key GNN Architectures\"},\"91\":{\"h\":\"GCN\",\"t\":[\"A Graph Convolutional Network (GCN) is a neural network architecture designed to process graph-structured data. It extends convolutional operations to irregular graphs by aggregating features from a node's local neighborhood. GCNs are widely used for tasks like node classification, link prediction, and graph classification.\"]},\"92\":{\"h\":\"Key Components:\",\"t\":[\"Graph Structure:\",\"Adjacency matrix A∈RN×N, where N is the number of nodes.\",\"Degree matrix D, where Dii​=∑j​Aij​.\",\"Node feature matrix X∈RN×F, where F is the feature dimension.\",\"Self-Loops: Add self-connections to A to include a node’s own features:\",\"A~=A+I\",\"Normalized Adjacency Matrix:\",\"A^=D~−21​A~D~−21​\",\"where D~ is the degree matrix of A~.\"]},\"93\":{\"h\":\"Layer-wise Propagation Rule\",\"t\":[\"The output H(l+1) of the (l+1)-th GCN layer is computed as:\",\"H(l+1)=σ(A^H(l)W(l))\",\"H(l): Input features of the l-th layer (H(0)=X).\",\"W(l): Trainable weight matrix.\",\"σ: Activation function (e.g., ReLU).\",\"Example\",\"Step 1: Define Inputs\",\"Adjacency Matrix (3 nodes):\",\"A=​011​100​100​​,A~=A+I=​111​110​101​​\",\"Node Features (X∈R3×2):\",\"X=​101​011​​\",\"Step 2: Compute Normalized Adjacency Matrix\",\"Degree MatrixD~:\",\"D~=diag(3,2,2)\",\"Normalized A^ (values rounded for simplicity):\",\"A^=D~−21​A~D~−21​≈​0.330.410.41​0.410.50​0.4100.5​​\",\"Step 3: Apply GCN Layer\",\"Assume W(0)=[1−1​−11​] and ReLU activation:\",\"H(1)=ReLU(A^XW(0))=ReLU​​0.330.410.41​0.410.50​0.4100.5​​​101​011​​[1−1​−11​]​\",\"After matrix multiplication and ReLU, the output features are transformed.\"]},\"94\":{\"h\":\"GAT\",\"t\":[\"A Graph Attention Network (GAT) is a neural network architecture designed to process graph-structured data, similar to GCNs. However, GAT introduces an attention mechanism to weigh the importance of neighboring nodes dynamically. This allows the model to focus on more relevant neighbors during feature aggregation, making it more flexible and expressive than GCNs.\"]},\"95\":{\"h\":\"Key Components:\",\"t\":[\"Graph Structure:\",\"Adjacency matrix A∈RN×N, where N is the number of nodes.\",\"Node feature matrix X∈RN×F, where F is the feature dimension.\",\"Attention Mechanism: Computes attention coefficients between nodes to weigh their contributions during aggregation.\"]},\"96\":{\"h\":\"Attention Mechanism\",\"t\":[\"For a node i and its neighbor j, the attention coefficient eij​ is computed as:\",\"eij​=LeakyReLU(aT[Whi​∥Whj​])\",\"hi​,hj​: Feature vectors of nodes i and j.\",\"W∈RF′×F: Shared weight matrix for feature transformation.\",\"a∈R2F′: Weight vector for the attention mechanism.\",\"∥: Concatenation operation.\",\"LeakyReLU: Nonlinear activation function.\"]},\"97\":{\"h\":\"Normalized Attention Coefficients\",\"t\":[\"The attention coefficients are normalized using the softmax function:\",\"αij​=softmaxj​(eij​)=∑k∈Ni​​exp(eik​)exp(eij​)​\",\"Ni​: Set of neighbors of node i.\"]},\"98\":{\"h\":\"Feature Aggregation\",\"t\":[\"The output feature hi′​ for node i is computed as a weighted sum of its neighbors' features:\",\"hi′​=σ​j∈Ni​∑​αij​Whj​​\",\"σ: Nonlinear activation function (e.g., ReLU).\",\"Example\",\"Step 1: Define Inputs\",\"Adjacency Matrix (3 nodes):\",\"A=​011​100​100​​\",\"Node Features (X∈R3×2):\",\"X=​101​011​​\",\"Step 2: Compute Attention Coefficients\",\"Assume W=[1−1​−11​] and a=[1,−1]:\",\"For node 1 and its neighbors (nodes 2 and 3):\",\"e12​=LeakyReLU([1,−1]T[Wh1​∥Wh2​])\",\"e13​=LeakyReLU([1,−1]T[Wh1​∥Wh3​])\",\"Normalize using softmax:\",\"α12​=exp(e12​)+exp(e13​)exp(e12​)​\",\"α13​=exp(e12​)+exp(e13​)exp(e13​)​\",\"Step 3: Aggregate Features\",\"Compute the output feature for node 1:\",\"h1′​=ReLU(α12​Wh2​+α13​Wh3​)\"]},\"99\":{\"h\":\"GraphSAGE\",\"t\":[\"GraphSAGE is a framework for inductive representation learning on large graphs. Unlike GCNs, which require the entire graph during training, GraphSAGE generates node embeddings by sampling and aggregating features from a node's local neighborhood. This makes it scalable to large graphs and capable of generalizing to unseen nodes.\"]},\"100\":{\"h\":\"Key Components:\",\"t\":[\"Graph Structure:\",\"Adjacency matrix A∈RN×N, where N is the number of nodes.\",\"Node feature matrix X∈RN×F, where F is the feature dimension.\",\"Neighborhood Sampling: For each node, a fixed-size subset of neighbors is sampled to reduce computational complexity.\",\"Aggregation Functions: Combines features from a node's neighbors. Common choices include mean, LSTM, or pooling aggregators.\"]},\"101\":{\"h\":\"Layer-wise Propagation Rule\",\"t\":[\"For each node v, the embedding hv(k)​ at the k-th layer is computed as:\",\"Aggregate Neighbor Features\",\"hN(v)(k)​=AGGREGATE({hu(k−1)​,∀u∈N(v)})\",\"N(v): Sampled neighbors of node v.\",\"AGGREGATE: Aggregation function (e.g., mean, LSTM, max-pooling).\",\"Combine Features:\",\"hv(k)​=σ(W(k)⋅[hv(k−1)​∥hN(v)(k)​])\",\"W(k): Trainable weight matrix.\",\"∥: Concatenation operation.\",\"σ: Nonlinear activation function (e.g., ReLU).\"]},\"102\":{\"h\":\"Output Embedding\",\"t\":[\"After K layers, the final embedding for node v is:\",\"zv​=hv(K)​\",\"GraphSAGE\"]},\"103\":{\"h\":\"Summary\",\"t\":[\"In summary, GNNs differ in which components exchange information with each other, the aggregation function, and the update function.\",\"Types of GNN\"]},\"104\":{\"h\":\"Code\",\"t\":[\"Here we write codes that can randomly produce several graph alongside their features. We use networkx to visualize. For better understand the calculation process, we preset the parameters to be identity matrix.\",\"input graph\",\"result\",\"import networkx as nx import matplotlib.pyplot as plt import torch import torch.nn as nn class GCNs(nn.Module): def __init__(self, num_feature_in, num_feature_out): super(GCNs, self).__init__() # self.W = torch.nn.Parameter(torch.randn([num_feature_in, num_feature_out])) self.W = torch.nn.Parameter(torch.eye(num_feature_in, num_feature_out)) def forward(self, adj_matrix, node_feature): ''' if the input is not in batch, we need to add a dimension(batch dimension) to the input. ''' if len(adj_matrix.shape) == 2: adj_matrix = torch.unsqueeze(adj_matrix, dim=0) if len(node_feature.shape) == 2: node_feature = torch.unsqueeze(node_feature, dim=0) # make sure the input is float type. adj_matrix = adj_matrix.float() node_feature = node_feature.float() # number of nodes num_nodes = adj_matrix.shape[1] A = adj_matrix + torch.eye(num_nodes) A = A.float() # calculate degree matrix D = torch.diag_embed(A.sum(dim=-1)) D = D.float() D_sqrt = D.pow(-0.5) D_sqrt[D_sqrt.isinf()] = 0 H_t1 = torch.bmm(D_sqrt, A) H_t1 = torch.bmm(H_t1, D_sqrt) H_t1 = torch.bmm(H_t1, node_feature) H_t1 = torch.nn.functional.relu(torch.matmul(H_t1, self.W)) return H_t1 # input batch_size = 4 num_node = 6 num_feature = 2 # create upper triangular part of the adjacent matrix map_input_n = torch.triu(torch.randint(0, 2, (batch_size, num_node, num_node))) map_input_n = map_input_n + map_input_n.transpose(-2, -1) map_input_n.diagonal(dim1=1, dim2=2).zero_() # diagonal part = 0 print(map_input_n) # node feature map_input_feature_n = torch.randint(0, 3, (batch_size, num_node, num_feature)) # visualize graph = [nx.Graph(i.numpy()) for i in map_input_n] # list conatins nxgraph fig, axe = plt.subplots(2, 2) axe = axe.flatten() # flatten for better index for i in range(batch_size): ax = axe[i] for j in range(num_node): # add feature vector to every node graph[i].nodes[j]['feature_vector'] = map_input_feature_n[i][j][:].numpy() nx.draw(graph[i], ax=ax, with_labels=True, font_size=12, font_color='black', node_color='lightblue', edge_color='gray') ax.set_title(f\\\"Graph {i + 1}\\\") plt.tight_layout() plt.show() # output net = GCNs(num_feature, num_feature) with torch.no_grad(): print(f\\\"Your input is: \\\\n {map_input_feature_n}\\\") output = net(map_input_n, map_input_feature_n) print(f\\\"Your output is: \\\\n {output}\\\") \"]},\"105\":{\"c\":[\"ML\"]},\"106\":{\"h\":\"Log-Derivative Trick\"},\"107\":{\"c\":[\"ML\"]},\"108\":{\"h\":\"机器学习\",\"t\":[\"本节整理机器学习的基本问题\"]},\"109\":{\"h\":\"基本概念\",\"t\":[\"magnitude: 幅度 order of magnitude：数量级 converge: 收敛 oscillate: 振荡 fit: 拟合 fine-tune: 调参 generalization ability：泛化能力 dimension: 量纲 deviation：偏离\"]},\"110\":{\"h\":\"一、绪论\",\"t\":[\"什么是机器学习和深度学习？ 机器学习是一种实现人工智能的方法。人工智能是想要达成的目标，而机器学习是想要达成目标的手段：希望机器通过学习的手段，可以跟人一样聪明。机器学习是研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构，使不断改善自身的性能。而深度学习，就是机器学习的其中一种方法。\",\"机器学习流程？ **表示 represent：**将数据对象进行特征(feature)化表示\",\"**训练 train：**给定一个数据样本集，从中学习出规律（模型），目标：该规律不仅适用于训练数据，也适用于未知数据(称为泛化能力)\",\"**测试 test：**对于一个新的数据样本，利用学到的模型进行预测\",\"在机器学习中，学习率这种参数叫什么？学习率太大和太小的可能影响？\",\"学习率(learning rate)也叫步长，指更新参数步幅。表征了参数每次更新的幅度（represent the magnitude of parameter update）\",\"If the learning rate is too large, the gradient descent algorithm will not converge and will diverge or oscillate; if the learning rate is too small, the gradient descent algorithm will converge very slowly.\",\"根据学习方式的划分，机器学习三个主要分类是什么？请简要说明他们之间的关系。 划分为三个主要分类：监督学习、非监督学习、强化学习。关系见下\",\"简述监督学习，非监督学习以及强化学习的定义和区别？ 定义：\",\"（1）监督学习：是一种通过使用已知输出来训练模型的学习方式。在监督学习中，训练数据包括输入数据和对应的输出数据（也称为标签或目标），算法通过学习这些数据，建立输入和输出之间的映射关系，以预测新的输入数据的输出。监督学习通常用于分类（分类器）和回归（回归器）问题。\",\"（2）非监督学习：是一种在没有标签或目标的情况下，从数据中发现模式或结构的学习方式。在非监督学习中，算法只能使用输入数据进行学习，目标是找到输入数据之间的相似性和区别，以便对数据进行聚类、降维、异常检测等操作。\",\"（3）强化学习：又称为再励学习，是指从环境状态到行为映射的学习，使系统行为从环境中获得的累积奖励值最大的一种机器学习方法。\",\"区别：\",\"（1）监督学习有反馈，无监督学习无反馈，强化学习执行多步后反馈；\",\"（2）强化学习的目标与监督学习目标不同，强化学习看重行为序列下的长期收益，监督学习关注与标签或已知输出的误差；\",\"（3）强化学习的奖惩概念没有正确和错误之分，而监督学习的标签是正确的。 强化学习是一个学习+决策的过程，有和环境交互的能力，监督学习不具备。\",\"简要说明监督学习和非监督学习之间的区别，并分别给出监督和非监督学习的两种算法。\",\"区别\",\"监督学习和非监督学习是机器学习中两种不同的学习方式。监督学习需要已知的输入和输出数据，目标是学习输入 和输出之间的映射关系。非监督学习只需要输入数据，目标是从数据中发现模式和结构，而不需要预先定义的目标 变量。在实际应用中，监督学习和非监督学习常常结合使用，以提高机器学习的效果和性能。\",\"举例\",\"常见的监督学习算法包括线性回归（linear regression）、逻辑回归(logistic regression)、决策树(decision tree)、支持向量机(support vector machine)、朴素贝叶斯、神经网络等。\",\"常见的非监督学习算法包括聚类、主成分分析（Principle Component Analysis）、独立成分分析（ICA）、自编码器、变分自编码器等。\",\"训练集、测试集、验证集区别联系？\",\"训练集:用于模型拟合的数据样本,即用于训练的样本集合,主要用来训练神经网络中的参数; train\",\"验证集：模型训练过程中单独留出的样本集，它可以用于调整模型的超参数和用于对模型的能力进行初步评估; tuning\",\"测试集：用来评估模最终模型的泛化能力。但不能作为调参、选择特征等算法相关的选择的依据。 evaluate\",\"各数据集的作用\",\"训练集的作用\",\"拟合模型，调整网络权重。\",\"验证集的作用\",\"作用 1：快速调参，也就是通过验证集我们可以选择超参数（网络层数、网络节点数、迭代次数 epoch、学习率 learning rate、优化器）等。\",\"作用 2：选择超参数，为了让我们的模型在测试集表现得更好，调参是不可避免地一部分，如果把测试集当验证集，调参去拟合测试集合，是不可行地，这相当于作弊。\",\"作用 3：监控模型是否正常\",\"验证集的重要性\",\"如果没有设置验证集，我们通常得等到测试集才可以知道我们模型真正得实力，然后再来调整参数，这样子时间代价较高，通过验证集我们可以训练几个 epoch 后查看模型的训练效果及我们的网络是否出现异常，然后决定怎么调整我们的超参数。\",\"测试集的作用\",\"仅仅用来评估模最终模型的泛化能力，确认网络的实际预测能力。\",\"什么是泛化性能？（generalization ability） 是指机器学习算法对新鲜样本的适应能力。 学习的目的是学到隐含在数据背后的规律，对具有同一规律的学习集以外的数据，经过训练的网络也能给出合适的输出，该能力称为泛化能力。\",\"在数据处理时，为什么通常要进行标准化处理 在实际问题中，我们使用的样本通常是多维数据，每一维对应一个特征，这些特征的量纲和数量级都是不一样的 这时需要对数据进行标准化处理，使得所有的特征具有同样的尺度。\"]},\"111\":{\"h\":\"二、模型评估与选择\",\"t\":[\"过拟合、欠拟合定义、原因、解决办法？\",\"（1）定义：\",\"过拟合：学习器把训练样本学习的“太好”，将训练样本本身的特点当做所有（潜在）样本（都会具有）的一般性质，导致泛化性能下降。\",\"欠拟合：训练样本的一般性质尚未学好，在训练样本上都存在较大的经验误差。\",\"（2）原因：\",\"过拟合原因： 1）模型复杂度过低 2）特征量过少 3）训练样本过少\",\"欠拟合原因： 1）训练集的数量级 N 与模型复杂度 M 不匹配 2）训练集与测试集的特征分布不一致 3）样本噪声过多，模型过分记住了噪声，忽略了真实的输入输出 4）权值学习迭代次数过多，拟合了不具代表性的特征\",\"（3）解决办法：\",\"过拟合: 1）清洗数据 2）减小模型复杂度 3）增广训练集 4）交叉验证 5）正则化 regularization，约束模型特征 6）early stopping 迭代次数截断 7）dropout，让一些神经元以一定的概率不工作.\",\"欠拟合应对： 1）增加新特征 2）增加模型复杂度，如决策树的扩展分支，神经网络的训练轮数等 3）扩大训练集。\",\"错误率及误差概念？\",\"错误率 & 误差：\",\"（1）错误率：错分样本的占比\",\"（2）误差：样本真实输出与预测输出之间的差异\",\"（3）训练(经验)误差： 训练集上\",\"（4）测试误差： 测试集上\",\"（5）泛化误差： 除训练集外所有样本\",\"评估方法\",\"留出法、交叉验证法（k=m 时是留一法）、自助法等。\",\"交叉验证法和自助法异同？\",\"相同点\",\"交叉验证法和自助法都是随机采样法。它们作为人工智能中评估模型的方法，根据一定规则从数据集 D 中划分训练集和测试（验证）集，从而评价模型在数据集上的表现，便于我们选择合适的模型。\",\"不同点\",\"这两种方法最大的不同点在于每次划分过程中每个样本点是否只有一次被划入训练集或测试集的机会。下面将针对这方面详细展开论述：\",\"交叉验证法采用的是无放回的随机采样方式，这种方式可以保持数据分布的一致性条件，并严格划分训练集与测试集的界限，从而增强测试评估的稳定性和可靠性。\",\"自助法主要面向数据集同规模的划分问题。其采用的是有放回的随机抽样方法，可以使得得到的模型更为稳健，解决了交叉验证法中模型选择阶段和最终模型训练阶段的训练集规模差异问题；但训练集 T 和原始数据集 D 中数据的分布未必相一致，因此对一些对数据分布敏感的模型选择并不适用。\",\"偏差、方差、噪声含义？\",\"偏差度量了学习算法期望预测与真实结果的偏离程度: 即刻画了学习法本身的拟合能力;\",\"方差度量了同样大小训练集的变动所导致的学习性能的变化: 即刻画了数据扰动所造成的影响;\",\"噪声表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界即刻画了学习问题本身的难度。\",\"Bias measures the degree of deviation between the expected prediction of the learning algorithm and the actual result: it depicts the fitting ability of the learning method itself;\",\"Variance measures the change in learning performance caused by changes in the same size training set: it depicts the impact of data perturbations;\",\"Noise expresses the lower bound of the expected generalization error that any learning algorithm can achieve on the current task, which depicts the difficulty of the learning problem itself.\",\"偏差-方差分解角度解释泛化性能 泛化性能是出学习算法的能力、数据的充分性以及学习任务本身的难度所共同决定的。给定学习任务为了取得好的泛化性能，需要使偏差小(充分拟合数据)而且方差较小(减少数据扰动产生的影响)。\",\"偏差方差冲突\",\"请给出你对泛化误差的理解\",\"泛化误差 = 偏差+方差+噪声\",\"偏差：度量了学习算法的期望预测与真实结果的偏离程度，刻画了学习算法本身的拟合能力\",\"方差：度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响\",\"噪声：表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度\"]},\"112\":{\"h\":\"三、线型模型\",\"t\":[\"线型模型优势与不足？ 优势：第三章 3.1.2 P1 不足： 第三章 3.2.7 P10\",\"简述 LDA 算法的基本思想及算法流程。\",\"基本思想是将高维的模式样本投影到最佳鉴别矢量空间，以达到抽取分类信息和压缩特征空间维数的效果，投影后保证模式样本在新的子空间有最大的类间距离和最小的类内距离,即模式在该空间中有最佳的可分离性。 流程：\",\"逻辑回归和线性回归的异同\",\"不同之处：\",\"（1）逻辑回归解决的是分类问题，因此因变量是离散的；而线性回归解决的是回归问题，因此因变量是连续的。这是两者最本质的区别；\",\"（2）在自变量和超参数确定的情况下逻辑回归可看作广义的线性模型在因变量下服从二元分布的一个特殊情况\",\"（3）使用最小二乘法求解线性回归时我们认为因变量服从正态分布。\",\"相同之处：\",\"（1）二者在求解超参数的过程中都使用梯度下降的方法 ；\",\"（2）二者都使用了极大似然估计对训练样本进行建模。\"]},\"113\":{\"h\":\"四、决策树\",\"t\":[\"决策树三种导致递归返回的情况\",\"（1）当前结点包含的样本全属于同一类别，无需划分，直接把该结点做为叶结点，类别划分为该结点下所有样本同属的类别；\",\"（2）当前属性集为空，或者所有样本在所有属性上取值相同，无法划分，直接把该结点做为叶结点，类别划分为该结点下所有样本中出现次数最多的类别；\",\"（3）当前结点包含的样本集合为空集，不能划分，直接把该结点做为叶结点，类别划分为父结点中出现次数最多的类别。\",\"决策树中剪枝方式分为哪两种？请简述这两种方式的优缺点？\",\"预剪枝（prepruning）和后剪枝(postpruning)。\",\"预剪枝\",\"优点：降低过拟合风险，显著降低训练时间和测试时间的开销。\",\"缺点：有些分支当前划分虽然不能提升泛化性能，但在其基础上进行的后续划分有可能使得性能显著提高，预剪枝基于“贪心”，禁止这些分支展开，有欠拟合风险。\",\"后剪枝\",\"优点：比预剪枝保留了更多分支，欠拟合风险小，泛化性能更好。\",\"缺点：训练时间开销大，后剪枝过程是在生成完全的决策树之后，自底向上对所有非叶节点逐一考察。\"]},\"114\":{\"h\":\"五、神经网络\",\"t\":[\"在神经网络中，非线性激活函数的主要作用是什么？请给出常用的几种非线性激活函数及其导数；\",\"激活函数（activation function）可以加入非线性因素，解决线性模型所不能解决的问题。激活函数是神经网络的一个重要组成部分。如果不使用激活函数，则无论该神经网络有多少层，最终的输出都是输入的线性组合，与没有隐藏层的效果相当，这种情况就是最原始的感知机（perceptron），无法解决线性不可分问题。\",\"神经网络分类\",\"神经网络根据是否存在网络回路（联接方式），可以分为：前向型和反馈型。 按学习方式：有导师的学习（监督学习）、无导师的学习（无监督学习）、再励学习（强化学习）\",\"简述神经网络的学习过程\",\"神经网络的学习过程主要基于反向传播算法，分为前向传播和反向传播两个阶段。在前向传播中，神经网络计算加权输入并应用激活函数得到输出。在反向传播中，通过计算网络输出与实际输出之间的误差，将误差反向传播到每个参数，更新权重和偏置项，优化网络的预测能力。\",\"简要介绍卷积概念及其作用、池化的作用是什么？卷积前后图像尺寸之间的关系是什么？\",\"见\",\"简述神经网络中梯度下降方法的原理和作用（作用请从机器学习训练阶段的三个步骤的角度来阐述）。\",\"梯度下降方法的原理：梯度下降方法通过求出损失函数在某点对于参数 θ 的微分值，并以负梯度方向为搜索方向，沿着梯度下降的方向求解极小值；作用是在训练阶段的第三个步骤中，通过梯度下降来寻找更优的学习参数 b 和 w，达到优化模型的效果。\"]},\"115\":{\"h\":\"六、支持向量机 SVM\",\"t\":[\"试述硬间隔、软间隔、基于核函数的 SVM 的原理、优缺点、三者最终计算方式以及限制条件。 硬间隔 SVM：\",\"原理：硬间隔 SVM 假设数据本身是线性可分的，即存在一个超平面可以将不同类别的样本完全分开。这个超平面需要满足离其最近的点到其的距离最大化。\",\"优点：简单明了，适用于线性可分的数据集。\",\"缺点：对于非线性可分的数据集，硬间隔 SVM 无法找到一个有效的超平面。此外，硬间隔 SVM 对异常点非常敏感，因为异常点可能导致无法找到一个满足所有约束条件的超平面。\",\"软间隔 SVM：\",\"原理：软间隔 SVM 放松了对数据线性可分的假设，允许在某些情况下出现分类错误。软间隔 SVM 通过引入松弛变量来处理噪声和异常点。通过调整这些变量，可以控制对分类错误的容忍程度。\",\"优点：能够处理非线性可分的数据集和噪声数据。\",\"缺点：需要调整松弛变量和惩罚参数，以找到最佳的分类效果。\",\"基于核函数的 SVM：\",\"原理：对于非线性可分的数据集，基于核函数的 SVM 通过使用核函数将输入空间映射到高维特征空间，使得在高维特征空间中数据变得线性可分。常用的核函数有线性核、多项式核和 RBF 核（高斯核）。\",\"优点：能够处理非线性可分的数据集。\",\"缺点：选择合适的核函数和参数是一个挑战，同时计算复杂度可能会较高。\",\"SVM 与 logistic 回归区别联系\",\"相同点:\",\"都是分类算法 如果不考虑核函数，LR 和 SVM 都是线性分类算法 LR 和 SVM 都是监督学习算法。 LR 和 SVM 都是判别模型.\",\"不同点:\",\"本质上是其 loss function 不同 支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局. 在解决非线性问题时，支持向量机采用核函数的机制，而 LR 通常不采用核函数的方法 线性 SVM 依赖数据表达的距离测度，所以需要对数据先做 normalization，LR 不受其影响。 SVM 的损失函数就自带正则。\",\"SVM、logistic 回归、决策树各自优缺点\",\"逻辑回归\",\"逻辑回归的优点：\",\"（1）便利的观测样本概率分数；\",\"（2）已有工具的高效实现；\",\"（3）对逻辑回归而言，多重共线性并不是问题，它可以结合 L2 正则化来解决；\",\"（4）逻辑回归广泛的应用于工业问题上（这一点很重要）。\",\"逻辑回归的缺点： （1）当特征空间很大时，逻辑回归的性能不是很好；\",\"（2）不能很好地处理大量多类特征或变量；\",\"（3）对于非线性特征，需要进行转换；\",\"（4）依赖于全部的数据（个人觉得这并不是一个很严重的缺点）。\",\"决策树\",\"（1）直观的决策规则\",\"（2）可以处理非线性特征\",\"（3）考虑了变量之间的相互作用\",\"（4）训练集上的效果高度优于测试集，即过拟合[随机森林克服了此缺点\",\"（5）没有将排名分数作为直接结果\",\"SVM\",\"SVM 的优点：\",\"（1）能够处理大型特征空间\",\"（2）能够处理非线性特征之间的相互作用\",\"（3）无需依赖整个数据\",\"SVM 的缺点：\",\"（1）当观测样本很多时，效率并不是很高\",\"（2）有时候很难找到一个合适的核函数\",\"为什么要引入对偶问题? （1）对偶问题将原始问题中的约束转为了对偶问题中的等式约束。\",\"（2）改变了问题的复杂度。由求特征向量转化为求比例系数。在原始问题下，求解的复杂度与样本的维度有关即 w 的维度。在对偶问题下，求解的是 a，复杂度只与样本数量有关。\",\"（3）可以自然地引入核函数，从而推广到非线性分类问题\"]},\"116\":{\"h\":\"七、聚类 Cluster\",\"t\":[\"聚类方法分类？\",\"无监督学习方法主要有两大类：基于概率密度函数的估计方法和基于样本间相似性度量的间接聚类方法。\",\"K-means、层次聚类、DBSCAN 聚类方法原理、区别、优缺点\",\"K-means 聚类\",\"工作原理： K-means 算法将数据划分为 K 个簇，每个簇包含最接近其质心的数据点。它通过迭代地将数据点分配给最近的质心并更新质心来执行聚类。\",\"优点： 简单且高效，适用于大型数据集。它的结果易于解释和可视化。\",\"缺点：需要事先指定簇数 K。对于非球形簇或具有不同密度的簇效果较差。\",\"层次聚类\",\"工作原理： 层次聚类将数据集逐渐分割或合并成不同的层次簇。它可以是自底向上的聚合聚类（凝聚型）或自顶向下的分裂聚类（分裂型）。\",\"优点： 不需要预先指定簇数，可视化结果以树状结构呈现，对不同形状的簇和噪声具有较好的鲁棒性。\",\"缺点：不具有很好的可伸缩性: 时间复杂性至少是 0(n^2)，其中 n 对象总数。合并或分裂的决定需要检查和估算大量的对象或簇。不能撤消已做的处理，聚类之间不能交换对象。如果某一步没有很好地选择合并或分裂的决定，可能会导致低质量的聚类结果\",\"DBSCAN（密度聚类）\",\"工作原理： DBSCAN 根据数据点的密度将它们分为核心点、边界点和噪声点。核心点是在指定半径范围内有足够多邻居的点，它们被用于扩展簇。\",\"优点： 可以对任意形状的稠密数据集进行聚类。可以在聚类的同时发现异常点，对数据集异常点不敏感。聚类结果没有偏倚。\",\"缺点： 对参数的选择敏感，可能需要调整半径参数和最小邻居数。对高维数据和不均匀密度数据的处理相对困难。样本集较大时，聚类收敛时间较长。\",\"区别：\",\"（1）簇数的预先指定： K-means 需要提前指定簇数 K，而层次聚类和 DBSCAN 不需要。层次聚类会生成层次结构，可以根据需要切割簇。DBSCAN 通过密度自动确定簇的数量。\",\"（2）对簇形状和密度的适应性： K-means 假定簇是球形且密度均匀，不适合不规则形状和不同密度的簇。层次聚类和 DBSCAN 能够发现各种形状和密度的簇。\",\"（3）计算复杂度： K-means 通常是最快的，DBSCAN 次之，而层次聚类较慢，特别是在大型数据集上。\",\"（4）噪声处理： DBSCAN 在处理噪声点时比较鲁棒，可以将它们识别为噪声。K-means 和层次聚类通常需要额外的后处理步骤来处理噪声点。\"]},\"117\":{\"h\":\"八、降维\",\"t\":[\"请简要说明主成分分析（PCA）和线性判别分析（LDA）之间的区别和联系\",\"相同点：\",\"（1）两者均可以对数据进行降维。\",\"（2）两者在降维时均使用了矩阵特征分解的思想。\",\"（3）两者都假设数据符合高斯分布。\",\"不同点：\",\"（1）LDA 是有监督的降维方法，而 PCA 是无监督的降维方法\",\"（2）LDA 降维最多降到类别数 k-1 的维数，而 PCA 没有这个限制。\",\"（3）LDA 除了可以用于降维，还可以用于分类。\",\"（4）LDA 选择分类性能最好的投影方向，而 PCA 选择样本点投影具有最大方差的方向。\"]},\"118\":{\"h\":\"Further Reading: Questions in English\"},\"119\":{\"h\":\"1. What is Regularization?\",\"t\":[\"Regularization is a technique used in machine learning to prevent overfitting by adding a penalty（惩罚项） to the model's complexity. It works by incorporating additional terms to the loss function, such as L1 (Lasso)（L1 正则化） or L2 (Ridge)（L2 正则化） penalties, which constrain the magnitude of the model parameters. This helps in reducing the variance without substantially increasing the bias, leading to better generalization on unseen data.\"]},\"120\":{\"h\":\"2. What is Cross Validation?\",\"t\":[\"Cross validation is a technique for assessing the performance and robustness of a machine learning model. It involves partitioning the dataset into training and validation sets multiple times to ensure that the model's performance is evaluated on different subsets of data. The most common method is k-fold cross validation（k 折交叉验证）, where the dataset is divided into k equally sized folds, and the model is trained and validated k times, each time using a different fold as the validation set and the remaining folds as the training set. This helps in obtaining a more reliable estimate of model performance.\"]},\"121\":{\"h\":\"3. PCA Principle\",\"t\":[\"Principal Component Analysis (PCA)（主成分分析） is a dimensionality reduction technique that transforms the data into a new coordinate system such that the greatest variances by any projection of the data come to lie on the first coordinates (called principal components（主成分）). It achieves this by calculating the eigenvectors（特征向量） and eigenvalues（特征值） of the data's covariance matrix. The principal components are orthogonal to each other, and by selecting the top k principal components, we can reduce the dimensionality of the data while preserving most of its variance.\"]},\"122\":{\"h\":\"4. K-Means Principle\",\"t\":[\"K-Means（K 均值） is a clustering algorithm that partitions a dataset into k distinct, non-overlapping subgroups (clusters（簇）). It works by initializing k centroids（质心） randomly, assigning each data point to the nearest centroid, and then updating the centroids to be the mean of the data points assigned to them. This process iterates until the centroids no longer change significantly. The goal is to minimize the within-cluster variance, resulting in compact and well-separated clusters.\"]},\"123\":{\"h\":\"5. Support Vector Machine Principle\",\"t\":[\"Support Vector Machine (SVM)（支持向量机） is a supervised learning algorithm used for classification and regression tasks. The principle of SVM is to find the optimal hyperplane（超平面） that best separates the data into different classes. This hyperplane is defined by support vectors（支持向量）, which are the data points closest to the hyperplane. SVM aims to maximize the margin（间隔）, which is the distance between the hyperplane and the support vectors. For non-linearly separable data, SVM uses kernel functions（核函数） to transform the data into a higher-dimensional space where a linear separator can be found.\"]},\"124\":{\"h\":\"6. Decision Tree Principle\",\"t\":[\"A Decision Tree（决策树） is a supervised learning algorithm used for both classification and regression tasks. It works by recursively splitting the data based on feature values to create a tree structure, where each internal node represents a decision based on a feature, each branch represents the outcome of the decision, and each leaf node represents a class label or a continuous value. The splits are chosen to maximize the reduction in impurity（杂质）, commonly measured by metrics like Gini impurity（基尼杂质） or information gain (entropy)（信息增益或熵）. The resulting model is a tree that predicts the target variable by traversing from the root to a leaf node based on the feature values of the input data.\"]},\"125\":{\"c\":[\"ML\"]},\"126\":{\"h\":\"元胞自动机 Cellular Automata\",\"t\":[\"本节简单介绍一下元胞自动机模型，包括其定义、基本概念、应用等方面，具体讲解可以看这篇文章与这篇文章。\"]},\"127\":{\"h\":\"定义\",\"t\":[\"元胞自动机（Cellular Automata, CA）是自动机理论（Theory of automata）中的一种离散计算模型，最初由Stanislaw Ulam和John von Neumann 于 20 世纪 40 年代在洛斯阿拉莫斯国家实验室同时提出[1]。一个完整的元胞自动机模型包含 元胞、元胞空间、元胞邻居、元胞边界、元胞规则 五大部分，下面将分别做进一步阐述。\"]},\"128\":{\"h\":\"元胞\",\"t\":[\"元胞是 CA 模型的基本单元，是模型迭代的直接参与者，从概念上就可以理解元胞就好似生物体的细胞。每一个元胞都有一个状态,一般为二维（如 0-1），复杂情况下也有多维。\"]},\"129\":{\"h\":\"元胞空间\",\"t\":[\"元胞空间为空间内元胞的集合，即按一定方式对空间划分，元胞呈一定形状。元胞空间划分方式大致有 正方形（类似栅格化）、三角形、正六边形 等类型。\"]},\"130\":{\"h\":\"元胞邻居\",\"t\":[\"元胞邻居是某一元胞周围的元胞，是否为邻居，取决于元胞状态更新时所要搜索的空间域，在二维空间下，最常用的邻居类型是 Moore 型和 Von Neumann 型，如图一所示：\",\"图1 元胞类型\",\"Moore 邻居定义为下式：\",\"NM​(x0​,y0​)={(x,y):∣x−x0​∣<=r,∣y−y0​∣<=r}\",\"Von Neumann 邻居定义为：\",\"NV​(x0​,y0​)={(x,y):∣x−x0​∣+∣y−y0​∣<=r}\"]},\"131\":{\"h\":\"边界条件\",\"t\":[\"边界条件是元胞空间外的部分，是为了让最外围元胞能够有像内部元胞一样的邻域条件所创建的虚拟元胞。常用的邻居边界条件类型有：固定型，周期型，绝热型和映射型这四种，常用为固定型和周期型。\",\"注\",\"固定型：在外围补上固定不变的、虚拟的元胞。\",\"周期型：每个维度的第一个元胞与最后一个元胞互为边界。\",\"绝热型：边界元胞与自己相同。\",\"映射型：边界元胞为元胞每个维度内侧邻近元胞。 一般常用为固定型和周期型边界条件。\"]},\"132\":{\"h\":\"元胞规则\",\"t\":[\"元胞规则即每次迭代，每个元胞按照当前状态及周围邻居的状态来更新下一时刻该元胞状态，每个元胞按照该规则进行状态更新，相互作用，由局部到整体，从而引起全局的变化。元胞规则是整个 CA 模型最为关键的部分。\",\"相关信息\",\"元胞自动机更新规则特征[2]：\",\"离散型：空间、时间及状态都是离散的;\",\"同质性：服从相同的规律分布方式相同；\",\"并行性：元胞的状态更新规则变化是同步进行的；\",\"高维度：元胞自动机是一类无穷维动力系统。\"]},\"133\":{\"h\":\"生命游戏\",\"t\":[\"生命游戏是最著名的二维元胞自动机生命游戏，由John Conway于 1970 年设计。它由二维元胞网格组成，其状态可能是死亡 (0) 或活着 (1)。该游戏采用标准 Moore 邻居，其元胞规则为：\",\"对于“活着”的格子，若它的 8 个 Moore 邻居中有 2-3 个为“活着”，则该格继续保持“活着”，否则就变为“死亡”。 对于“死亡”的格子，若它的 8 个邻居中有 3 个“生”，则该格变为“生”，否则继续保持“死”。\",\"用函数表示如下：\",\"vt+1(α)=⎩⎨⎧​0,νt(α),1,​S<2∨S>3S=2S=3​\",\"图2 生命游戏\"]},\"134\":{\"h\":\"应用\",\"t\":[\"元胞自动机的应用大致有以下几类：\",\"作为物理、化学、生物过程的基础模型\",\"计算单元\",\"模拟现实复杂动态系统\",\"英文总结\",\"Cellular automaton is a discrete computing model in the Theory of automata. A complete cellular automaton model includes five parts: cell, cell space, cell neighbor, cell boundary, and cell rules. Each cell has a state which can be 0 or 1, alive or dead. Each cell follows a set of rules and updates the state at every time step based on the current state and their neighbors' state, thereby triggering global changes. It has many applications in computing and simulation.\"]},\"135\":{\"c\":[\"交通\"]},\"136\":{\"h\":\"改进NS模型\",\"t\":[\"本节介绍NS模型基本内容、改进NS模型以及对应代码的实现。\"]},\"137\":{\"h\":\"1 基本概念\"},\"138\":{\"h\":\"1.1 Nagel–Schreckenberg model NS模型\",\"t\":[\"Nagel–Schreckenberg model，简称NS model，最初由德国物理学家Kai Nagel and Michael Schreckenberg,是一种基于CA模型的用于交通仿真的理论模型。\",\"Nagel–Schreckenberg model, short for NS model, first develped by German physicists Kai Nagel and Michael Schreckenberg, is a theoretical Cellular Automata-based model for traffic simulation.\"]},\"139\":{\"h\":\"1.2 Rule 184\",\"t\":[\"补充一下基于CA交通仿真中最常见的规则，即 Rule 184，即对于一条道路上连续的三个cell，他们的状态有以下8种（用0代表空，用1代表有车占有）,如下图所示：\",\"Rule 184\"]},\"140\":{\"h\":\"1.3 Phantom traffic jam 幽灵拥堵\",\"t\":[\"定义：莫名发生的交通拥堵 traffic jam without clear reasons\",\"原因：在拥堵（heavy）交通流中，微小的交通扰动（small disturbances），如司机过度刹车或者与其他车里的太近，都会被放大形成(be amplified into)内生性的交通拥堵。 Small disturbances, such as overreations of braking or getting too close to another vehicle, in heavy traffic can be amplified into a self-sustained traffic jam.\",\"结果：形成stop-and-go wave\",\"Stop-And-Go Wave\",\"字面意是停走波\",\"形成原因：\",\"微观（从driver角度）：vehicle slowly move → decelerate → stop → accelerate →\",\"宏观，如下图Time-Space Diagram所示：\",\"Time-Space Diagram 时空图\",\"由上图可以看到stop-and-go wave沿车流末端传播的速度，即wave speed以及波次间隔时间wave period。\",\"这样的stop-and-go wave会造成以下危害：\",\"更费油与更多排放 more fuels and emmisions.\",\"更加危险 more dangerous.\"]},\"141\":{\"h\":\"1.4 基本图 Fundamental Diagram\",\"t\":[\"Fundamental Diagram 基本图\",\"Capacity/Maximum Flow 极大流率：qk曲线极值\",\"Free Flow Speed 畅行速度：k=0对应的速度\",\"Critical Density 临界密度：极大流率对应的密度\",\"Critical Speed 临界速度：极大流率对应的速度\",\"Jam Density 堵塞密度：v=0时的密度\"]},\"142\":{\"h\":\"2 Model Description 模型描述\"},\"143\":{\"h\":\"2.1 Model Information 模型说明\",\"t\":[\"空间时间上均离散化。 Discrete in space and time\",\"所有车辆同步更新。 Each vehicle is updated parallel\",\"每个元胞长7.5m（包括了车长+距离前车的安全距离）。 Lane is divided into cells of length 7.5 m, which includes the vehicle length and the safe distance to the preceeding/leading vehicle.\",\"每个元胞为空或仅被一辆车占据。 Each cell can either be empty or occupied by only one vehicle.\",\"每个车辆拥有坐标、速度属性，车辆有最大速度限制。 Each vehicle is characterized by its velocity and coordinates. The velocity has a limit.\"]},\"144\":{\"h\":\"2.2 Model Step 更新规则\",\"t\":[\"Step 1：加速 Acceleration：对于未到达最大速度的车辆，以一个单位加速。 For vehicle not reaching speed limit, it accelerates with 1 unit.\",\"v(i)→minv(i)+1,Vmax​\",\"这反应了司机希望开得越快越好 reflect the desire of drivers to drive as fast as possible.\",\"Step 2: 减速 Safe Distance Judgment：当汽车当前速度大于与前车的距离（前方的空元胞数），则汽车减速为安全距离，否则维持原速。 If vehicle speed is larger than the number of empty cells in front of it, it velocity reduces to that empty cell amount. Otherwise, it keeps its speed.\",\"v(i)→minv(i),d(i)\",\"It encodes the interaction between the vehicle. In this simple model, interactions only occur to avoid collision.\",\"Step 3：随机慢化 Randomization：车辆以p的概率随机减速一个单位，p称为随机慢化概率。 The speed of vehicle reduces by one unit with slowdown probability p.\",\"v(i)→maxv(i)−1,0\",\"反映驾驶员的不完美驾驶行为（imperfect behavior of drivers），比如在步骤2减速时刹车踩的过大。在现实场景，驾驶员不可能总是按照一定速度(constant speed)行驶，总会有一定的波动(fluctuations)。当车流量足够大，驾驶员的这一行为会引起交通系统的一系列反应，最终形成拥堵现象。这也是反映了拥堵总是在没有任何外部原因（external reasons ，如事故）就会发生的，因此称为幽灵拥堵（phantom jam）。\",\"Randomization reflects the imperfect behavior of drivers like the overreations of braking in Step 2. In real-world scenario, drivers cannot always drive at a constant speed. There always has fluctuations of the velocity.\",\"Step 4: 位置更新 Driving：每个车辆前进当前速度的格数。 Every vehicle forward by v(i) cells.\"]},\"145\":{\"h\":\"3 改进NS模型 NS Model for Inhomogenous Traffic Flow in a Single Lane\"},\"146\":{\"h\":\"3.1 改进点：\",\"t\":[\"车辆以最大加速度 amax​ 加减速而不是以1 Vehicle accelerates/decelerates with maximum acceleration not 1.\",\"引入慢启动系数s（反应静止车辆启动较慢） Introduce the Slow Start probability(short for s) to reflect the difficuty of stationary vehicles to start up.\",\"考虑异质(inhomogeneous)车流（反映在车长，最大速度，最大加速度） Consider the inhomogeneous flow like the vehicle length, maximum velocity and maximum acceleration.\"]},\"147\":{\"h\":\"3.2 模型信息\",\"t\":[\"单车道 周期边界 Single Lane with Periodic Boundary\",\"模型信息\"]},\"148\":{\"h\":\"3.3 结果\",\"t\":[\"通过改变货车占比r(propotion of trucks)、随机慢化概率p以及慢启动系数s，分别做FD图以及时空位置图，分别分析其对于交通系统的影响影响。\",\"如当s = 0.1, p = 0.3 条件下，分别做r=0,0.1,0.2,0.3的时空位置图(Time-Space Diagram)以及基本图（Fundemantal Diagram, FD）,红线为货车，黑线为小汽车。\",\"r=0\",\"r=0.1\",\"r=0.2\",\"r=0.3\",\"r=0 含速度\",\"r=0.1 含速度\",\"r=0.2 含速度\",\"r=0.3 含速度\",\"可以看到随着货车比例的增加，wave speed逐渐降低，wave period也逐渐变小，反应交通系统越来越拥堵。\",\"做FD图如下：\",\"r=0\",\"r=0.1\",\"r=0.2\",\"r=0.3\",\"r\",\"极大流率Qm\",\"临界密度Kc\",\"临界速度Vc\",\"堵塞密度Kj\",\"畅行速度Vf\",\"0\",\"2326.9176\",\"19\",\"122.4693\",\"140\",\"126.7668\",\"0.01\",\"1782.0972\",\"40\",\"44.5524\",\"138\",\"126.8262\",\"0.03\",\"1702.6758\",\"40\",\"42.5669\",\"131\",\"126.9054\",\"0.05\",\"1647.4248\",\"39\",\"42.2417\",\"127\",\"46.3035\",\"0.1\",\"1534.3903\",\"37\",\"43.0448\",\"115\",\"46.4292\",\"0.2\",\"1377.4356\",\"34\",\"40.5128\",\"99\",\"46.0799\",\"0.3\",\"1241.7444\",\"32\",\"38.8045\",\"86\",\"42.9739\",\"随着货车比例r的增加，Qm逐渐减小，Kc先增加后减小，Vc先急剧减小后基本不变，Kj逐渐减小，Vf先不变后急剧减小。\",\"通过改变s与p的大小，也可以同理做出上述图表，这里不一一展示。\"]},\"149\":{\"h\":\"3.4 代码\",\"t\":[\"完整代码，内含动画演示。\"]},\"150\":{\"h\":\"3.5 进一步的改进点\",\"t\":[\"多车道，考虑换道，如STCA模型。\",\"Velocity-Dependent-Randomization（VDR）：NS模型中p是固定不变的，VDR中p是v的函数（与加入随机慢化概率s效果类似）。\",\"STCA（[以下内容源于][m]）\",\"由Rickert M和Chowdhury D等人在单车道元胞自动机Nasch模型的基础上提出了双车道元胞自动机STCA模型。该模型将模拟的道路环境扩展为双车道，增加了车辆换道规则，使之能够更真实准确地模拟出道路上交通流的运行状况。\",\"STCA模型在应用过程中将一个时步分为两个相同的子时步。第一个时步为车辆换道时步，车辆在这个时步内按照设计的换道规则决定是否发生换道行为；第二个时步为演化更新时步，两条车道上的车辆在该时步均按照单车道元胞自动机模型中设计的演化更新规则运行。\",\"换道规则一般包括两个部分：\",\"换道动机： 换道动机是指驾驶员想换道的意愿和条件。当某车辆在当前车道上无法达到驾驶员的期望速度，而另一条车道上的驾驶条件可以满足驾驶员对速度的要求时，车辆会以一定的换道概率进行换道。\",\"dn​<Δtmin(vn​+1,vmax​)dn, sidefront ​>dn​⋯​\",\"安全条件： 安全条件是指车辆在决定换道时要确定当前交通状况下换道对于自身和其他车辆是否安全，避免因车辆换道引起交通事故，危害生命财产安全。\",\"dn, sideback ​>dsafe ​\"]},\"151\":{\"c\":[\"交通\"]},\"152\":{\"c\":[\"traffic\"]},\"153\":{\"h\":\"行人流仿真 Pedestrian Simulation\",\"t\":[\"本文探讨基于 CA 模型的行人流仿真，如果你还不了解 CA 模型，请先移步这篇文章。\"]},\"154\":{\"h\":\"概述\",\"t\":[\"行人流仿真是通过模拟人群在不同环境下的移动，研究行人行为及心理的特点的研究，其在城市规划、交通管理、疏散计划、建筑平面设计等方面有着广泛的应用。\",\"Pedestrian simulation studies pedestrians' behavioral patterns and psychological aspects by modeling and simulating the movement of crowds in various scnarios. It finds extensive applications in urban planning, traffic management, evacuation planning, and architectural layout design.\",\"分类\",\"行人流仿真按仿真规模可以大致分为三种，即 宏观(macroscopic) 、 微观(microscopic) 、介于宏微观之间(mesoscopic) 这三种：\",\"宏观：以整个人群为研究对象，研究整体移动特征如速度、密度、流向等，每个个体没有行为特征（distinct behavior），如研究拥堵、紧急疏散人群移动特点。\",\"微观：以个体为研究对象，研究个体行为，每个个体有着独特的行为特征。\",\"介观：介于宏微观之间，人群中每个个体有着相同的行为特征，既研究整体特征也研究个体特征。\",\"常见的模型方法有：\",\"基于物理规则模型：这类模型主要是基于物理规则（law of physics），来模拟人与环境的交互，如流体动力学模型(fluid dynamic model)，社会力模型（social force model）等。 社会力模型由几种力组成，如使人达到向着目的地前进，达到一定期望速度的acceleration force，以及一些环境阻碍的力repulsive force，如来自人行道边界（crosswalk boundary）、周围行人（surrounding pedestrian）、冲突车辆（conflicting vehicles）和信号灯状态（signal phase）等阻力，这些力的合力即为行人朝着什么方向（力的方向），以多大的速度（由力的大小决定）前进。\",\"元胞自动机模型（Celluar Automata, CA）\",\"数据驱动模型：基于现实场景数据，对数据进行合适的编码（encode），采用深度学习等方法预测行人的轨迹、行为，常见的数据有：\",\"轨迹信息（trajectory information），可以是2D平面图，也可以是3D坐标（第三维为速度）的形式（2D spatial or 3D global coordinates + velocity）。\",\"视觉信息（visual information）：包含做过语义分割处理的地图（semantic map）与行人图像(images of pedestrian)，地图主要是反应道路周围的环境，如道路结构（road structure）、其他道路使用者状态（the state of other road users）以及信号状态（signal phase），行人图像主要是反应行人的行为，如动作（motion）、姿势（pose）、朝向（head orientation）等。\",\"自主车辆（ego vehicle）：包含感知周围环境传感器的车辆的信息(vehicle equiped with sensors)，如车辆坐标（coordinates）、速度（velocity）等。\",\"数据驱动模型\",\"⏩ 不同方法并非只适用于一个规模，对行人流仿真进一步了解可以移步此篇论文\"]},\"155\":{\"h\":\"问题描述\",\"t\":[\"模拟行人穿过以平台\",\"相关信息\",\"平台基本信息：\",\"平台 16×30，左三个入口，大小分别为 1×1,1×4,1×1，右四个出口，均为 1×1，中间设有障碍，行人不得通过障碍。\",\"行人每秒走一格，每个行人占一格。\",\"行人随机从各个入口进入。\",\"规定仿真时间为 960s\",\"图1 平面示意图 \"]},\"156\":{\"h\":\"模型设置\"},\"157\":{\"h\":\"基本设置\",\"t\":[\"元胞：状态设置为占有（0）与不占有（1）\",\"元胞空间：划分为方格形。\",\"元胞邻居：采用基本 Moore 型（r=1）\",\"边界条件：采用固定型。\"]},\"158\":{\"h\":\"更新规则\",\"t\":[\"更新规则即行人如何选择下一步走到哪里一个方格，这里引入元胞潜能 Cell Potential，其定义如下：\",\"Ni,j​=Ei,j​exp(kS​Si,j​+kD​Di,j​)\",\"其中，Ei,j​ 代表位置(i, j)处元胞状态，0 代表占有，1 代表不占有，Ni,j​ 代表位置(i, j)处的元胞潜能，可以发现，当元胞占有时 Ei,j​=0 ，即该处元胞潜能为 0，反应了元胞有人占据，就无法选择。Si,j​ 为元胞静态势能 Static Potential，Di,j​ 为元胞动态势能 Dynamic Potential， kS​ 和 kD​ 分别为对应系数。\",\"元胞静态势能反应了行人在选择下一步时，环境中静止物体的影响，这里主要考虑为出口与障碍物，定义为：\",\"Si,j​=kL​Li,j​+kO​Oi,j​\",\"其中，Li,j​ 为位置为(i, j)的元胞距出口的距离，Oi,j​ 为位置为(i, j)的元胞周边的非障碍数目。\",\"元胞动态势能反应行人在选择下一步时，环境的动态影响，这里以位置为(i, j)的元胞周围空元胞数目 Di,j​ 为指标。\",\"所以最后，元胞潜能可以写为：\",\"Ni,j​=Ei,j​exp(k1​Li,j​+k2​Oi,j​+k3​Di,j​)\",\"最后对 9 个位置进行标号、对元胞潜能进行归一化，即可得到选择 9 个位置的选择概率 Transition ProbabilitiePi,j​。\",\"图2 Moore邻居选择示意图\"]},\"159\":{\"h\":\"部分代码解释\",\"t\":[\"代码整体思路如下：\",\"图3 代码流程图\"]},\"160\":{\"h\":\"参数设置\",\"t\":[\"clc,clear format short; n=16; %平台宽度 h=30; %平台长度 star_x = ones(1,6); % 入口横坐标 star_y = [4,7:10,13]; % 入口纵坐标 hurdle_x = repelem(14:16,2); % 障碍 hurdle_x = cat(2,hurdle_x,[20 20]); hurdle_y = repmat(8:9,1,3); hurdle_y = cat(2,hurdle_y,[5 12]); final_x = ones(1,4)\\\\*h; % 出口 final_y = [3,6,11,14]; x=n+2; % 边界矩阵宽 y=h+2; % 边界矩阵长 platform=ones(n,h); %初始化平台 obstacle_map=ones(n+4,h+4); %设置非障碍矩阵 obstacle_map(1:2,:)=0; obstacle_map(end-1:end,:)=0; obstacle_map(:,1:2)=0; obstacle_map(:,end-1:end); border=ones(x,y); %边界矩阵 border(1,:)=0; border(end,:)=0; border(:,1)=0; border(:,end)=0; Sm=ones(n+4,h+4); % 图 Sm(1:2,:)=0; Sm(end-1:end,:)=0; Sm(:,1:2)=0; Sm(:,end-1:end); for i = 1:size(hurdle_y,2) Sm(hurdle_y(i)+2,hurdle_x(i)+2)=0; %设置障碍 obstacle_map(hurdle_y(i)+2,hurdle_x(i)+2)=0; %设置障碍 end step=1; %初始迭代次数 po=1:1:9; %位置矩阵 pp = zeros(1,9); neigh = [-1,-1;0,-1;1,-1;-1,0;0,0;1,0;-1,1;0,1;1,1]; L=zeros(n,h,size(final_y,2)); % 不含边界距离矩阵 N=zeros(n+2,h+2,size(final_y,2)); % 元胞潜力 N_choose=zeros(n+2,h+2); % 最终选择 P=zeros(n+2,h+2); %预留内存 prob=zeros(1,9); %概率矩阵、预留内存 go=0; % 出发人数 arrive=0; % 到达终点人数 total=960; % 迭代时间 time_people_star = zeros(n,h,total); % 记录时刻平台信息 \",\"这里设置了 4 个 map：\",\"platform：反应平台实时状态\",\"border：在 platform 外加了一圈障碍，表示边界条件。\",\"Sm 与 obstacle：在 platform 外加了两圈障碍，分别用计算 $ O*{i, j} $ 与 $ D_{i, j} $。\"]},\"161\":{\"h\":\"参数计算\",\"t\":[\"Li,j​ 的计算：\",\"Dis = zeros(n+2,h+2,size(final_y,2)); Dis = Dis + inf; % 分别计算边界内每个原胞到出口的距离 for f=1:size(final_y,2) for i=1:n for j=1:h L(i,j,f)=sqrt((i-final_y(f))^2+(j-final_x(f))^2); %不含边界的距离矩阵 Lij end end end Dis(2:n+1,2:h+1,:)=L; for i = 1:size(hurdle_y,2) Dis(hurdle_y(i)+1,hurdle_x(i)+1,:)=inf; %障碍视为距离无穷 end \",\"Oi,j​ 与 Di,j​ 的计算：\",\"O=obstacle_map(1:x,2:y+1)+obstacle_map(3:x+2,2:y+1)+obstacle_map(2:x+1,1:y)+obstacle_map(2:x+1,3:y+2)+obstacle_map(1:x,1:y)+obstacle_map(3:x+2,1:y)+obstacle_map(1:x,3:y+2)+obstacle_map(3:x+2,3:y+2); D=Sm(1:x,2:y+1)+Sm(3:x+2,2:y+1)+Sm(2:x+1,1:y)+Sm(2:x+1,3:y+2)+Sm(1:x,1:y)+Sm(3:x+2,1:y)+Sm(1:x,3:y+2)+Sm(3:x+2,3:y+2); \",\"这段代码思想为用一个 n-2×n-2 大小的滑动窗口在 n×n 的平台上，依次从需要计算的 8 个周边位置滑动，最后得到所求，可以自己手动画一个图验证一下。\",\"计算元胞潜力：\",\"% 计算原胞潜力 N for f = 1:size(final_y,2) for i=1:x for j=1:y N(i,j,f)=border(i,j)*exp(-5*Dis(i,j,f)+D(i,j)+O(i,j)); end end end for i = 1:size(final_y,2) N(final_y(i)+1,final_x(i)+1)=1e10; % 设置出口原胞潜力为 1e10，可视为无穷大 end N_1 = max(N(:,:,1),N(:,:,2)); %最大作为原胞潜力 N N_2 = max(N(:,:,3),N(:,:,4)); N_choose = max(N_1,N_2); \",\"这里因为有四个入口，所以需要分别计算四个出口的元胞潜力大小，最后取最大。\",\"位置更新：\",\"for j=h+1:-1:2 for i=2:n+1 if(border(i,j)==0) %如果位置（即原胞）有人，计算所有邻居原胞的原胞潜力N % 计算位置1到9各原胞潜力大小，并进行归一化处理 for xy = po pp(1,xy) = N_choose(i+neigh(xy,2),j+neigh(xy,1)); end prob = pp/sum(pp); if sum([pp(2),pp(3),pp(6),pp(8),pp(9)]~=0) % 上下前三个方向不全都有人 S=randsrc(1,1,[po;prob]); %依原胞潜力N，选择下一位置，即进行位置更新 else S = 5; end k = i + neigh(S,2); t = j + neigh(S,1); if platform(k-1,t-1)==0 % 选择新位置已占，则选回原位置 S = 5; k = i + neigh(S,2); t = j + neigh(S,1); end platform(i-1,j-1)=1; % 位置更新，原来原胞更新为空状态 platform(k-1,t-1)=0; % 位置更新，新选择原胞更新为占有状态 end end end \",\"这里用 border 矩阵进行计算，然后在 platform 上进行更新，最后把再 border = platform ，从而实现每一次迭代的整体更新。此外，代码设定，如果上下和前面三个位置共 5 个位置没有人的话才进行选择，否则就待在原地，贴近现实中人是向前走的；如果选择的位置被占，则待在原地。代码从离平台近的位置向远处开始遍历，反应人流变化的方向与源头。\"]},\"162\":{\"h\":\"结果\",\"t\":[\"图4 动态演示\",\"图5 热力图（迭代周期960s）\",\"完整代码\"]},\"163\":{\"c\":[\"交通\"]},\"164\":{\"c\":[\"traffic\"]},\"165\":{\"h\":\"Cutting-edge Technology 前沿科技\",\"t\":[\"本文不定期更新前沿科技简介.\"]},\"166\":{\"h\":\"\",\"t\":[\"大语言模型是利用深度学习技术，如预训练、微调、Transformer 架构等，通过海量多样化数据训练，得到的具有 数以亿计参数的，具有学习、理解、适应能力的，能够处理多任务如自然语言处理、计算机视觉、语音识别等任务的机器学习模型。\",\"Foudation models are machine learning models, trained on massive and diverse datasets using deep learning techniques such as pre-training, fine-tuning, and Transformer architectures, which possess billions of parameters. These models are capable of learning, understanding, and adapting, enabling them to handle various tasks, including natural language processing, computer vision, and speech recognition.\",\"!Foudation Model\"]},\"167\":{\"h\":\"Sora 模型对于交通行业影响\",\"t\":[\"Sora结合了GPT技术，能够理解和生成高质量的文本提示（prompts），这些提示用于指导视频内容的生成。\",\"Sora integrates GPT, enabling it to understand and produce high-quality text prompts that guide the creation of videos.\",\"1.对于自动驾驶\",\"提供高质量的真实数据用于训练与模拟 Provide high-quality datasets including real scnarios for training and simulating.\",\"提供新的开发算法思路、帮助开发更为智能的决策方案（自动驾驶系统所需具良好的泛化能力，在复杂环境中做出快速决策的，目前的系统往往在特定场景下表现良好，但在新环境中可能无法适应。Sora生成的视频可以模拟各种决策场景，以帮助开发更为智能的决策制定算法。）Offer more adaptive algorithms to better handle the complex issues in real-world scenarios.\",\"提供虚拟测试的平台 Offer platforms for virtual test\",\"2.对交通规划\",\"交通规划模拟与分析（通过生成不同交通规划方案，模拟拟不同的交通规划方案，帮助规划者评估其对交通流量、拥堵和行人安全的影响。通过模拟特定时间段的交通流，Sora可以帮助预测和分析交通模式，为交通规划提供数据支持）\",\"提供应急响应与安全管理案例（在应急响应规划中，Sora可以用来模拟自然灾害、事故等紧急情况下的交通状况，帮助制定有效的应急响应策略。通过模拟不同紧急情况下的交通流动，Sora有助于提高交通管理系统的准备性和响应能力）\\n*Generating videos of traffic fomular\"]},\"168\":{\"h\":\"Vocabulary in Planning Algorithms\",\"t\":[\"recurring：反复出现，经常的 One recurring theme is that\",\"increment：增长 decrement：减少 The robot takes discrete steps in one of four directions (up, down, left, right), each of which increments or decrements one coordinate.、\",\"redundant：冗余的，重复因而不必要的 1.recurring：反复出现，经常的 One recurring theme is that\",\"increment：增长 decrement：减少 The robot takes discrete steps in one of four directions (up, down, left, right), each of which increments or decrements one coordinate.、\",\"omit：省略，忽略，遗忘 Figure 2.4 omits several details that...\",\"monotonicity：单调性\",\"asymptotic：渐进的\",\"trivial：不重要的 unimportant\",\"cumbersome: 笨重的，沉重的，难以去做的 however, the notation and explanation can become more cumbersome because\",\"superficial: 表面的，表皮的，肤浅的\",\"recurrence: 重复出现\",\"preclude：阻止，防止（人/事） prevent\",\"onward：向前的/地\"]},\"169\":{\"h\":\"节奏、节拍和律动\",\"t\":[\"节奏是用音强弱组织起来的音的长短关系，强调长短，合拍。\",\"节奏型是曲中典型的、反复出现的节奏片段。\",\"节拍是强拍和弱拍按照一定顺序的循环，强调强弱。节拍的节是循环的节点，即小节，图中表示为竖线，后面跟一个强拍。\",\"常说的四四拍：\",\"四四拍\",\"分母代表以哪个音符的时值为一拍，分子代表一节有几个这样的拍子。\",\"常见拍号：\",\"常见拍号\",\"律动：由节奏推动，引人晃动，或因韵律而感动（情感共鸣）。\",\"常见节奏型\",\"前附点延长前面音一半的拍，也就是第一个“大”变成了1/2拍+1/4拍=3/4拍。\",\"后附点同理。\",\"切分音重音在第二个拍上。\",\"三连音，三个八分音符组成一拍，每个音符占1/3个音符。\"]},\"170\":{\"c\":[\"GT\"]},\"171\":{\"h\":\"五线谱、简谱、六线谱\"},\"172\":{\"h\":\"五线谱\",\"t\":[\"五线谱主要是四个内容：什么调，什么节奏，不同位置音符代表不同音高（由下图，从下往上依次是MI FA SOL LA SI DO RE MI FA），以及音符不同时值带来的节奏。\",\"五线谱\",\"两线时间叫做间。\"]},\"173\":{\"h\":\"简谱\",\"t\":[\"简谱在音符那课已经讲了很多，主要见下图。\",\"简谱\"]},\"174\":{\"h\":\"六线谱\",\"t\":[\"六线谱\",\"左右手势\",\"两条细线是段落线，一个细线加一个粗线是结尾。\",\"反复标记\",\"连音符与延音符\",\"延音符：同样音高的相连，表示后面音符时值加给前面的音符，即后面的不弹，但保持后面音符的时值。\",\"连音符是不同音高相连，仅表示音符相连。\"]},\"175\":{\"c\":[\"GT\"]},\"176\":{\"h\":\"拨片 & 五种音阶\"},\"177\":{\"h\":\"拨片\",\"t\":[\"拨片分类\",\"拿的位置：拨片一般捏在1/2或2/3处，不要捏的太紧，靠手指指纹与拨片表面摩擦力来固定。\",\"拨弦位置：拨片1-2mm与琴弦接触。\",\"角度：拨片与琴弦角度如下图：\",\"拨片与琴弦角度\",\"拨弦区域：音孔右侧边缘位置，如下图：\",\"拨弦区域\",\"发力方式：同一根线：手腕发力。 上下拨弦：手腕带动小臂。\",\"右手固定与支撑方式：\",\"1）小指无名指抵在护板，如下图：\",\"适合分解练习，爬音阶和爬格子\",\"2）右手小鱼际放在5、6线的固弦锥上，如下图：\",\"适合弹奏1-4弦位置，闷音\",\"3）右手整体悬空，以右手肘与琴箱交点为支点，适合扫弦。\",\"上拨下拨符号如下，最右侧为上拨，左侧为下拨。\",\"拨弦符号\"]},\"178\":{\"h\":\"五种音阶\",\"t\":[\"C大调mi音阶\",\"其他四种音阶\",\"左手拇指固定后就不要来回移动了，从而锻炼手指展开能力。\"]},\"179\":{\"c\":[\"GT\"]},\"180\":{\"h\":\"音符与休止符\",\"t\":[\"本系列课程源于高峰老师课程，为我本人购买后的学习过程的记录，如有侵权请联系我。\"]},\"181\":{\"h\":\"音符\",\"t\":[\"节奏简单可以理解为音的长短和强弱。音符是记录音长短的符号，是五线谱、六线谱、简谱等记谱法的最重要元素。\",\"具体内容见下图:\",\"常见音符\",\"其中，时值是不同音符代表的时间值，即这个音要发多长时间。手拍下来再上来为一拍（只拍下去为半拍），或节拍器响一下开始到下一次再响起为一拍。\",\"一拍的时间不是固定的，是由曲风所决定的,具体速度会在谱中标出，例如下图，代表1分钟60拍，即1s一拍。\",\"1s 1拍\",\"又如下图：\",\"例子\",\"代表一个四分音符+一个八分音符+两个十六分音符\",\"常见基本节奏型：\",\"常见基本节奏型\",\"其他谱的音符表示：\",\"简谱：\",\"简谱音符表示\",\"-：增时符，延续一拍。_：减时符，减半拍。\",\"1---代表全音符， 3处代表四分音符， 4下面一个横线代表八分音符。\",\"六线谱：\",\"也是增时符与减时符的应用。\",\"六线谱全音符\",\"六线谱八分音符\"]},\"182\":{\"h\":\"休止符\",\"t\":[\"休止符是记录不发声间断的符号，其规律同音符，如下图：\",\"休止符\"]},\"183\":{\"c\":[\"GT\"]},\"184\":{\"h\":\"Multi-Agent Teamwise Cooperative Path Finding and  Traffic Intersection Coordination\",\"t\":[\"This paper is published at 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2024. Full paper at\"]},\"185\":{\"h\":\"Abstract\",\"t\":[\"Abstract — When coordinating the motion of connected autonomous vehicles at a signal-free intersection, the vehicles from each direction naturally forms a team and each team seeks to minimize their own traversal time through the intersection, without concerning the traversal times of other teams. Since the intersection is shared by all teams and agent-agent collision must be avoided, the coordination has to trade the traversal time of one team for the other. This paper thus investigates a problem called Multi-Agent Teamwise Cooperative Path Finding (TCPF), which seeks a set of collision-free paths for the agents from their respective start to goal locations, and agents are grouped into multiple teams with each team having its own objective function to optimize. In general, there are more than one teams and hence multiple objectives. TCPF thus seeks the Pareto-optimal front that represents possible tradeoffs among the teams. We develop a centralized planner for TCPF by leveraging the Multi-Agent Path Finding techniques to resolve agent-agent collision, and Multi-Objective Optimization to find Pareto-optimal solutions. We analyze the completeness and optimality of the planner, which is then tested in various settings with up to 40 agents to verify the runtime efficiency and showcase the usage in intersection coordination.\"]},\"186\":{\"h\":\"Key Points in This Paper\"},\"187\":{\"h\":\"Problem Statement\"},\"188\":{\"h\":\"1. Workspace and Agents\",\"t\":[\"There is a set of N agents, each indexed by a set I={1,2,…,N}.\",\"The environment is represented as a finite graphG=(V,E), where:\",\"V is the set of vertices (representing possible locations or states for agents).\",\"E⊆V×V is the set of edges (representing the actions or moves an agent can take between two vertices).\"]},\"189\":{\"h\":\"2. Agent Paths and Costs\",\"t\":[\"Each agent i∈I has:\",\"A start locationvio​∈V.\",\"A goal locationvid​∈V.\",\"The goal is to find a collision-free pathπi for each agent, from its start to goal, which consists of a sequence of vertices (vi1​,vi2​,…,vik​).\",\"The cost of a path πi, denoted as g(πi), is the sum of the costs of the edges along the path:\",\"g(πi)=j=1∑k−1​cost(vij​,vij+1​)\",\"The agents must avoid two types of conflicts:\",\"Vertex conflict: Two agents occupying the same vertex at the same time.\",\"Edge conflict: Two agents moving through the same edge in opposite directions at the same time.\"]},\"190\":{\"h\":\"3. Teams and Objectives\",\"t\":[\"The agents are grouped into M teams, denoted as {Tj​}j=1M​, where each team Tj​⊆I consists of a subset of agents. Teams are not necessarily disjoint, meaning an agent can belong to multiple teams.\",\"Each team Tj​ has an objective function gTj​​ to minimize. The objective function depends on the paths of all agents in the team and is non-decreasing with respect to the individual path costs. That is, if an individual agent’s path cost increases, the team’s objective function will either stay the same or increase.\",\"- Common Objectives Include:\",\"Min-sum: Minimize the sum of individual path costs of all agents in the team.\",\"gTj​​=i∈Tj​∑​g(πi)\",\"Min-max: Minimize the maximum individual path cost within the team.\",\"gTj​​=i∈Tj​max​g(πi)\"]},\"191\":{\"h\":\"4. Pareto-Optimality\",\"t\":[\"Since the TCPF problem involves multiple teams with different objectives, a single optimal solution is not always possible. Instead, the goal is to find Pareto-optimal solutions.\",\"A solution is Pareto-optimal if no team’s objective can be improved without making another team’s objective worse. In other words, it's impossible to improve one team's traversal time without increasing that of another team.\",\"The set of all non-dominated, feasible solutions forms the Pareto-optimal front. A solution dominates another if it has a lower (or equal) cost for all teams and strictly lower cost for at least one team.\"]},\"192\":{\"h\":\"5. General and Fully Cooperative TCPF\",\"t\":[\"General TCPF: The standard TCPF problem, where each team has its own objective and the goal is to find a set of Pareto-optimal solutions.\",\"Fully Cooperative TCPF: A special case where all agents belong to the same team (i.e., Tj​=I for all j), meaning the entire system works together to minimize a common objective. In this case, the problem reduces to a single-objective Multi-Agent Path Finding (MAPF) problem.\"]},\"193\":{\"h\":\"Dominance and Pareto-Optimality\",\"t\":[\"In the TCPF problem, multiple teams of agents are navigating the environment, and each team has its own objective function to optimize. These objectives often conflict because the shared environment (such as an intersection) has limited resources (space and time), and improving one team's objective may worsen another's.\",\"For example:\",\"Team 1 may aim to minimize the total traversal time for all its agents (min-sum objective).\",\"Team 2 may aim to minimize the maximum traversal time among its agents (min-max objective).\",\"There is no single solution that can optimize both objectives simultaneously for different teams. As a result, we need a way to compare and evaluate trade-offs between different solutions where one team may benefit at the expense of another.\"]},\"194\":{\"h\":\"1 Pareto-Optimality and Trade-Offs\",\"t\":[\"To address these conflicting objectives, the paper seeks Pareto-optimal solutions—solutions where it's impossible to improve one team's objective without making another team’s worse. This leads to the introduction of dominance, which helps figure out which solutions deserve to be called Pareto-optimal.\"]},\"195\":{\"h\":\"2 Definition of Dominance\",\"t\":[\"Given two solutions a and b, each represented by an objective vector (a set of objective values for all teams):\",\"a=(a1​,a2​,…,aM​)andb=(b1​,b2​,…,bM​)\",\"where each aj​ and bj​ represents the objective value for team Tj​.\",\"Solution a dominates solution b if:\",\"aj​≤bj​ for all teams j=1,2,…,M (i.e., the objective values of a are no worse than those of b for all teams).\",\"There exists at least one team j where aj​<bj​ (i.e., a is strictly better for at least one team).\",\"If solution a dominates solution b, then b can hit the road 🛣 because it's worse and can be discarded from the search for optimal solutions.\"]},\"196\":{\"h\":\"3 Why Dominance is Necessary\",\"t\":[\"Pruning Inferior Solutions: Dominance is your friend! It helps get rid of solutions that are just straight-up inferior and don't offer anything better. If one solution dominates another, you can toss the dominated one, making your search for optimal paths much more efficient .\",\"Handling Multiple Objectives: Each team has its own objective, and trade-offs are the name of the game . The dominance relation helps identify which solutions are on the Pareto-optimal front—aka the set of solutions representing the best possible trade-offs. If a solution is dominated, there's another one that's better in at least one objective without being worse in any other.\",\"Comparing Non-Dominated Solutions: Not all solutions are comparable. Two solutions a and b might be non-dominated with respect to each other—this means a is better for some teams, while b is better for others. Both solutions should be kept because they’re valid trade-offs .\"]},\"197\":{\"h\":\"4 How to Choose the Final or Best Solution When Finding the Pareto-Optimal Front\",\"t\":[\"There is no one solution that can be called the best. The solution finally chose is up to the preferences, trade-offs balance or other rules.\",\"Example:\",\"Suppose TC-CBS has found the following three Pareto-optimal solutions for a system with two teams:\",\"Solution A:\",\"(20,40) → Team 1 has a cost of 20, Team 2 has a cost of 40.\",\"Solution B:\",\"(25,35) → Team 1 has a cost of 25, Team 2 has a cost of 35.\",\"Solution C:\",\"(30,30) → Team 1 has a cost of 30, Team 2 has a cost of 30.\",\"Preference-Based: If Team 1 has higher priority, the system might choose Solution A because it gives Team 1 the lowest cost, even though it increases Team 2’s cost.\",\"Trade-Off Analysis: If the decision-maker values fairness, they might choose Solution C, as it balances the costs for both teams equally.\",\"or Using a weighted sum with weights\"]},\"198\":{\"h\":\"Conflict-Based Search (CBS)\",\"t\":[\"Step 1：Initializing\",\"Construct a Search tree with root node Proot​, which is a tuple of (π,g,Ω).\",\"where: π=(π1,π2,...,πN) is a joint path that connects starts and goals of\",\"agents respectively; g is the scalar cost value of π(i.e.,g=g(π)=∑i∈I​gi​(πi​))\",\"Ω is a set of constraints, and each constraint is of form (i,v,t) or (i,e,t), which indicates agent i is forbidden to enter node v (or edge e) at time t.\",\"Employ A* (Low level search) to generate π of every agent without Ω and thus obtain Proot​.\",\"Step 2：Expanding\",\"Choose the node with the minimum g-value for expansion. (High level search)\",\"Step 3：Check for Conflicts\",\"Check the conflicts of the selected node, namely π=(π1,π2,...,πN).\",\"If there exists conflicts, the algorithm splits the search into two branches in the postion of the selected node, each of which has new constraint sets ΩS{i,v,t} and ΩS{j,v,t} are generated.\",\"Step 4: Replanning\",\"A* is rerun for the affected agent, considering the newly added constraints(regarded as an obstacle)\",\"A new joint path π′ is then formed by first copying π and then updating agent i’s individual path πi​ with π′i.\",\"Finally, for each of the two split constraints, a corresponding node is generated and added to the tree for future expansion.\",\"Run Step 2-4 iteratively until there exits one π which contain no conflicts.\"]},\"199\":{\"h\":\"Teamwise Cooperative CBS (TC-CBS)\",\"t\":[\"TC-CBS follows a similar workflow as CBS.\",\"Key differences:\",\"Instead of a single objective (like sum or max), each solution is associated with an objective vector g(π)=(g1,g2,...,gN).\",\"TC-CBS aims to find Pareto-optimal solutions, where no team’s objective can be improved without worsening another team’s objective.\",\"TC-CBS uses lexicographic ordering to compare objective vectors instead of using a scalar cost to prioritize nodes.\",\"TC-CBS terminates when the high-level search has explored all possible paths and found all non-dominated solutions.\",\"Lexicographic Ordering for Comparing Objective Vectors\",\"Lexicographic ordering is used in TC-CBS to compare objective vectors and prioritize solutions during the search process. Here's how it works:\",\"Definition of Lexicographic Ordering: Suppose we have two objective vectors,\",\"a=(a1​,a2​,…,aM​)andb=(b1​,b2​,…,bM​)\",\"where each component represents the objective value for a different team.\",\"Lexicographic ordering compares the objective vectors element by element:\",\"a is lexicographically smaller than b (denoted as a≺b) if there exists an index k such that:\",\"a1​=b1​,a2​=b2​,…,ak−1​=bk−1​ (all previous elements are equal), and\",\"ak​<bk​ (the first non-equal element is smaller in a than in b).\",\"Example: Let's say we have two objective vectors for a 3-team system:\",\"a=(5,10,15)\",\"b=(5,12,10)\",\"To compare them lexicographically:\",\"Compare the first elements: a1​=b1​=5 (they are equal).\",\"Compare the second elements: a2​=10 and b2​=12. Since a2​<b2​, we conclude that a≺b.\",\"Thus, a is lexicographically smaller than b, even though b’s third element is smaller than a’s third element. Lexicographic ordering only cares about the first non-equal element.\",\"Why Lexicographic Ordering? Lexicographic ordering helps in prioritizing solutions during the search:\",\"It allows TC-CBS to expand the solution that is lexicographically smallest (considering the first team, then the second team, and so on), ensuring a structured exploration of the solution space.\",\"It provides a way to break ties between solutions where multiple objectives (teams) are involved.\"]},\"200\":{\"h\":\"TC-CBS-t Algorithm\",\"t\":[\"TC-CBS-t is a modified version of TC-CBS that addresses the incompleteness of the original TC-CBS. In some cases, TC-CBS may fail to terminate in finite time because it can generate an infinite number of non-dominated solutions. TC-CBS-t uses a transformation on the objective vector to ensure the algorithm always terminates while still finding a subset of the Pareto-optimal solutions.\",\"The transformation involves adding a small weight ϵ，ϵ to the objectives of other teams. This essentially converts the problem into a fully cooperative problem, where every team’s objective is slightly influenced by the others.\",\"gf​(πTj​):=g(πTj​)+ϵΣi∈/Tj​​g(πi)\",\"The transformed problem is easier to solve because it guarantees that the algorithm will find a finite set of Pareto-optimal solutions. However, TC-CBS-t may not find all Pareto-optimal solutions.\"]},\"201\":{\"h\":\"EXPERIMENTAL RESULTS and Conclusion\",\"t\":[\"See at full paper.\"]},\"202\":{\"c\":[\"PR\"]},\"203\":{\"h\":\"iMTSP: Solving Min-Max Multiple Traveling  Salesman Problem with Imperative Learning\",\"t\":[\"论文原址\",\"This paper presents a network-based approach to solving the Min-Max Multiple Traveling Salesman Problem (MTSP) by integrating a deep learning model with a traditional TSP solver. The MTSP problem is formulated as a bilevel optimization problem:\",\"Upper level: Optimizes the assignment of cities to agents using an allocation network. This network leverages a Compositional Message Passing Neural Network (CMPNN) to encode city topological relationships and an attention mechanism to determine which agent should visit which cities.\",\"Lower level: Uses a traditional TSP solver to compute the optimal visiting order of cities for each agent, based on the assignment provided by the upper level.\",\"The objective is to minimize the longest tour length among all agents, which is non-differentiable and leads to high variance in gradient estimates, making optimization difficult.\",\"To address this:\",\"The log-derivative trick is applied to estimate the gradients for the non-differentiable allocation process.\",\"A surrogate network is introduced to predict the maximum tour length based on the city assignments. This acts as a control variate, reducing the variance in gradient estimates and stabilizing the training process.\",\"The final gradien of the parameter θ in allocation network is a combination of:\",\"The log-derivative-based gradient, adjusted by the difference between the surrogate network’s prediction and the actual maximum tour length.\",\"The gradient of the surrogate network, which helps improve its prediction accuracy.\",\"the original loss function of allocation network\",\"the new loss function of allocation network\",\"Experimental results demonstrate that the proposed approach significantly improves convergence speed, solution quality (shorter tours), and scalability to larger problem instances.\"]},\"204\":{\"c\":[\"PR\"]},\"205\":{\"h\":\"Chapter 1 Introduction\",\"t\":[\"PLANNING ALGORITHMS\",\"Steven M. LaValle\",\"University of Illinois\",\"Copyright Steven M. LaValle 2006\",\"Available for downloading at here.\",\"Published by Cambridge University Press\"]},\"206\":{\"h\":\"1.1 What is planning?\",\"t\":[\"Planning is a branch of algorithms.\",\"The user of the plan can be referred as robot or decision maker (robot, agent, controller are interchangeable)\"]},\"207\":{\"h\":\"1.2 Basic Ingredients of Planning\",\"t\":[\"State\",\"State can represent the position and orientation of a robot, the locations of tiles in a puzzle, or the position and velocity of a helicopter.\",\"The collection of state: state space.\",\"Can be both discrete (finite, or countably infinite) and continuous (uncountably infinite).\",\"Can be explicitly represented or implicitly.\",\"Time\",\"All planning problems involve a sequence of decisions that must be applied over time.\",\"Can be explicitly modeled or implicitly.\",\"Action\",\"A plan generates actions that manipulate the state.\",\"States changes when actions applied (through state-valued function under discrete time or differential equation under continuous time)\",\"Initial and goal states\",\"Planning problems involve starting from the initial state, finally arriving at the goal states (a set of)\",\"Criterion\",\"Feasiblity or Optimality\",\"Plan\",\"A plan can specify a sequence of actions to be taken or specify actions as a function of state.\",\"相关信息\",\"Once a plan is determined, there are three ways to use it.\",\"Execution: Execute it either in simulation or in a mechanical device (robot) connected to the physical world.\",\"Refinement: Refine it into a better plan. The new plan may take more problem aspects into account, or it may simply be more efficient (see at the following picture). Refinement can be executed repeatedly until the final one.\",\"A refinement approach that has been used for decades in robotics\",\"The first plan yields a collision-free path through the building. The second plan transforms the route into one that satisfies differential constraints based on wheel motions (recall Figure 1.11). The third plan considers how to move the robot along the path at various speeds while satisfying momentum considerations. The fourth plan incorporates feedback to ensure that the robot stays as close as possible to the planned path in spite of unpredictable behavior.\",\"Hierarchical inclusion: Under hierarchical inclusion, a plan is incorporated as an action in a larger plan. The original plan can be imagined as a subroutine in the larger plan.Hierarchical inclusion can be performed any number of times, resulting in a rooted tree of plans. This leads to a general model of hierarchical planning. Each vertex in the tree is a plan. The root vertex represents the master plan. The children of any vertex are plans that are incorporated as actions in the plan of the vertex. There is no limit to the tree depth or number of children per vertex.\"]},\"208\":{\"h\":\"1.3 Organization of this book\",\"t\":[\"PART 1 Intro: Chapter 1-2\",\"PART 2 Motion planning: Chapter 3-8\",\"PART 3 Decision-Theoretic Planning: Chapter 9-12\",\"PART 4 Planning Under Differential Constraint: Chapter 13-15\"]},\"209\":{\"c\":[\"PR\"]},\"210\":{\"h\":\"Chapter 2 Discrete Planning\"},\"211\":{\"h\":\"2.1 Introduction to Discrete Feasible Planning\"},\"212\":{\"h\":\"2.1.1 Problem Formulation\",\"t\":[\"Formulation 2.1\",\"State: x\",\"State space: X, nonempty, finite or infinite.\",\"Action: u\",\"Action space: U(x) , x∈X\",\"State transition fuction: x′=f(x,u). Each current state x, when applied with each action u, produces a new state x'.\",\"Initial state: xI​∈X\",\"Goal state: XG​∈X\"]},\"213\":{\"h\":\"2.1.2 Examples of Discrete Planning\",\"t\":[\"Moving on a 2D gird\",\"Suppose that a robot moves on a grid in which each grid point has integer coordinates of the form (i, j). The robot takes discrete steps in one of four directions (up, down, left, right), each of which increments or decrements one coordinate. The motions and corresponding state transition graph are shown in Figure 2.1, which can be imagined as stepping from tile to tile on an infinite tile floor. This will be expressed using Formulation 2.1. Let X be the set of all integer pairs of the form (i,j), in which i,j∈Z (Z denotes the set of all integers). Let U={(0,1),(0,−1),(1,0),(−1,0)} . Let U(x)=Uforallx∈X. The state transition equation is f(x,u)=x+u, in which x∈X and u∈U are treated as two-dimensional vectors for the purpose of addition. For example, if x = (3, 4) and u = (0, 1), then f (x, u) = (3, 5). Suppose for convenience that the initial state is xI=(0,0). Many interesting goal sets are possible. Suppose, for example, that xG={(100,100)}. It is easy to find a sequence of actions that transforms the state from (0, 0) to (100, 100).\",\"Rubik's Cube Puzzle\"]},\"214\":{\"h\":\"2.2 Searching for Feasible Plans\",\"t\":[\"The methods presented in this section are just graph search algorithms, but with the understanding that the state transition graph is revealed incrementally through the application of actions, instead of being fully specified in advance.\",\"An important point is that the search algorithms must be systematic, that is, the algorithm must keep track of states already visited.\"]},\"215\":{\"h\":\"2.2.1 General Forward Search\",\"t\":[\"The following figure gives a general template of search algorithms. At any point during the search, there will be three kinds of states:\",\"Unvisited: States that have not been visited yet. Initially, this is every state except xI.\",\"Dead: States that have been visited, and for which every possible next state has also been visited. A next state of x is a state x′ for which there exists a u∈U(x) such that x′=f(x,u) In a sense, these states are dead because there is nothing more that they can contribute to the search. In some circumstances, of course, dead state can become alive again.\",\"Alive: States that have been encountered, but possibly have unvisited next states. These are considered alive. Initially, the only alive state is xI​.\",\"Forward Search\",\"Q.insert(x_I) and mark x_I as visited. while Q not empty do: x = Q.GetFirst() if x in x_G: return SUCCESS for all u in U(x): x' = f(x,u) if x' not visited: Q.insert(x') mark x' visited else: Resolve duplicate x' Return FAILURE \",\"The above is a general template of forward search algorithm. Two focuses are presented here: How efficient is the test to determine whether x∈XG​ in line 4? How can one tell whether x′ has been visited in line 8 and line 9?\"]},\"216\":{\"h\":\"2.2.2 Particular Forward Search Methods\",\"t\":[\"This section presents several search algorithms, each of which constructs a search tree. Each search algorithm is a special case of the algorithm of the forward search algorithm template demonstrated before, obtained by defining a different sorting function for Q. Most of these are just classical graph search algorithms.\",\"Breath First: Specify Q as a First-In First-Out (FIFO) queue. All plans that have k steps are exhausted before plans with k + 1 steps are investigated. Therefore, breadth first guarantees that the first solution found will use the smallest number of steps. The asymptotic running time of breadth-first search is O(∣V∣+∣E∣).\",\"Depth First: Specify Q as a First-In Last-Out (FILO) stack. The running time of depth first search is also O(∣V∣+∣E∣).\",\"Dijkstra’s algorithm: Use cost-to-come (distance between initial state and current state), short for Function C，C(x) to sort Q.\",\"A*: Incorporate a heuristic estimate of the cost called cost-to-go (distance between current state and goal state), short for G(x) with C(x).\",\"Best first: Only use G(x) to sort Q.\",\"Iterative deepening: An approach integrates Breath first and Depth first method. That means performs Depth first search at i depth (i=1, 2, 3....max depth). Initially, i is equal to 1 and will increase with step going on. For example, if i = 1, the algorithm cannot find XG​. Then i will be 2, perform the same operation. If we still cannot find the solution, i will be 3 until we reach XG​.\"]},\"217\":{\"h\":\"2.2.3 Other General Search Schemes\",\"t\":[\"Backward search: For many planning problems, it might be the case that the branching factor is large when starting from xI. In this case, it might be more efficient to start the search at a goal state and work backward until the initial state is encountered.\",\"BACKWARD SEARCH\",\"Q.insert(x_G) and mark x_G as visited. while Q not empty do: x = Q.GetFirst() if x in xI: return SUCCESS for all u^-1 in U(x)^-1: x' = f^-1(x,u^-1) if x' not visited: Q.insert(x') mark x' visited else: Resolve duplicate x' Return FAILURE \",\"Bidirectional search: One tree is grown from the initial state, and the other is grown from the goal state. The search terminates with success when the two trees meet. Failure occurs if either priority queue has been exhausted.\",\"BIDIRECTIONAL SEARCH\",\"Q_G.insert(X_G) and mark x_G as visited. Q_I.insert(X_I) and mark x_I as visited. while Q_G and Q_I not empty do: x = Q_I.GetFirst() if x already visited from x_G return SUCCESS for all u in U(x): x' = f(x,u) if x' not visited: Q_I.insert(x') mark x' visited else: Resolve duplicate x' x = Q_G.GetFirst() if x already visited from x_I return SUCCESS for all u^-1 in U(x)^-1: x' = f^-1(x,u^-1) if x' not visited: x_G.insert(x') mark x' visited else: Resolve duplicate x' Return FAILURE \"]},\"218\":{\"h\":\"2.2.4 A Unified View of the Search Methods\",\"t\":[\"For all search methods, there usually involves the following 6 steps:\",\"Initialization: Initial graph G(V, E) and include some starting states in empty V, which could be XG​ or XI​ (Bidirectional search or backward search)\",\"Select Vertex: Select states in priority queue Q sorted with some rules.\",\"Apply an Action: Obtain a new state x' from f(x, u).\",\"Insert a Directed Edge into the Graph: If certain algorithm-specific tests are passed, then generate an edge from x to x' for the forward case or an edge from xnew to x for the backward case. If x' is not yet in V , it will be inserted into V.\",\"Check for Solution: Determine whether G encodes a path from XI​ to XG​. If there is a single search tree, then this is trivial. If there are two or more search trees, then this step could be expensive.\",\"Return to Step 2: Iterate unless a solution has been found or an early termination condition is satisfied, in which case the algorithm reports failure.\"]},\"219\":{\"h\":\"2.3 Discrete Optimal Planning\",\"t\":[\"This section discusses optimal planning problems involving optimizing time, distance, energy consumed.\",\"Formulation 2.2(Discrete Fixed-Length Optimal Planning)\",\"All of the components from Formulation 2.1 will be inherited in this section like X,U(x),f,xI​,xG​. Notably, here X is finite.\",\"The number of the stages, K, is defined, which is the exact length of the plan. It can be measured as the number of the actions. xk+1​ is obtained after uk​ is applied.\",\"Introduce cost functional:\",\"L(πK​)=sumk=1K​l(xk​,uk​)+LF​(xF​)\",\"L denote a stage-additive cost (or loss) functional, which is applied to a K-step plan, πK​. This means that the sequence (u1​,......,uK​) of actions and the sequence (x1​,......,xK+1​) of states may appear in an expression of L.\",\"For convenience, let F denote the final stage, F=K+1 (the application of uK​ advances the stage to K+1)\",\"The cost term l(xk​,uk​) yields a real value for every xk​∈Xanduk​∈U(xk​).\",\"The final term lF​(xF​) is outside of the sum and is defined as lF​(xF​)=0 if xF​∈XG​, and lF​(xF​)=∞ otherwise.\",\"Distinguish\",\"l(xk​,uk​) is the cost term after applying uk​ at xk​ while f(xk​,uk​) is the state transition fuction to obtain xk+1​\"]},\"220\":{\"h\":\"2.3.1 Optimal Fixed-Length Plans\",\"t\":[\"This section will mainly discuss the value iteration algorithm, which is to iteratively compute optimal cost-to-go (or cost-to-come) functions over the state space. In some conditions, it can be reduced to Dijkstra algorithm. There are mainly two versions of this algorithm, namely backward value iteration and forward value iteration.\"]},\"221\":{\"h\":\"2.3.1.1 Backward value iteration\",\"t\":[\"Firstly, we will introduce a new cost fuctional called Gk∗​, which represents the cost-to-go fuction accumulated through stage k to F. It can be written as the following equation:\",\"Gk∗​(xk​)=uk...uKmin​sumi=kK​l(xk​,uk​)+lF​(xF​)\",\"(1)\",\"This can be converted to the following equation (the proof process is omitted here as the formula will be well understood from its definition):\",\"Gk∗​(xk​)=ukmin​l(xk​,uk​)+Gk+1∗​(xk+1​)\",\"(2)\",\"This produces the recurrence, which can be used to obtain Gk∗​(xk​) iteratively from Gk+1∗​(xk+1​). It's like:\",\"GF∗​(xF​)→GK∗​(xK​)→GK−1∗​(xK−1​)→...→Gk∗​(xk​)→...→G1∗​(x1​)\",\"x1​ may contain the xI​.\",\"Example 1\",\"Figure 1 A five-state example. Each vertex represents a state, and each edge represents an input that can be applied to the state transition equation to change the state. The weights on the edges represent l(xk, uk) (xk is the originating vertex of the edge).\",\"Suppose that K=4,xI​=a,xG​=d. Hence, there will be four iterations by constructing G4∗​(x4​),G3∗​(x3​),G2∗​(x2​),G1∗​(x1​).\",\"Firstly, G5∗​(x5​)=xF​, For state a, b, c, e, they are not in xG​, so each value of them is ∞. For state d, the value is 0.\",\"K=4,G4∗​(x4​)=u4min​l(x4​,u4​)+G5∗​(x5​), x5​ can be a, b, c, d, e. Let's assume a as the current state(x4​) for instance. G5∗​(c)=∞, the equation goes to G4∗​(a)=u4​min​l(a,u4​)+G5∗​(x5​). Here, x5​ can be a, b, c, d, e. u4​ is the edge from a to x5​. We need to find out the smallest of the five combinations of a and x5​(a,b,c,d,e). Obviously, all of them is ∞.\",\"Let's take b, c as the x4​, respectively. You can see that G4∗​(b)=l(b,ub​d)+G5∗​(d)=4+0=4. G4∗​(c)=l(c,uc​d)+G5∗​(d)=1+0=1.\",\"K=3, the potential options of x4​=b,c. You need to take a,b,c,d,e as x3​ to calculate their optimal value G3∗​(x3​). For example, d as x3​. There are five circumstances, in which 3 of them are ∞(a, d, e). So the left two are G3∗​(d)=l(d,ud​c)+G3∗​(c)=1+1=2. G3∗​(d)=l(d,ud​b)+G3∗​(b)=∞+0=∞ So G3∗​(d)=2.\",\"In this way can you easily obtain G2∗​(x2​),G1∗​(x1​). The results are shown in the following table.\",\"a\",\"b\",\"c\",\"d\",\"e\",\"G₅*\",\"∞\",\"∞\",\"∞\",\"0\",\"∞\",\"G₄*\",\"∞\",\"4\",\"1\",\"∞\",\"∞\",\"G₃*\",\"6\",\"2\",\"∞\",\"2\",\"∞\",\"G₂*\",\"4\",\"6\",\"3\",\"∞\",\"∞\",\"G₁*\",\"6\",\"4\",\"5\",\"4\",\"∞\"]},\"222\":{\"h\":\"2.3.1.2 Forward value iteration\",\"t\":[\"The ideas from Section 2.3.1.1 may be recycled to yield a symmetrically equivalent method that computes optimal cost-to-come functions from the initial stage.\",\"In the backward case, xG​ must be fixed, and in the forward case, xI​ must be fixed.\",\"Symmetrically, here we introduce Ck∗​, which denotes the optimal cost-to-come value from stage 1 to k. lI​ serves as the same role of lF​. That is\",\"Ck∗​(x1​)=lI​(x1​)\",\"in which lI​ is a new function that yields lI​(xI​)=0, and lI​(x)=∞ for all x=xI​. Thus, any plans that try to start from a state other than xI​ will immediately receive infinite cost.\",\"Likewise, we can get the same equation:\",\"Ck∗​(xk​)=u1...uk−1min​sumi=1k−1​l(xk​,uk​)+lI​(xI​)\",\"(3)\",\"Also the recurrence:\",\"Ck∗​(xk​)=uk−1min​l(xk​,uk​)+Ck−1∗​(xk−1​)\",\"(4)\",\"Example 2\",\"We can still use the net in Figure 1, perform the forward iteration:\",\"Suppose K=4, we need to calculate C4∗​ for a, b, c, d, e. Each has 5 options of Ck−1∗​(xk−1​). For instance, C4∗​(xc​), there exsits a-c, b-c, c-c, d-c, e-c. C3∗​(xe​)=∞, la−c​,lc−c​=∞. Thus we only need to compare lb−c​+C3∗​(xb​) and ld−c​+C3∗​(xd​). The former is smaller, which equals to 5 while the latter is 7.\",\"a\",\"b\",\"c\",\"d\",\"e\",\"C₁*\",\"0\",\"∞\",\"∞\",\"∞\",\"∞\",\"C₂*\",\"2\",\"2\",\"∞\",\"∞\",\"∞\",\"C₃*\",\"4\",\"4\",\"3\",\"6\",\"∞\",\"C₄*\",\"4\",\"6\",\"5\",\"4\",\"7\",\"C₅*\",\"6\",\"6\",\"5\",\"6\",\"5\"]},\"223\":{\"h\":\"2.3.2 Optimal Plans of Unspecified Lengths\",\"t\":[\"In section 2.3.1 we learn algorithm solving optimal fixed-length plans. However, it is obviously unreasonable. To begin with, we don't know the exact length of the solution. We need to set it in advance, which can be inappropriate. In example 1, we can obtain the optimal path from G2​(a). But we repeat another redundant iteration in G1​.\",\"So how to address this issue? That is variable-length plan.\",\"The value-iteration method, originally used for fixed-length plans, is generalized for plans of different lengths. There is no upper limit to how long a plan can be, making this approach a true generalization of earlier fixed-length formulations.\",\"How to accomplish this? Here we introduce a special \\\"termination\\\" action, denoted as uT​. This action allows a plan to stop at any state, effectively saying, \\\"We are done.\\\" Once this action is applied, the state no longer changes, and no additional costs are incurred. This enables plans of different lengths to be handled uniformly. For example, a two-step plan (u1​,u2​) that reaches the goal can be extended to a longer plan by simply repeating the termination action without changing the cost like (u1​,u2​,uT​,uT​,uT​).\",\"The termination action is applied when the system has reached a goal state, meaning the current state x∈xG​.\",\"Once the goal is achieved, further actions are unnecessary, and the cost will not increase. So, the system applies the termination action to \\\"stop\\\" further planning or changes in the state.\",\"For iteration going on, we introduce two similar formulas.\",\"For backward value iteration:\",\"This formula calculates the optimal action (u∗) at a given state x:\",\"u∗=argu∈U(x)min​(l(x,u)+G∗(f(x,u)))\",\"(5)\",\"l(x,u): Represents the cost incurred by taking action u in state x.\",\"G∗(f(x,u)): The optimal cost-to-go function, which estimates the remaining cost to the goal from the next state f(x,u), the state that results from applying action $u $ to state x.\",\"The formula minimizes the total cost, which is the sum of the immediate cost l(x,u) and the cost-to-go from the resulting state. The argmin part means we are selecting the action u∗ that yields the lowest total cost.\",\"For forward value iteration:\",\"u=argu−1∈U−1(x)min​(C∗(f−1(x,u−1))+l(f−1(x,u−1),u′))\",\"(6)\",\"f−1(x,u−1): Refers to the state from which action u−1 would bring the system into state x.\",\"C∗: The optimal cost-to-come function, analogous to G∗, but in a forward direction. It tells us the best cost incurred to reach x from some previous state.\",\"l(f−1(x,u−1),u′): Represents the cost of the action leading from the predecessor state to x.\",\"In this way can we not rely on the specified k. Since we select the action u∗ that yields the lowest total cost every iteration.\",\"Distinguish\",\"Key Differences between (2) (4) in fixed-length planning and (5) (6) in variable-length planning\",\"Fixed vs. Variable Length:\",\"(2)(4) is used in the context of fixed-length planning, where the number of stages is known and the goal is to minimize the cost over a set number of steps.\",\"(5)(6) is for variable-length planning, where the number of stages is unspecified, and you want to minimize the overall cost-to-go/cost-to-come, with no constraint on the number of steps.\",\"Stage Dependency:\",\"(2)(4) depends on the stage index k (since the cost-to-go depends on the specific stage).\",\"(5)(6) is independent of any stage index because it is used for unspecified-length plans, where the focus is on minimizing the total cost regardless of how long the plan takes.\",\"Similarity\",\"Both formulas aim to minimize the total cost by selecting the optimal action at each state based on a cost function that combines immediate cost and the future cost-to-go/past cost-to-come. The mechanism for selecting actions is the same—iteratively finding the action that leads to the least total cost.\",\"Example 1 will be changed into:\",\"a\",\"b\",\"c\",\"d\",\"e\",\"G₀*\",\"∞\",\"∞\",\"∞\",\"0\",\"∞\",\"G₋₁*\",\"∞\",\"4\",\"1\",\"0\",\"∞\",\"G₋₂*\",\"6\",\"2\",\"1\",\"0\",\"∞\",\"G₋₃*\",\"4\",\"2\",\"1\",\"0\",\"∞\",\"G₋₄*\",\"4\",\"2\",\"1\",\"0\",\"∞\",\"G*\",\"4\",\"2\",\"1\",\"0\",\"∞\"]},\"224\":{\"h\":\"2.3.3 Dijkstra Revisited\",\"t\":[\"The key differences between Dijkstra algorithm and forward value iteration algorithm are shown as below:\",\"Feature\",\"Dijkstra's Algorithm\",\"Forward Value Iteration\",\"Cost Metric\",\"Minimizes cost-to-come (from start to current state)\",\"Propagates cost-to-come in forward iteration\",\"Approach\",\"Greedy, explores states with minimum cost-to-come\",\"Dynamic programming, iterates over all states simultaneously\",\"Exploration Strategy\",\"Expands one state at a time based on smallest cost-to-come\",\"Updates all states simultaneously in each iteration\",\"Priority\",\"Uses a priority queue to expand least-cost states first\",\"Does not prioritize; updates globally\",\"Set of Alive States\",\"Yes, maintains a set of \\\"alive\\\" states (states yet to be finalized)\",\"No, updates all states without maintaining alive states\",\"Best Use Case\",\"Finding the shortest path to a goal state\",\"Computing global cost-to-come for all states\",\"Efficiency\",\"More efficient for single-goal problems\",\"Less efficient; explores the entire state space\",\"If Dijkstra’s algorithm seems so clever, then why have we spent time covering the value-iteration method? For some problems it may become too expensive to maintain the sorted queue, and value iteration could provide a more efficient alternative. A more important reason is that value iteration extends easily to a much broader class of problems. Examples include optimal planning over continuous state spaces (Sections 8.5.2 and 14.5), stochastic optimal planning (Section 10.2), and computing dynamic game equilibria (Section 10.5).\",\"Dijkstra’s algorithm belongs to a broader family of label-correcting algorithms, which all produce optimal plans by making small modifications to the general forward-search algorithm.\",\"FORWARD LABEL CORRECTING(xG​)\",\"1 Set C(x) = ∞ for all x(except xI), and set C(xI) = 0. 2 Q.Insert(xI) 3 while Q not empty do: 4 x ← GetFirst(Q) 5 for all u in U(x) 6 x' ← f(x, u) 7 if C(x) + l(x, u) < min{C(x'), C(xG)} 8 C(x') ← C(x) + l(x, u) 9 if x' is not xG 10 Q.Insert(x') \",\"Notably, the label-correcting approach uses the cost at the goal state to prune away many candidate paths; this is shown in line 7. Thus, it is only formulated to work for a single goal state; it can be adapted to work for multiple goal states, but performance degrades. The motivation for including C(xG​) in line 7 is that there is no need to worry about improving costs at some state, x′, if its new cost-to-come would be higher than C(xG​); there is no way it could be along a path that improves the cost to go to xG​.\"]},\"225\":{\"h\":\"2.4 Using Logic to Formulate Discrete Planning\",\"t\":[\"For many discrete planning problems that we would like a computer to solve, the state space is enormous (e.g., 10100 states). Therefore, substantial effort has been invested in constructing implicit encodings of problems in hopes that the entire state space does not have to be explored by the algorithm to solve the problem.\",\"Pros and Cons of logic-based representations:\"]},\"226\":{\"h\":\"2.4.1 A STRIPS-Like Representation\",\"t\":[\"STRIPS-like representations have been the most common logic-based representations for discrete planning problems. This refers to the STRIPS system, which is considered one of the first planning algorithms and representations; its name is derived from the STanford Research Institute Problem Solver. There are many variations of STRIPS-like representations. Here is one formulation:\",\"A finite, nonempty set I of instances. Instances are just the object existing in the world like books or trees.\",\"A finite, nonempty set P of predicates, which are binary-valued (partial) functions of one of more instances. Each application of a predicate to a specific set of instances is called a positive literal. A logically negated positive literal is called a negative literal.\",\"The predicates can form the basic properties or statements of certain instances. For example, a predicate called Under might be used to indicate things like Under(Book, Table) (the book is under the table) or Under(Dirt, Rug). (Here book and table are arguments)\",\"A predicate can be interpreted as a kind of function that yields true or false values;\",\"however, it is important to note that it is only a partial function which needs argument(s) to be inserted. And here instances are the argument(s). It might not be desirable to allow any instance to be inserted as an argument to the predicate.(In other words, some combinations of predicates and instance are obviously true or false.)\",\"Summary: predicates(argument 1, argument 2, argument 3..., argument n) = positive/negative literal (instances are arguments)\",\"A finite, nonempty set O of operators, each of which has: 1) preconditions, which are positive or negative literals that must hold for the operator to apply, and 2) effects, which are positive or negative literals that are the result of applying the operator.\",\"An initial setS which is expressed as a set of positive literals. Negative literals are implied. For any positive literal that does not appear in S, its corresponding negative literal is assumed to hold initially.\",\"A goal setG which is expressed as a set of both positive and negative literals.\",\"Summary: preconditions, effects, initial sets and goal sets are all made up of literals.\",\"The process involves:\",\"Initially, we have several instances and define several predicates so as to form literals. We set the initial sets and goal sets. Based on corresponding preconditions, we need to find the operators that produces the effects we want (reach the destination).\",\"Example 3\",\"An example that involves putting batteries into a flashlight\",\"Imagine a planning problem that involves putting two batteries into a flashlight, as shown in the following figure. The set of instances are\",\"I={Battery1,Battery2,Cap,Flashlight}\",\"Two different predicates will be defined, On and In, each of which is a partial function on I. The predicate On may only be applied to evaluate whether the Cap is On the Flashlight and is written as On(Cap, Flashlight). The predicate In may be applied in the following two ways: In(Battery1, Flashlight), In(Battery2, F lashlight), to indicate whether either battery is in the flashlight.\",\"Recall that predicates are only partial functions in general. For the predicate In, it is not desirable to apply any instance to any argument. For example, it is meaningless to define In(Battery1, Battery1) and In(F lashlight, Battery2) (they could be included in the model, always retaining a negative value, but it is inefficient).\",\"The initial set is\",\"S={On(Cap,Flashlight)}.\",\"Based on S, both ¬In(Battery1, F lashlight) and ¬In(Battery2, Flashlight) are assumed to hold. Thus, S indicates that the cap is on the flashlight, but the batteries are outside. The goal state is\",\"G=On(Cap,Flashlight),In(Battery1,Flashlight),In(Battery2,Flashlight)\",\"which means that both batteries must be in the flashlight, and the cap must be on. The set O consists of the four operators, which are\",\"Name\",\"Preconditions\",\"Effects\",\"PlaceCap\",\"{¬On(Cap,Flashlight)}\",\"{On(Cap,Flashlight)}\",\"RemoveCap\",\"{On(Cap,Flashlight)}\",\"{¬On(Cap,Flashlight)}\",\"Insert(i)\",\"{¬On(Cap,Flashlight),¬In(i,Flashlight)}\",\"{In(i,Flashlight)}\",\"Here is a plan that reaches the goal state in the smallest number of steps:\",\"(Remove(Cap),Insert(Battery1),Insert(Battery2),Place(Cap))\",\"In words, the plan simply says to take the cap off, put the batteries in, and place the cap back on.\",\"This example appears quite simple, and one would expect a planning algorithm to easily find such a solution. It can be made more challenging by adding many more instances to I, such as more batteries, more flashlights, and a bunch of objects that are irrelevant to achieving the goal. Also, many other predicates and operators can be added so that the different combinations of operators become overwhelming.\"]},\"227\":{\"h\":\"2.4.2 Converting to the State-Space Representation\",\"t\":[\"By converting the logic-based representation to the state-space representation, we can easily use the algorithm introduced previously to solve the problem.\",\"Up to now, the notion of “state” has been only vaguely mentioned in the context of the STRIPS-like representation. Now consider making this more concrete. Suppose that every predicate has k arguments, and any instance could appear in each argument. This means that there are ∣P∣∣I∣k complementary pairs, which corresponds to all of the ways to substitute instances into all arguments of all predicates.(obviously there are p predicates, all of which has k places that each has I options(instances))\",\"To express the state, a positive or negative literal must be selected from every complementary pair. For convenience, this selection can be encoded as a binary string by imposing a linear ordering on the instances and predicates.\",\"Using Example 3, the state might be specified in order as\",\"(On(Cap,Flashlight),¬In(Battery1,Flashlight1),In(Battery2,Flashlight))\",\"Using a binary string, each element can be “0” to denote a negative literal or “1” to denote positive literal. The encoded state is x = 101 for the formula above.\",\"If any instance can appear in the argument of any predicate, then the length of the string is ∣P∣∣I∣k. The total number of possible states of the world that could possibly be distinguished corresponds to the set of all possible bit strings. This set has size\",\"2∣P∣∣I∣k.\",\"(Each place has 0 or 1 two options)\",\"We can see that a small number of P and I can generate enormous state spaces, leading inefficient performance of algorithm.\",\"The next step in converting to a state-space representation is to encode the initial state xI​ as a string. The goal set, XG​, is the set of all strings that are consistent with the positive and negative goal literals. This can be compressed by extending the string alphabet to include a “don’t care” symbol, δ.(Namely 1 or 0 in this place are both acceptable.) A single string that has a “0” for each negative literal, a “1” for each positive literal, and a “δ” for all others would suffice in representing any XG that is expressed with positive and negative literals.\",\"Now convert the operators. To apply the search techniques of Section 2.2, note that it is not necessary to determine U(x) explicitly in advance for all x∈X. Instead, U(x) can be computed whenever each x is encountered for the first time in the search.\",\"The effects of the operator are encoded by the state transition equation. From a given x∈X, the next state, f(x,u), is obtained by flipping part of the bits(0 → 1 or inversely) when operators applied.\"]},\"228\":{\"h\":\"2.5 Logic-Based Planning Methods\",\"t\":[\"This section discusses how to adapt the value-iteration method to work under the logic-based representation, yielding optimal plans.\"]},\"229\":{\"h\":\"2.5.1 Searching in a Space of Partial Plans\",\"t\":[\"One alternative to searching directly in X is to construct partial plans without reference to particular states. By using the operator representation, partial plans can be incrementally constructed. The idea is to iteratively achieve required subgoals in a partial plan while ensuring that no conflicts arise that could destroy the solution developed so far.\",\"A partial plan σ is defined as\",\"A set Oσ​ of operators that need to be applied. If the operators contain variables, these may be filled in by specific values or left as variables. The same operator may appear multiple times in Oσ​, possibly with different values for the variables.\",\"A partial ordering relation ≺σ​ on Oσ​, which indicates for some pairs o1,o2∈Oσ​ that one must appear before other: o1≺σ​o2.\",\"A set Bσ​ of binding constraints, in which each indicates that some variables across operators must take on the same value.\",\"A set Cσ​ of causal links, in which each is of the form (o1,l,o2) and indicates that o1 achieves the literal l for the purpose of satisfying a precondition of o2(In other words, o1 achieved l which is the precondition of o2).\",\"Example 4 (A Partial Plan)\",\"Each partial plan encodes a set of possible plans. Recall the model from Example 3. Suppose\",\"Oσ​={RemoveCap,Insert(Battery1)}.\",\"A sensible ordering constraint is that\",\"RemoveCap≺σ​Insert(Battery1)\",\"A causal link\",\"(RemoveCap,¬On(Cap,Flashlight),Insert(Battery1))\",\"indicates that the RemoveCap operator achieves the literal ¬On(Cap,Flashlight), which is a precondition of Insert(Battery1).\",\"There are no binding constraints for this example. The partial plan implicitly represents the set of all plans for which RemoveCap appears before Insert(Battery1), under the constraint that the causal link is not violated.\",\"A vertex in the partial-plan search graph is a partial plan, and an edge is constructed by extending one partial plan to obtain another partial plan that is closer to completion. Although the general template is simple, the algorithm performance depends critically on the choice of initial plan and the particular flaw that is resolved in each iteration. One straightforward generalization is to develop multiple partial plans and decide which one to refine in each iteration.\",\"In early works, methods based on partial plans seemed to offer substantial benefits; however, they are currently considered to be not “competitive enough” in comparison to methods that search the state space. One problem is that it becomes more difficult to develop application-specific heuristics without explicit references to states. Also, the vertices in the partial-plan search graph are costly to maintain and manipulate in comparison to ordinary states.\"]},\"230\":{\"c\":[\"PR\"]},\"231\":{\"h\":\"\",\"t\":[\"404 Not Found\"]},\"232\":{\"h\":\"ML\"},\"233\":{\"h\":\"Traffic\"},\"234\":{\"h\":\"English\"},\"235\":{\"h\":\"Daily\"},\"236\":{\"h\":\"Guitar\"},\"237\":{\"h\":\"Paper\"},\"238\":{\"h\":\"Path\"},\"239\":{\"h\":\"Pla\"}},\"dirtCount\":0,\"index\":[[\"≺σ​\",{\"1\":{\"229\":1}}],[\"δ\",{\"1\":{\"227\":2}}],[\"¬on\",{\"1\":{\"226\":3,\"229\":2}}],[\"¬in\",{\"1\":{\"226\":3,\"227\":1}}],[\"<\",{\"1\":{\"224\":1}}],[\"<var\",{\"1\":{\"66\":1}}],[\"←\",{\"1\":{\"224\":3}}],[\"∞\",{\"1\":{\"221\":15,\"222\":8,\"223\":10,\"224\":1}}],[\"^\",{\"1\":{\"217\":2}}],[\"^2\",{\"1\":{\"161\":1}}],[\"^2+\",{\"1\":{\"161\":1}}],[\"论文原址\",{\"1\":{\"203\":1}}],[\"ϵ\",{\"1\":{\"200\":2}}],[\"ωs\",{\"1\":{\"198\":2}}],[\"ω\",{\"1\":{\"198\":3}}],[\"休止符是记录不发声间断的符号\",{\"1\":{\"182\":1}}],[\"休止符\",{\"0\":{\"182\":1},\"1\":{\"182\":1}}],[\"延续一拍\",{\"1\":{\"181\":1}}],[\"延音符\",{\"1\":{\"174\":1}}],[\"例子\",{\"1\":{\"181\":1}}],[\"例如下图\",{\"1\":{\"181\":1}}],[\"例如\",{\"1\":{\"78\":1}}],[\"又如下图\",{\"1\":{\"181\":1}}],[\"又称为再励学习\",{\"1\":{\"110\":1}}],[\"只拍下去为半拍\",{\"1\":{\"181\":1}}],[\"手拍下来再上来为一拍\",{\"1\":{\"181\":1}}],[\"手腕带动小臂\",{\"1\":{\"177\":1}}],[\"手腕发力\",{\"1\":{\"177\":1}}],[\"闷音\",{\"1\":{\"177\":1}}],[\"爬音阶和爬格子\",{\"1\":{\"177\":1}}],[\"小指无名指抵在护板\",{\"1\":{\"177\":1}}],[\"小研究\",{\"1\":{\"4\":1}}],[\"右手整体悬空\",{\"1\":{\"177\":1}}],[\"右手小鱼际放在5\",{\"1\":{\"177\":1}}],[\"右手固定与支撑方式\",{\"1\":{\"177\":1}}],[\"右四个出口\",{\"1\":{\"155\":1}}],[\"发力方式\",{\"1\":{\"177\":1}}],[\"发现自己之前做过的很多小项目\",{\"1\":{\"2\":1}}],[\"音符是记录音长短的符号\",{\"1\":{\"181\":1}}],[\"音符\",{\"0\":{\"181\":1}}],[\"音符与休止符\",{\"0\":{\"180\":1}}],[\"音孔右侧边缘位置\",{\"1\":{\"177\":1}}],[\"音乐区\",{\"1\":{\"2\":1}}],[\"角度\",{\"1\":{\"177\":1}}],[\"拨弦符号\",{\"1\":{\"177\":1}}],[\"拨弦区域\",{\"1\":{\"177\":2}}],[\"拨弦位置\",{\"1\":{\"177\":1}}],[\"拨片与琴弦角度\",{\"1\":{\"177\":1}}],[\"拨片与琴弦角度如下图\",{\"1\":{\"177\":1}}],[\"拨片1\",{\"1\":{\"177\":1}}],[\"拨片一般捏在1\",{\"1\":{\"177\":1}}],[\"拨片分类\",{\"1\":{\"177\":1}}],[\"拨片\",{\"0\":{\"176\":1,\"177\":1}}],[\"靠手指指纹与拨片表面摩擦力来固定\",{\"1\":{\"177\":1}}],[\"拿的位置\",{\"1\":{\"177\":1}}],[\"仅表示音符相连\",{\"1\":{\"174\":1}}],[\"仅仅用来评估模最终模型的泛化能力\",{\"1\":{\"110\":1}}],[\"连音符是不同音高相连\",{\"1\":{\"174\":1}}],[\"连音符与延音符\",{\"1\":{\"174\":1}}],[\"左手拇指固定后就不要来回移动了\",{\"1\":{\"178\":1}}],[\"左侧为下拨\",{\"1\":{\"177\":1}}],[\"左右手势\",{\"1\":{\"174\":1}}],[\"左三个入口\",{\"1\":{\"155\":1}}],[\"什么节奏\",{\"1\":{\"172\":1}}],[\"什么调\",{\"1\":{\"172\":1}}],[\"什么是泛化性能\",{\"1\":{\"110\":1}}],[\"什么是机器学习和深度学习\",{\"1\":{\"110\":1}}],[\"切分音重音在第二个拍上\",{\"1\":{\"169\":1}}],[\"情感共鸣\",{\"1\":{\"169\":1}}],[\"引人晃动\",{\"1\":{\"169\":1}}],[\"引入慢启动系数s\",{\"1\":{\"146\":1}}],[\"律动\",{\"1\":{\"169\":1}}],[\"合拍\",{\"1\":{\"169\":1}}],[\"合并或分裂的决定需要检查和估算大量的对象或簇\",{\"1\":{\"116\":1}}],[\"强调强弱\",{\"1\":{\"169\":1}}],[\"强调长短\",{\"1\":{\"169\":1}}],[\"强化学习是一个学习+决策的过程\",{\"1\":{\"110\":1}}],[\"强化学习的奖惩概念没有正确和错误之分\",{\"1\":{\"110\":1}}],[\"强化学习的目标与监督学习目标不同\",{\"1\":{\"110\":1}}],[\"强化学习看重行为序列下的长期收益\",{\"1\":{\"110\":1}}],[\"强化学习执行多步后反馈\",{\"1\":{\"110\":1}}],[\"强化学习\",{\"1\":{\"110\":2,\"114\":1}}],[\"节拍的节是循环的节点\",{\"1\":{\"169\":1}}],[\"节拍是强拍和弱拍按照一定顺序的循环\",{\"1\":{\"169\":1}}],[\"节拍和律动\",{\"0\":{\"169\":1}}],[\"节奏简单可以理解为音的长短和强弱\",{\"1\":{\"181\":1}}],[\"节奏型是曲中典型的\",{\"1\":{\"169\":1}}],[\"节奏是用音强弱组织起来的音的长短关系\",{\"1\":{\"169\":1}}],[\"节奏\",{\"0\":{\"169\":1}}],[\"地\",{\"1\":{\"168\":1}}],[\"地图主要是反应道路周围的环境\",{\"1\":{\"154\":1}}],[\"向前的\",{\"1\":{\"168\":1}}],[\"事\",{\"1\":{\"168\":1}}],[\"事故等紧急情况下的交通状况\",{\"1\":{\"167\":1}}],[\"防止\",{\"1\":{\"168\":1}}],[\"阻止\",{\"1\":{\"168\":1}}],[\"肤浅的\",{\"1\":{\"168\":1}}],[\"难以去做的\",{\"1\":{\"168\":1}}],[\"沉重的\",{\"1\":{\"168\":1}}],[\"笨重的\",{\"1\":{\"168\":1}}],[\"渐进的\",{\"1\":{\"168\":1}}],[\"单调性\",{\"1\":{\"168\":1}}],[\"单车道\",{\"1\":{\"147\":1}}],[\"遗忘\",{\"1\":{\"168\":1}}],[\"忽略\",{\"1\":{\"168\":1}}],[\"忽略了真实的输入输出\",{\"1\":{\"111\":1}}],[\"省略\",{\"1\":{\"168\":1}}],[\"冗余的\",{\"1\":{\"168\":1}}],[\"拥堵和行人安全的影响\",{\"1\":{\"167\":1}}],[\"帮助制定有效的应急响应策略\",{\"1\":{\"167\":1}}],[\"帮助规划者评估其对交通流量\",{\"1\":{\"167\":1}}],[\"帮助开发更为智能的决策方案\",{\"1\":{\"167\":1}}],[\"语音识别等任务的机器学习模型\",{\"1\":{\"166\":1}}],[\"适合扫弦\",{\"1\":{\"177\":1}}],[\"适合弹奏1\",{\"1\":{\"177\":1}}],[\"适合分解练习\",{\"1\":{\"177\":1}}],[\"适应能力的\",{\"1\":{\"166\":1}}],[\"适用于大型数据集\",{\"1\":{\"116\":1}}],[\"适用于线性可分的数据集\",{\"1\":{\"115\":1}}],[\"理解\",{\"1\":{\"166\":1}}],[\"理解attention的本质\",{\"1\":{\"49\":1}}],[\"具体速度会在谱中标出\",{\"1\":{\"181\":1}}],[\"具体内容见下图\",{\"1\":{\"181\":1}}],[\"具体讲解可以看这篇文章与这篇文章\",{\"1\":{\"126\":1}}],[\"具有学习\",{\"1\":{\"166\":1}}],[\"得到的具有\",{\"1\":{\"166\":1}}],[\"架构等\",{\"1\":{\"166\":1}}],[\"大\",{\"1\":{\"169\":1}}],[\"大语言模型是利用深度学习技术\",{\"1\":{\"166\":1}}],[\"大小的滑动窗口在\",{\"1\":{\"161\":1}}],[\"大小分别为\",{\"1\":{\"155\":1}}],[\"热力图\",{\"1\":{\"162\":1}}],[\"动态演示\",{\"1\":{\"162\":1}}],[\"贴近现实中人是向前走的\",{\"1\":{\"161\":1}}],[\"矩阵进行计算\",{\"1\":{\"161\":1}}],[\"新选择原胞更新为占有状态\",{\"1\":{\"161\":1}}],[\"上拨下拨符号如下\",{\"1\":{\"177\":1}}],[\"上下拨弦\",{\"1\":{\"177\":1}}],[\"上下前三个方向不全都有人\",{\"1\":{\"161\":1}}],[\"上进行更新\",{\"1\":{\"161\":1}}],[\"上采样\",{\"1\":{\"78\":1}}],[\"~=0\",{\"1\":{\"161\":1}}],[\"依原胞潜力n\",{\"1\":{\"161\":1}}],[\"依次从需要计算的\",{\"1\":{\"161\":1}}],[\"依赖于全部的数据\",{\"1\":{\"115\":1}}],[\"依赖数据表达的距离测度\",{\"1\":{\"115\":1}}],[\"参数计算\",{\"0\":{\"161\":1}}],[\"参数设置\",{\"0\":{\"160\":1}}],[\"外加了两圈障碍\",{\"1\":{\"160\":1}}],[\"外加了一圈障碍\",{\"1\":{\"160\":1}}],[\"记录时刻平台信息\",{\"1\":{\"160\":1}}],[\"记录日常生活\",{\"1\":{\"4\":1}}],[\"迭代周期960s\",{\"1\":{\"162\":1}}],[\"迭代时间\",{\"1\":{\"160\":1}}],[\"迭代次数截断\",{\"1\":{\"111\":1}}],[\"迭代次数\",{\"1\":{\"110\":1}}],[\"到达终点人数\",{\"1\":{\"160\":1}}],[\"出发人数\",{\"1\":{\"160\":1}}],[\"出口\",{\"1\":{\"160\":1}}],[\"概率矩阵\",{\"1\":{\"160\":1}}],[\"概述\",{\"0\":{\"154\":1}}],[\"预留内存\",{\"1\":{\"160\":2}}],[\"预剪枝基于\",{\"1\":{\"113\":1}}],[\"预剪枝\",{\"1\":{\"113\":2}}],[\"位置矩阵\",{\"1\":{\"160\":1}}],[\"位置更新\",{\"1\":{\"144\":1,\"161\":3}}],[\"初始迭代次数\",{\"1\":{\"160\":1}}],[\"初始化平台\",{\"1\":{\"160\":1}}],[\"设置出口原胞潜力为\",{\"1\":{\"161\":1}}],[\"设置障碍\",{\"1\":{\"160\":2}}],[\"设置非障碍矩阵\",{\"1\":{\"160\":1}}],[\"障碍视为距离无穷\",{\"1\":{\"161\":1}}],[\"障碍\",{\"1\":{\"160\":1}}],[\"入口纵坐标\",{\"1\":{\"160\":1}}],[\"入口横坐标\",{\"1\":{\"160\":1}}],[\"部分代码解释\",{\"0\":{\"159\":1}}],[\"环境的动态影响\",{\"1\":{\"158\":1}}],[\"环境中静止物体的影响\",{\"1\":{\"158\":1}}],[\"处的元胞潜能\",{\"1\":{\"158\":1}}],[\"处元胞状态\",{\"1\":{\"158\":1}}],[\"代表全音符\",{\"1\":{\"181\":1}}],[\"代表一个四分音符+一个八分音符+两个十六分音符\",{\"1\":{\"181\":1}}],[\"代表1分钟60拍\",{\"1\":{\"181\":1}}],[\"代表不占有\",{\"1\":{\"158\":1}}],[\"代表占有\",{\"1\":{\"158\":1}}],[\"代表位置\",{\"1\":{\"158\":2}}],[\"代码从离平台近的位置向远处开始遍历\",{\"1\":{\"161\":1}}],[\"代码设定\",{\"1\":{\"161\":1}}],[\"代码流程图\",{\"1\":{\"159\":1}}],[\"代码整体思路如下\",{\"1\":{\"159\":1}}],[\"代码\",{\"0\":{\"149\":1}}],[\"划分为方格形\",{\"1\":{\"157\":1}}],[\"划分为三个主要分类\",{\"1\":{\"110\":1}}],[\"状态设置为占有\",{\"1\":{\"157\":1}}],[\"规定仿真时间为\",{\"1\":{\"155\":1}}],[\"均为\",{\"1\":{\"155\":1}}],[\"均值\",{\"1\":{\"122\":1}}],[\"平面示意图\",{\"1\":{\"155\":1}}],[\"平台长度\",{\"1\":{\"160\":1}}],[\"平台宽度\",{\"1\":{\"160\":1}}],[\"平台\",{\"1\":{\"155\":1}}],[\"平台基本信息\",{\"1\":{\"155\":1}}],[\"平移不变性是指输出结果对输入对小量平移基本保持不变\",{\"1\":{\"78\":1}}],[\"平移不变性\",{\"1\":{\"78\":1}}],[\"⏩\",{\"1\":{\"154\":1}}],[\"速度\",{\"1\":{\"154\":1}}],[\"速度属性\",{\"1\":{\"143\":1}}],[\"朝向\",{\"1\":{\"154\":1}}],[\"姿势\",{\"1\":{\"154\":1}}],[\"包含感知周围环境传感器的车辆的信息\",{\"1\":{\"154\":1}}],[\"包含做过语义分割处理的地图\",{\"1\":{\"154\":1}}],[\"包括了车长+距离前车的安全距离\",{\"1\":{\"143\":1}}],[\"包括其定义\",{\"1\":{\"126\":1}}],[\"视觉信息\",{\"1\":{\"154\":1}}],[\"轨迹信息\",{\"1\":{\"154\":1}}],[\"力的方向\",{\"1\":{\"154\":1}}],[\"冲突车辆\",{\"1\":{\"154\":1}}],[\"达到一定期望速度的acceleration\",{\"1\":{\"154\":1}}],[\"达到优化模型的效果\",{\"1\":{\"114\":1}}],[\"社会力模型由几种力组成\",{\"1\":{\"154\":1}}],[\"社会力模型\",{\"1\":{\"154\":1}}],[\"来模拟人与环境的交互\",{\"1\":{\"154\":1}}],[\"既研究整体特征也研究个体特征\",{\"1\":{\"154\":1}}],[\"介观\",{\"1\":{\"154\":1}}],[\"介于宏微观之间\",{\"1\":{\"154\":2}}],[\"紧急疏散人群移动特点\",{\"1\":{\"154\":1}}],[\"流向等\",{\"1\":{\"154\":1}}],[\"流程\",{\"1\":{\"112\":1}}],[\"密度\",{\"1\":{\"154\":1}}],[\"密度聚类\",{\"1\":{\"116\":1}}],[\"研究个体行为\",{\"1\":{\"154\":1}}],[\"研究整体移动特征如速度\",{\"1\":{\"154\":1}}],[\"研究行人行为及心理的特点的研究\",{\"1\":{\"154\":1}}],[\"建筑平面设计等方面有着广泛的应用\",{\"1\":{\"154\":1}}],[\"建立输入和输出之间的映射关系\",{\"1\":{\"110\":1}}],[\"疏散计划\",{\"1\":{\"154\":1}}],[\"行人随机从各个入口进入\",{\"1\":{\"155\":1}}],[\"行人每秒走一格\",{\"1\":{\"155\":1}}],[\"行人不得通过障碍\",{\"1\":{\"155\":1}}],[\"行人图像主要是反应行人的行为\",{\"1\":{\"154\":1}}],[\"行人流仿真按仿真规模可以大致分为三种\",{\"1\":{\"154\":1}}],[\"行人流仿真是通过模拟人群在不同环境下的移动\",{\"1\":{\"154\":1}}],[\"行人流仿真\",{\"0\":{\"153\":1}}],[\"行为\",{\"1\":{\"154\":1}}],[\"行驶\",{\"1\":{\"144\":1}}],[\"危害生命财产安全\",{\"1\":{\"150\":1}}],[\"避免因车辆换道引起交通事故\",{\"1\":{\"150\":1}}],[\"安全条件是指车辆在决定换道时要确定当前交通状况下换道对于自身和其他车辆是否安全\",{\"1\":{\"150\":1}}],[\"安全条件\",{\"1\":{\"150\":1}}],[\"换道动机是指驾驶员想换道的意愿和条件\",{\"1\":{\"150\":1}}],[\"换道动机\",{\"1\":{\"150\":1}}],[\"换道规则一般包括两个部分\",{\"1\":{\"150\":1}}],[\"两条细线是段落线\",{\"1\":{\"174\":1}}],[\"两条车道上的车辆在该时步均按照单车道元胞自动机模型中设计的演化更新规则运行\",{\"1\":{\"150\":1}}],[\"两线时间叫做间\",{\"1\":{\"172\":1}}],[\"两者都假设数据符合高斯分布\",{\"1\":{\"117\":1}}],[\"两者在降维时均使用了矩阵特征分解的思想\",{\"1\":{\"117\":1}}],[\"两者均可以对数据进行降维\",{\"1\":{\"117\":1}}],[\"完整代码\",{\"1\":{\"149\":1,\"162\":1}}],[\"9\",{\"1\":{\"158\":2,\"160\":4,\"161\":1,\"208\":1,\"215\":1,\"224\":1}}],[\"960s\",{\"1\":{\"155\":1}}],[\"9739\",{\"1\":{\"148\":1}}],[\"99\",{\"1\":{\"148\":1}}],[\"9054\",{\"1\":{\"148\":1}}],[\"9176\",{\"1\":{\"148\":1}}],[\"做fd图如下\",{\"1\":{\"148\":1}}],[\"含速度\",{\"1\":{\"148\":4}}],[\"黑线为小汽车\",{\"1\":{\"148\":1}}],[\"红线为货车\",{\"1\":{\"148\":1}}],[\"条件下\",{\"1\":{\"148\":1}}],[\"车流\",{\"1\":{\"146\":1}}],[\"车辆会以一定的换道概率进行换道\",{\"1\":{\"150\":1}}],[\"车辆在这个时步内按照设计的换道规则决定是否发生换道行为\",{\"1\":{\"150\":1}}],[\"车辆以最大加速度\",{\"1\":{\"146\":1}}],[\"车辆以p的概率随机减速一个单位\",{\"1\":{\"144\":1}}],[\"车辆有最大速度限制\",{\"1\":{\"143\":1}}],[\"考虑换道\",{\"1\":{\"150\":1}}],[\"考虑异质\",{\"1\":{\"146\":1}}],[\"考虑了变量之间的相互作用\",{\"1\":{\"115\":1}}],[\"加减速而不是以1\",{\"1\":{\"146\":1}}],[\"加速\",{\"1\":{\"144\":1}}],[\"驾驶员的这一行为会引起交通系统的一系列反应\",{\"1\":{\"144\":1}}],[\"驾驶员不可能总是按照一定速度\",{\"1\":{\"144\":1}}],[\"总会有一定的波动\",{\"1\":{\"144\":1}}],[\"前附点延长前面音一半的拍\",{\"1\":{\"169\":1}}],[\"前沿科技\",{\"0\":{\"165\":1}}],[\"前进\",{\"1\":{\"154\":1}}],[\"前方的空元胞数\",{\"1\":{\"144\":1}}],[\"前向型和反馈型\",{\"1\":{\"114\":1}}],[\"所有车辆同步更新\",{\"1\":{\"143\":1}}],[\"所以需要分别计算四个出口的元胞潜力大小\",{\"1\":{\"161\":1}}],[\"所以需要对数据先做\",{\"1\":{\"115\":1}}],[\"所以最后\",{\"1\":{\"158\":1}}],[\"所以便有了现在这个页面\",{\"1\":{\"1\":1}}],[\"所以迟迟不肯行动\",{\"1\":{\"1\":1}}],[\"堵塞密度kj\",{\"1\":{\"148\":1}}],[\"堵塞密度\",{\"1\":{\"141\":1}}],[\"临界速度vc\",{\"1\":{\"148\":1}}],[\"临界速度\",{\"1\":{\"141\":1}}],[\"临界密度kc\",{\"1\":{\"148\":1}}],[\"临界密度\",{\"1\":{\"141\":1}}],[\"畅行速度vf\",{\"1\":{\"148\":1}}],[\"畅行速度\",{\"1\":{\"141\":1}}],[\"极大流率qm\",{\"1\":{\"148\":1}}],[\"极大流率对应的速度\",{\"1\":{\"141\":1}}],[\"极大流率对应的密度\",{\"1\":{\"141\":1}}],[\"极大流率\",{\"1\":{\"141\":1}}],[\"宏观\",{\"1\":{\"140\":1,\"154\":2}}],[\"微调\",{\"1\":{\"166\":1}}],[\"微观\",{\"1\":{\"140\":1,\"154\":2}}],[\"微小的交通扰动\",{\"1\":{\"140\":1}}],[\"形成原因\",{\"1\":{\"140\":1}}],[\"形成stop\",{\"1\":{\"140\":1}}],[\"字面意是停走波\",{\"1\":{\"140\":1}}],[\"结果\",{\"0\":{\"148\":1,\"162\":1},\"1\":{\"140\":1}}],[\"内含动画演示\",{\"1\":{\"149\":1}}],[\"内生性的交通拥堵\",{\"1\":{\"140\":1}}],[\"内容方面有待优化\",{\"1\":{\"1\":1}}],[\"莫名发生的交通拥堵\",{\"1\":{\"140\":1}}],[\"幽灵拥堵\",{\"0\":{\"140\":1}}],[\"他们的状态有以下8种\",{\"1\":{\"139\":1}}],[\"补充一下基于ca交通仿真中最常见的规则\",{\"1\":{\"139\":1}}],[\"改进点\",{\"0\":{\"146\":1}}],[\"改进ns模型以及对应代码的实现\",{\"1\":{\"136\":1}}],[\"改进ns模型\",{\"0\":{\"136\":1,\"145\":1}}],[\"改变了问题的复杂度\",{\"1\":{\"115\":1}}],[\"交通规划模拟与分析\",{\"1\":{\"167\":1}}],[\"交通管理\",{\"1\":{\"154\":1}}],[\"交通流中\",{\"1\":{\"140\":1}}],[\"交通\",{\"2\":{\"135\":1,\"151\":1,\"163\":1}}],[\"交叉验证法采用的是无放回的随机采样方式\",{\"1\":{\"111\":1}}],[\"交叉验证法和自助法都是随机采样法\",{\"1\":{\"111\":1}}],[\"交叉验证法和自助法异同\",{\"1\":{\"111\":1}}],[\"交叉验证法\",{\"1\":{\"111\":1}}],[\"交叉验证\",{\"1\":{\"111\":1}}],[\"英文总结\",{\"1\":{\"134\":1}}],[\"英语学习\",{\"1\":{\"4\":1}}],[\"模拟拟不同的交通规划方案\",{\"1\":{\"167\":1}}],[\"模拟行人穿过以平台\",{\"1\":{\"155\":1}}],[\"模拟现实复杂动态系统\",{\"1\":{\"134\":1}}],[\"模型对于交通行业影响\",{\"0\":{\"167\":1}}],[\"模型设置\",{\"0\":{\"156\":1}}],[\"模型的行人流仿真\",{\"1\":{\"153\":1}}],[\"模型的基本单元\",{\"1\":{\"128\":1}}],[\"模型信息\",{\"0\":{\"147\":1},\"1\":{\"147\":1}}],[\"模型说明\",{\"0\":{\"143\":1}}],[\"模型描述\",{\"0\":{\"142\":1}}],[\"模型最为关键的部分\",{\"1\":{\"132\":1}}],[\"模型过分记住了噪声\",{\"1\":{\"111\":1}}],[\"模型复杂度过低\",{\"1\":{\"111\":1}}],[\"模型评估与选择\",{\"0\":{\"111\":1}}],[\"模型训练过程中单独留出的样本集\",{\"1\":{\"110\":1}}],[\"模型\",{\"1\":{\"110\":1,\"153\":1}}],[\"化学\",{\"1\":{\"134\":1}}],[\"化表示\",{\"1\":{\"110\":1}}],[\"作为物理\",{\"1\":{\"134\":1}}],[\"作用是在训练阶段的第三个步骤中\",{\"1\":{\"114\":1}}],[\"作用请从机器学习训练阶段的三个步骤的角度来阐述\",{\"1\":{\"114\":1}}],[\"作用\",{\"1\":{\"110\":3}}],[\"应用\",{\"0\":{\"134\":1}}],[\"应用等方面\",{\"1\":{\"126\":1}}],[\"νt\",{\"1\":{\"133\":1}}],[\"死\",{\"1\":{\"133\":1}}],[\"死亡\",{\"1\":{\"133\":2}}],[\"否则就待在原地\",{\"1\":{\"161\":1}}],[\"否则就变为\",{\"1\":{\"133\":1}}],[\"否则维持原速\",{\"1\":{\"144\":1}}],[\"否则继续保持\",{\"1\":{\"133\":1}}],[\"生物过程的基础模型\",{\"1\":{\"134\":1}}],[\"生\",{\"1\":{\"133\":2}}],[\"生命游戏是最著名的二维元胞自动机生命游戏\",{\"1\":{\"133\":1}}],[\"生命游戏\",{\"0\":{\"133\":1},\"1\":{\"133\":1}}],[\"则待在原地\",{\"1\":{\"161\":1}}],[\"则选回原位置\",{\"1\":{\"161\":1}}],[\"则汽车减速为安全距离\",{\"1\":{\"144\":1}}],[\"则该格变为\",{\"1\":{\"133\":1}}],[\"则该格继续保持\",{\"1\":{\"133\":1}}],[\"则无论该神经网络有多少层\",{\"1\":{\"114\":1}}],[\"86\",{\"1\":{\"148\":1}}],[\"8045\",{\"1\":{\"148\":1}}],[\"8262\",{\"1\":{\"148\":1}}],[\"8\",{\"1\":{\"133\":2,\"160\":1,\"161\":2,\"208\":1,\"215\":1,\"224\":2}}],[\"若它的\",{\"1\":{\"133\":2}}],[\"活着\",{\"1\":{\"133\":3}}],[\"邻居中有\",{\"1\":{\"133\":1}}],[\"邻居\",{\"1\":{\"133\":1}}],[\"邻居定义为\",{\"1\":{\"130\":1}}],[\"邻居定义为下式\",{\"1\":{\"130\":1}}],[\"年设计\",{\"1\":{\"133\":1}}],[\"年代在洛斯阿拉莫斯国家实验室同时提出\",{\"1\":{\"127\":1}}],[\"高维度\",{\"1\":{\"132\":1}}],[\"高斯核\",{\"1\":{\"115\":1}}],[\"服从相同的规律分布方式相同\",{\"1\":{\"132\":1}}],[\"同一根线\",{\"1\":{\"177\":1}}],[\"同样音高的相连\",{\"1\":{\"174\":1}}],[\"同质性\",{\"1\":{\"132\":1}}],[\"同时计算复杂度可能会较高\",{\"1\":{\"115\":1}}],[\"同时减小了下一层的输入大小\",{\"1\":{\"78\":1}}],[\"空间时间上均离散化\",{\"1\":{\"143\":1}}],[\"空间\",{\"1\":{\"132\":1}}],[\"离散型\",{\"1\":{\"132\":1}}],[\"由下图\",{\"1\":{\"172\":1}}],[\"由节奏推动\",{\"1\":{\"169\":1}}],[\"由力的大小决定\",{\"1\":{\"154\":1}}],[\"由rickert\",{\"1\":{\"150\":1}}],[\"由上图可以看到stop\",{\"1\":{\"140\":1}}],[\"由john\",{\"1\":{\"133\":1}}],[\"由局部到整体\",{\"1\":{\"132\":1}}],[\"由求特征向量转化为求比例系数\",{\"1\":{\"115\":1}}],[\"映射型\",{\"1\":{\"131\":1}}],[\"虚拟的元胞\",{\"1\":{\"131\":1}}],[\"注\",{\"1\":{\"131\":1}}],[\"绝热型\",{\"1\":{\"131\":1}}],[\"绝热型和映射型这四种\",{\"1\":{\"131\":1}}],[\"绝对的宿命论主义倾向\",{\"1\":{\"3\":1}}],[\"周围行人\",{\"1\":{\"154\":1}}],[\"周期边界\",{\"1\":{\"147\":1}}],[\"周期型\",{\"1\":{\"131\":2}}],[\"周边盆友的影响👬\",{\"1\":{\"2\":1}}],[\"固定型\",{\"1\":{\"131\":2}}],[\"边界矩阵\",{\"1\":{\"160\":1}}],[\"边界矩阵长\",{\"1\":{\"160\":1}}],[\"边界矩阵宽\",{\"1\":{\"160\":1}}],[\"边界元胞为元胞每个维度内侧邻近元胞\",{\"1\":{\"131\":1}}],[\"边界元胞与自己相同\",{\"1\":{\"131\":1}}],[\"边界条件是元胞空间外的部分\",{\"1\":{\"131\":1}}],[\"边界条件\",{\"0\":{\"131\":1},\"1\":{\"157\":1}}],[\"边界点和噪声点\",{\"1\":{\"116\":1}}],[\"图中表示为竖线\",{\"1\":{\"169\":1}}],[\"图5\",{\"1\":{\"162\":1}}],[\"图4\",{\"1\":{\"162\":1}}],[\"图\",{\"1\":{\"160\":1}}],[\"图3\",{\"1\":{\"159\":1}}],[\"图2\",{\"1\":{\"133\":1,\"158\":1}}],[\"图1\",{\"1\":{\"130\":1,\"155\":1}}],[\"图神经网络\",{\"1\":{\"86\":1}}],[\"型\",{\"1\":{\"130\":1,\"157\":1}}],[\"型和\",{\"1\":{\"130\":1}}],[\"取决于元胞状态更新时所要搜索的空间域\",{\"1\":{\"130\":1}}],[\"复杂情况下也有多维\",{\"1\":{\"128\":1}}],[\"复杂度只与样本数量有关\",{\"1\":{\"115\":1}}],[\"元胞潜力\",{\"1\":{\"160\":1}}],[\"元胞潜能可以写为\",{\"1\":{\"158\":1}}],[\"元胞动态势能反应行人在选择下一步时\",{\"1\":{\"158\":1}}],[\"元胞静态势能反应了行人在选择下一步时\",{\"1\":{\"158\":1}}],[\"元胞的状态更新规则变化是同步进行的\",{\"1\":{\"132\":1}}],[\"元胞类型\",{\"1\":{\"130\":1}}],[\"元胞呈一定形状\",{\"1\":{\"129\":1}}],[\"元胞是\",{\"1\":{\"128\":1}}],[\"元胞规则是整个\",{\"1\":{\"132\":1}}],[\"元胞规则即每次迭代\",{\"1\":{\"132\":1}}],[\"元胞规则\",{\"0\":{\"132\":1},\"1\":{\"127\":1}}],[\"元胞边界\",{\"1\":{\"127\":1}}],[\"元胞邻居是某一元胞周围的元胞\",{\"1\":{\"130\":1}}],[\"元胞邻居\",{\"0\":{\"130\":1},\"1\":{\"127\":1,\"157\":1}}],[\"元胞空间划分方式大致有\",{\"1\":{\"129\":1}}],[\"元胞空间为空间内元胞的集合\",{\"1\":{\"129\":1}}],[\"元胞空间\",{\"0\":{\"129\":1},\"1\":{\"127\":1,\"157\":1}}],[\"元胞\",{\"0\":{\"128\":1},\"1\":{\"127\":1,\"157\":1}}],[\"元胞自动机模型\",{\"1\":{\"154\":1}}],[\"元胞自动机的应用大致有以下几类\",{\"1\":{\"134\":1}}],[\"元胞自动机是一类无穷维动力系统\",{\"1\":{\"132\":1}}],[\"元胞自动机更新规则特征\",{\"1\":{\"132\":1}}],[\"元胞自动机\",{\"0\":{\"126\":1},\"1\":{\"127\":1}}],[\"世纪\",{\"1\":{\"127\":1}}],[\"世界杯\",{\"1\":{\"3\":1}}],[\"世界之本质\",{\"1\":{\"3\":1}}],[\"于\",{\"1\":{\"127\":1}}],[\"信息增益或熵\",{\"1\":{\"124\":1}}],[\"杂质\",{\"1\":{\"124\":1}}],[\"间隔\",{\"1\":{\"123\":1}}],[\"支持向量\",{\"1\":{\"123\":1}}],[\"支持向量机采用核函数的机制\",{\"1\":{\"115\":1}}],[\"支持向量机只考虑局部的边界线附近的点\",{\"1\":{\"115\":1}}],[\"支持向量机\",{\"0\":{\"115\":1},\"1\":{\"110\":1,\"123\":1}}],[\"超平面\",{\"1\":{\"123\":1}}],[\"质心\",{\"1\":{\"122\":1}}],[\"簇\",{\"1\":{\"122\":1}}],[\"簇数的预先指定\",{\"1\":{\"116\":1}}],[\"折交叉验证\",{\"1\":{\"120\":1}}],[\"惩罚项\",{\"1\":{\"119\":1}}],[\"还可以用于分类\",{\"1\":{\"117\":1}}],[\"还有很多排版\",{\"1\":{\"1\":1}}],[\"除了可以用于降维\",{\"1\":{\"117\":1}}],[\"除训练集外所有样本\",{\"1\":{\"111\":1}}],[\"没有这个限制\",{\"1\":{\"117\":1}}],[\"没有将排名分数作为直接结果\",{\"1\":{\"115\":1}}],[\"之间的区别和联系\",{\"1\":{\"117\":1}}],[\"八\",{\"0\":{\"117\":1}}],[\"特别是在大型数据集上\",{\"1\":{\"116\":1}}],[\"特征值\",{\"1\":{\"121\":1}}],[\"特征向量\",{\"1\":{\"121\":1}}],[\"特征量过少\",{\"1\":{\"111\":1}}],[\"特征降维\",{\"1\":{\"78\":1}}],[\"特征不变性\",{\"1\":{\"78\":1}}],[\"次之\",{\"1\":{\"116\":1}}],[\"计算机视觉\",{\"1\":{\"166\":1}}],[\"计算位置1到9各原胞潜力大小\",{\"1\":{\"161\":1}}],[\"计算所有邻居原胞的原胞潜力n\",{\"1\":{\"161\":1}}],[\"计算原胞潜力\",{\"1\":{\"161\":1}}],[\"计算元胞潜力\",{\"1\":{\"161\":1}}],[\"计算单元\",{\"1\":{\"134\":1}}],[\"计算复杂度\",{\"1\":{\"116\":1}}],[\"计算量相对大\",{\"1\":{\"78\":1}}],[\"计算量大\",{\"1\":{\"78\":1}}],[\"能够理解和生成高质量的文本提示\",{\"1\":{\"167\":1}}],[\"能够发现各种形状和密度的簇\",{\"1\":{\"116\":1}}],[\"能够处理多任务如自然语言处理\",{\"1\":{\"166\":1}}],[\"能够处理非线性特征之间的相互作用\",{\"1\":{\"115\":1}}],[\"能够处理非线性可分的数据集\",{\"1\":{\"115\":1}}],[\"能够处理非线性可分的数据集和噪声数据\",{\"1\":{\"115\":1}}],[\"能够处理大型特征空间\",{\"1\":{\"115\":1}}],[\"假定簇是球形且密度均匀\",{\"1\":{\"116\":1}}],[\"假设数据本身是线性可分的\",{\"1\":{\"115\":1}}],[\"时值是不同音符代表的时间值\",{\"1\":{\"181\":1}}],[\"时空图\",{\"1\":{\"140\":1}}],[\"时间及状态都是离散的\",{\"1\":{\"132\":1}}],[\"时间复杂性至少是\",{\"1\":{\"116\":1}}],[\"时是留一法\",{\"1\":{\"111\":1}}],[\"可视为无穷大\",{\"1\":{\"161\":1}}],[\"可视化结果以树状结构呈现\",{\"1\":{\"116\":1}}],[\"可能需要调整半径参数和最小邻居数\",{\"1\":{\"116\":1}}],[\"可能会导致低质量的聚类结果\",{\"1\":{\"116\":1}}],[\"可以自己手动画一个图验证一下\",{\"1\":{\"161\":1}}],[\"可以自然地引入核函数\",{\"1\":{\"115\":1}}],[\"可以发现\",{\"1\":{\"158\":1}}],[\"可以是2d平面图\",{\"1\":{\"154\":1}}],[\"可以看到随着货车比例的增加\",{\"1\":{\"148\":1}}],[\"可以将它们识别为噪声\",{\"1\":{\"116\":1}}],[\"可以根据需要切割簇\",{\"1\":{\"116\":1}}],[\"可以在聚类的同时发现异常点\",{\"1\":{\"116\":1}}],[\"可以对任意形状的稠密数据集进行聚类\",{\"1\":{\"116\":1}}],[\"可以处理非线性特征\",{\"1\":{\"115\":1}}],[\"可以控制对分类错误的容忍程度\",{\"1\":{\"115\":1}}],[\"可以分为\",{\"1\":{\"114\":1}}],[\"可以加入非线性因素\",{\"1\":{\"114\":1}}],[\"可以使得得到的模型更为稳健\",{\"1\":{\"111\":1}}],[\"可以跟人一样聪明\",{\"1\":{\"110\":1}}],[\"或节拍器响一下开始到下一次再响起为一拍\",{\"1\":{\"181\":1}}],[\"或因韵律而感动\",{\"1\":{\"169\":1}}],[\"或活着\",{\"1\":{\"133\":1}}],[\"或自顶向下的分裂聚类\",{\"1\":{\"116\":1}}],[\"或者所有样本在所有属性上取值相同\",{\"1\":{\"113\":1}}],[\"凝聚型\",{\"1\":{\"116\":1}}],[\"每个音符占1\",{\"1\":{\"169\":1}}],[\"每个行人占一格\",{\"1\":{\"155\":1}}],[\"每个个体有着独特的行为特征\",{\"1\":{\"154\":1}}],[\"每个个体没有行为特征\",{\"1\":{\"154\":1}}],[\"每个车辆前进当前速度的格数\",{\"1\":{\"144\":1}}],[\"每个车辆拥有坐标\",{\"1\":{\"143\":1}}],[\"每个元胞为空或仅被一辆车占据\",{\"1\":{\"143\":1}}],[\"每个元胞长7\",{\"1\":{\"143\":1}}],[\"每个元胞按照该规则进行状态更新\",{\"1\":{\"132\":1}}],[\"每个元胞按照当前状态及周围邻居的状态来更新下一时刻该元胞状态\",{\"1\":{\"132\":1}}],[\"每个维度的第一个元胞与最后一个元胞互为边界\",{\"1\":{\"131\":1}}],[\"每个簇包含最接近其质心的数据点\",{\"1\":{\"116\":1}}],[\"每一个元胞都有一个状态\",{\"1\":{\"128\":1}}],[\"每一维对应一个特征\",{\"1\":{\"110\":1}}],[\"个周边位置滑动\",{\"1\":{\"161\":1}}],[\"个位置没有人的话才进行选择\",{\"1\":{\"161\":1}}],[\"个位置的选择概率\",{\"1\":{\"158\":1}}],[\"个位置进行标号\",{\"1\":{\"158\":1}}],[\"个邻居中有\",{\"1\":{\"133\":1}}],[\"个为\",{\"1\":{\"133\":1}}],[\"个\",{\"1\":{\"133\":2,\"160\":1}}],[\"个簇\",{\"1\":{\"116\":1}}],[\"个人觉得这并不是一个很严重的缺点\",{\"1\":{\"115\":1}}],[\"工作原理\",{\"1\":{\"116\":3}}],[\"层次聚类和\",{\"1\":{\"116\":1}}],[\"层次聚类会生成层次结构\",{\"1\":{\"116\":1}}],[\"层次聚类将数据集逐渐分割或合并成不同的层次簇\",{\"1\":{\"116\":1}}],[\"层次聚类\",{\"1\":{\"116\":2}}],[\"聚类收敛时间较长\",{\"1\":{\"116\":1}}],[\"聚类结果没有偏倚\",{\"1\":{\"116\":1}}],[\"聚类之间不能交换对象\",{\"1\":{\"116\":1}}],[\"聚类方法原理\",{\"1\":{\"116\":1}}],[\"聚类方法分类\",{\"1\":{\"116\":1}}],[\"聚类\",{\"0\":{\"116\":1},\"1\":{\"116\":1}}],[\"七\",{\"0\":{\"116\":1}}],[\"求解的是\",{\"1\":{\"115\":1}}],[\"求解的复杂度与样本的维度有关即\",{\"1\":{\"115\":1}}],[\"求导涉及除法\",{\"1\":{\"78\":1}}],[\"效率并不是很高\",{\"1\":{\"115\":1}}],[\"随着货车比例r的增加\",{\"1\":{\"148\":1}}],[\"随机慢化概率p以及慢启动系数s\",{\"1\":{\"148\":1}}],[\"随机慢化\",{\"1\":{\"144\":1}}],[\"随机森林克服了此缺点\",{\"1\":{\"115\":1}}],[\"随笔等\",{\"1\":{\"4\":1}}],[\"随笔等其他内容\",{\"1\":{\"4\":1}}],[\"直观的决策规则\",{\"1\":{\"115\":1}}],[\"直接把该结点做为叶结点\",{\"1\":{\"113\":3}}],[\"多车道\",{\"1\":{\"150\":1}}],[\"多重共线性并不是问题\",{\"1\":{\"115\":1}}],[\"多项式核和\",{\"1\":{\"115\":1}}],[\"已有工具的高效实现\",{\"1\":{\"115\":1}}],[\"便利的观测样本概率分数\",{\"1\":{\"115\":1}}],[\"便于我们选择合适的模型\",{\"1\":{\"111\":1}}],[\"线性\",{\"1\":{\"115\":1}}],[\"线型模型优势与不足\",{\"1\":{\"112\":1}}],[\"线型模型\",{\"0\":{\"112\":1}}],[\"通常是最快的\",{\"1\":{\"116\":1}}],[\"通常不采用核函数的方法\",{\"1\":{\"115\":1}}],[\"通过模拟不同紧急情况下的交通流动\",{\"1\":{\"167\":1}}],[\"通过模拟特定时间段的交通流\",{\"1\":{\"167\":1}}],[\"通过生成不同交通规划方案\",{\"1\":{\"167\":1}}],[\"通过海量多样化数据训练\",{\"1\":{\"166\":1}}],[\"通过改变s与p的大小\",{\"1\":{\"148\":1}}],[\"通过改变货车占比r\",{\"1\":{\"148\":1}}],[\"通过密度自动确定簇的数量\",{\"1\":{\"116\":1}}],[\"通过使用核函数将输入空间映射到高维特征空间\",{\"1\":{\"115\":1}}],[\"通过调整这些变量\",{\"1\":{\"115\":1}}],[\"通过引入松弛变量来处理噪声和异常点\",{\"1\":{\"115\":1}}],[\"通过梯度下降来寻找更优的学习参数\",{\"1\":{\"114\":1}}],[\"通过计算网络输出与实际输出之间的误差\",{\"1\":{\"114\":1}}],[\"通过验证集我们可以训练几个\",{\"1\":{\"110\":1}}],[\"都会被放大形成\",{\"1\":{\"140\":1}}],[\"都会具有\",{\"1\":{\"111\":1}}],[\"都是判别模型\",{\"1\":{\"115\":1}}],[\"都是监督学习算法\",{\"1\":{\"115\":1}}],[\"都是线性分类算法\",{\"1\":{\"115\":1}}],[\"都是分类算法\",{\"1\":{\"115\":1}}],[\"回归\",{\"1\":{\"115\":1}}],[\"回归区别联系\",{\"1\":{\"115\":1}}],[\"回归器\",{\"1\":{\"110\":1}}],[\"核函数\",{\"1\":{\"123\":1}}],[\"核心点是在指定半径范围内有足够多邻居的点\",{\"1\":{\"116\":1}}],[\"核\",{\"1\":{\"115\":1}}],[\"常见基本节奏型\",{\"1\":{\"181\":2}}],[\"常见音符\",{\"1\":{\"181\":1}}],[\"常见节奏型\",{\"1\":{\"169\":1}}],[\"常见拍号\",{\"1\":{\"169\":2}}],[\"常见的数据有\",{\"1\":{\"154\":1}}],[\"常见的模型方法有\",{\"1\":{\"154\":1}}],[\"常见的非监督学习算法包括聚类\",{\"1\":{\"110\":1}}],[\"常见的监督学习算法包括线性回归\",{\"1\":{\"110\":1}}],[\"常说的四四拍\",{\"1\":{\"169\":1}}],[\"常用为固定型和周期型\",{\"1\":{\"131\":1}}],[\"常用的邻居边界条件类型有\",{\"1\":{\"131\":1}}],[\"常用的核函数有线性核\",{\"1\":{\"115\":1}}],[\"需要提前指定簇数\",{\"1\":{\"116\":1}}],[\"需要事先指定簇数\",{\"1\":{\"116\":1}}],[\"需要进行转换\",{\"1\":{\"115\":1}}],[\"需要调整松弛变量和惩罚参数\",{\"1\":{\"115\":1}}],[\"需要使偏差小\",{\"1\":{\"111\":1}}],[\"允许在某些情况下出现分类错误\",{\"1\":{\"115\":1}}],[\"放松了对数据线性可分的假设\",{\"1\":{\"115\":1}}],[\"放大图像\",{\"1\":{\"78\":1}}],[\"因为异常点可能导致无法找到一个满足所有约束条件的超平面\",{\"1\":{\"115\":1}}],[\"因此称为幽灵拥堵\",{\"1\":{\"144\":1}}],[\"因此因变量是连续的\",{\"1\":{\"112\":1}}],[\"因此因变量是离散的\",{\"1\":{\"112\":1}}],[\"因此对一些对数据分布敏感的模型选择并不适用\",{\"1\":{\"111\":1}}],[\"此外\",{\"1\":{\"115\":1,\"161\":1}}],[\"原来原胞更新为空状态\",{\"1\":{\"161\":1}}],[\"原理\",{\"1\":{\"115\":3}}],[\"原因\",{\"1\":{\"111\":2,\"140\":1}}],[\"原因大致有以下几点\",{\"1\":{\"2\":1}}],[\"硬间隔\",{\"1\":{\"115\":4}}],[\"基尼杂质\",{\"1\":{\"124\":1}}],[\"基于现实场景数据\",{\"1\":{\"154\":1}}],[\"基于物理规则模型\",{\"1\":{\"154\":1}}],[\"基于概率密度函数的估计方法和基于样本间相似性度量的间接聚类方法\",{\"1\":{\"116\":1}}],[\"基于核函数的\",{\"1\":{\"115\":3}}],[\"基本设置\",{\"0\":{\"157\":1}}],[\"基本图\",{\"0\":{\"141\":1},\"1\":{\"141\":1}}],[\"基本思想是将高维的模式样本投影到最佳鉴别矢量空间\",{\"1\":{\"112\":1}}],[\"基本概念\",{\"0\":{\"78\":1,\"109\":1,\"137\":1},\"1\":{\"126\":1}}],[\"软间隔\",{\"1\":{\"115\":4}}],[\"试述硬间隔\",{\"1\":{\"115\":1}}],[\"六线谱八分音符\",{\"1\":{\"181\":1}}],[\"六线谱全音符\",{\"1\":{\"181\":1}}],[\"六线谱\",{\"0\":{\"171\":1,\"174\":1},\"1\":{\"174\":1,\"181\":2}}],[\"六\",{\"0\":{\"115\":1}}],[\"沿着梯度下降的方向求解极小值\",{\"1\":{\"114\":1}}],[\"梯度下降方法通过求出损失函数在某点对于参数\",{\"1\":{\"114\":1}}],[\"梯度下降方法的原理\",{\"1\":{\"114\":1}}],[\"见\",{\"1\":{\"114\":1}}],[\"更新规则即行人如何选择下一步走到哪里一个方格\",{\"1\":{\"158\":1}}],[\"更新规则\",{\"0\":{\"144\":1,\"158\":1}}],[\"更新权重和偏置项\",{\"1\":{\"114\":1}}],[\"更加危险\",{\"1\":{\"140\":1}}],[\"更费油与更多排放\",{\"1\":{\"140\":1}}],[\"更方便优化\",{\"1\":{\"78\":1}}],[\"分子代表一节有几个这样的拍子\",{\"1\":{\"169\":1}}],[\"分母代表以哪个音符的时值为一拍\",{\"1\":{\"169\":1}}],[\"分类\",{\"1\":{\"154\":1}}],[\"分类器\",{\"1\":{\"110\":1}}],[\"分别计算边界内每个原胞到出口的距离\",{\"1\":{\"161\":1}}],[\"分别用计算\",{\"1\":{\"160\":1}}],[\"分别为对应系数\",{\"1\":{\"158\":1}}],[\"分别做r=0\",{\"1\":{\"148\":1}}],[\"分别做fd图以及时空位置图\",{\"1\":{\"148\":1}}],[\"分别分析其对于交通系统的影响影响\",{\"1\":{\"148\":1}}],[\"分裂型\",{\"1\":{\"116\":1}}],[\"分为前向传播和反向传播两个阶段\",{\"1\":{\"114\":1}}],[\"再励学习\",{\"1\":{\"114\":1}}],[\"按学习方式\",{\"1\":{\"114\":1}}],[\"按一定步长\",{\"1\":{\"78\":1}}],[\"联接方式\",{\"1\":{\"114\":1}}],[\"最右侧为上拨\",{\"1\":{\"177\":1}}],[\"最后把再\",{\"1\":{\"161\":1}}],[\"最后取最大\",{\"1\":{\"161\":1}}],[\"最后得到所求\",{\"1\":{\"161\":1}}],[\"最后对\",{\"1\":{\"158\":1}}],[\"最大作为原胞潜力\",{\"1\":{\"161\":1}}],[\"最大加速度\",{\"1\":{\"146\":1}}],[\"最大速度\",{\"1\":{\"146\":1}}],[\"最大池化操作在该神经元上很大概率的输出仍是\",{\"1\":{\"78\":1}}],[\"最大池化将会取\",{\"1\":{\"78\":1}}],[\"最终选择\",{\"1\":{\"160\":1}}],[\"最终形成拥堵现象\",{\"1\":{\"144\":1}}],[\"最终的输出都是输入的线性组合\",{\"1\":{\"114\":1}}],[\"最初由德国物理学家kai\",{\"1\":{\"138\":1}}],[\"最初由stanislaw\",{\"1\":{\"127\":1}}],[\"最常用的邻居类型是\",{\"1\":{\"130\":1}}],[\"非线性激活函数的主要作用是什么\",{\"1\":{\"114\":1}}],[\"非监督学习只需要输入数据\",{\"1\":{\"110\":1}}],[\"非监督学习以及强化学习的定义和区别\",{\"1\":{\"110\":1}}],[\"非监督学习\",{\"1\":{\"110\":2}}],[\"五种音阶\",{\"0\":{\"176\":1,\"178\":1}}],[\"五线谱主要是四个内容\",{\"1\":{\"172\":1}}],[\"五线谱\",{\"0\":{\"171\":1,\"172\":1},\"1\":{\"172\":1}}],[\"五大部分\",{\"1\":{\"127\":1}}],[\"五\",{\"0\":{\"114\":1}}],[\"比如在步骤2减速时刹车踩的过大\",{\"1\":{\"144\":1}}],[\"比预剪枝保留了更多分支\",{\"1\":{\"113\":1}}],[\"比赛\",{\"1\":{\"3\":2}}],[\"禁止这些分支展开\",{\"1\":{\"113\":1}}],[\"贪心\",{\"1\":{\"113\":1}}],[\"缺点\",{\"1\":{\"113\":2,\"115\":3,\"116\":3}}],[\"显著降低训练时间和测试时间的开销\",{\"1\":{\"113\":1}}],[\"降低过拟合风险\",{\"1\":{\"113\":1}}],[\"降维最多降到类别数\",{\"1\":{\"117\":1}}],[\"降维\",{\"0\":{\"117\":1},\"1\":{\"110\":1}}],[\"类别划分为父结点中出现次数最多的类别\",{\"1\":{\"113\":1}}],[\"类别划分为该结点下所有样本中出现次数最多的类别\",{\"1\":{\"113\":1}}],[\"类别划分为该结点下所有样本同属的类别\",{\"1\":{\"113\":1}}],[\"类似栅格化\",{\"1\":{\"129\":1}}],[\"类似\",{\"1\":{\"78\":1}}],[\"无需依赖整个数据\",{\"1\":{\"115\":1}}],[\"无需划分\",{\"1\":{\"113\":1}}],[\"无监督学习方法主要有两大类\",{\"1\":{\"116\":1}}],[\"无监督学习\",{\"1\":{\"114\":1}}],[\"无监督学习无反馈\",{\"1\":{\"110\":1}}],[\"无导师的学习\",{\"1\":{\"114\":1}}],[\"无法找到一个有效的超平面\",{\"1\":{\"115\":1}}],[\"无法解决线性不可分问题\",{\"1\":{\"114\":1}}],[\"无法划分\",{\"1\":{\"113\":1}}],[\"当元胞占有时\",{\"1\":{\"158\":1}}],[\"当某车辆在当前车道上无法达到驾驶员的期望速度\",{\"1\":{\"150\":1}}],[\"当车流量足够大\",{\"1\":{\"144\":1}}],[\"当汽车当前速度大于与前车的距离\",{\"1\":{\"144\":1}}],[\"当观测样本很多时\",{\"1\":{\"115\":1}}],[\"当特征空间很大时\",{\"1\":{\"115\":1}}],[\"当前结点包含的样本集合为空集\",{\"1\":{\"113\":1}}],[\"当前结点包含的样本全属于同一类别\",{\"1\":{\"113\":1}}],[\"当前属性集为空\",{\"1\":{\"113\":1}}],[\"当然通过改变参数的形状\",{\"1\":{\"78\":1}}],[\"四四拍\",{\"1\":{\"169\":1}}],[\"四\",{\"0\":{\"113\":1}}],[\"投影后保证模式样本在新的子空间有最大的类间距离和最小的类内距离\",{\"1\":{\"112\":1}}],[\"优缺点\",{\"1\":{\"115\":1,\"116\":1}}],[\"优化网络的预测能力\",{\"1\":{\"114\":1}}],[\"优化器\",{\"1\":{\"110\":1}}],[\"优点\",{\"1\":{\"113\":2,\"115\":3,\"116\":3}}],[\"优势\",{\"1\":{\"112\":1}}],[\"三个八分音符组成一拍\",{\"1\":{\"169\":1}}],[\"三连音\",{\"1\":{\"169\":1}}],[\"三角形\",{\"1\":{\"129\":1}}],[\"三者最终计算方式以及限制条件\",{\"1\":{\"115\":1}}],[\"三\",{\"0\":{\"112\":1}}],[\"度量了同样大小的训练集的变动所导致的学习性能的变化\",{\"1\":{\"111\":1}}],[\"度量了学习算法的期望预测与真实结果的偏离程度\",{\"1\":{\"111\":1}}],[\"刻画了学习算法本身的拟合能力\",{\"1\":{\"111\":1}}],[\"请先移步这篇文章\",{\"1\":{\"153\":1}}],[\"请给出常用的几种非线性激活函数及其导数\",{\"1\":{\"114\":1}}],[\"请给出你对泛化误差的理解\",{\"1\":{\"111\":1}}],[\"请简要说明主成分分析\",{\"1\":{\"117\":1}}],[\"请简要说明他们之间的关系\",{\"1\":{\"110\":1}}],[\"请简述这两种方式的优缺点\",{\"1\":{\"113\":1}}],[\"减半拍\",{\"1\":{\"181\":1}}],[\"减时符\",{\"1\":{\"181\":1}}],[\"减少\",{\"1\":{\"168\":2}}],[\"减少数据扰动产生的影响\",{\"1\":{\"111\":1}}],[\"减速\",{\"1\":{\"144\":1}}],[\"减小模型复杂度\",{\"1\":{\"111\":1}}],[\"充分拟合数据\",{\"1\":{\"111\":1}}],[\"给定学习任务为了取得好的泛化性能\",{\"1\":{\"111\":1}}],[\"给定一个数据样本集\",{\"1\":{\"110\":1}}],[\"数以亿计参数的\",{\"1\":{\"166\":1}}],[\"数据驱动模型\",{\"1\":{\"154\":2}}],[\"数据的充分性以及学习任务本身的难度所共同决定的\",{\"1\":{\"111\":1}}],[\"数量级\",{\"1\":{\"109\":1}}],[\"噪声处理\",{\"1\":{\"116\":1}}],[\"噪声\",{\"1\":{\"111\":1}}],[\"噪声表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界即刻画了学习问题本身的难度\",{\"1\":{\"111\":1}}],[\"噪声含义\",{\"1\":{\"111\":1}}],[\"方差分解角度解释泛化性能\",{\"1\":{\"111\":1}}],[\"方差度量了同样大小训练集的变动所导致的学习性能的变化\",{\"1\":{\"111\":1}}],[\"方差\",{\"1\":{\"111\":2}}],[\"偏差+方差+噪声\",{\"1\":{\"111\":1}}],[\"偏差方差冲突\",{\"1\":{\"111\":1}}],[\"偏差度量了学习算法期望预测与真实结果的偏离程度\",{\"1\":{\"111\":1}}],[\"偏差\",{\"1\":{\"111\":3}}],[\"偏离\",{\"1\":{\"109\":1}}],[\"中间设有障碍\",{\"1\":{\"155\":1}}],[\"中的一种离散计算模型\",{\"1\":{\"127\":1}}],[\"中数据的分布未必相一致\",{\"1\":{\"111\":1}}],[\"中划分训练集和测试\",{\"1\":{\"111\":1}}],[\"解决线性模型所不能解决的问题\",{\"1\":{\"114\":1}}],[\"解决了交叉验证法中模型选择阶段和最终模型训练阶段的训练集规模差异问题\",{\"1\":{\"111\":1}}],[\"解决办法\",{\"1\":{\"111\":2}}],[\"集\",{\"1\":{\"111\":1}}],[\"验证\",{\"1\":{\"111\":1}}],[\"验证集的重要性\",{\"1\":{\"110\":1}}],[\"验证集的作用\",{\"1\":{\"110\":1}}],[\"验证集\",{\"1\":{\"110\":1}}],[\"验证集区别联系\",{\"1\":{\"110\":1}}],[\"根据数据点的密度将它们分为核心点\",{\"1\":{\"116\":1}}],[\"根据一定规则从数据集\",{\"1\":{\"111\":1}}],[\"根据学习方式的划分\",{\"1\":{\"110\":1}}],[\"它由二维元胞网格组成\",{\"1\":{\"133\":1}}],[\"它们被用于扩展簇\",{\"1\":{\"116\":1}}],[\"它们作为人工智能中评估模型的方法\",{\"1\":{\"111\":1}}],[\"它的结果易于解释和可视化\",{\"1\":{\"116\":1}}],[\"它通过迭代地将数据点分配给最近的质心并更新质心来执行聚类\",{\"1\":{\"116\":1}}],[\"它可以是自底向上的聚合聚类\",{\"1\":{\"116\":1}}],[\"它可以结合\",{\"1\":{\"115\":1}}],[\"它可以用于调整模型的超参数和用于对模型的能力进行初步评估\",{\"1\":{\"110\":1}}],[\"相关信息\",{\"1\":{\"132\":1,\"155\":1,\"207\":1}}],[\"相互作用\",{\"1\":{\"132\":1}}],[\"相同之处\",{\"1\":{\"112\":1}}],[\"相同点\",{\"1\":{\"111\":1,\"115\":1,\"117\":1}}],[\"相较于全连接层直接把图像展开成一个行向量\",{\"1\":{\"78\":1}}],[\"留出法\",{\"1\":{\"111\":1}}],[\"评估方法\",{\"1\":{\"111\":1}}],[\"泛化性能更好\",{\"1\":{\"113\":1}}],[\"泛化性能是出学习算法的能力\",{\"1\":{\"111\":1}}],[\"泛化误差\",{\"1\":{\"111\":2}}],[\"泛化能力\",{\"1\":{\"109\":1}}],[\"经常的\",{\"1\":{\"168\":2}}],[\"经验\",{\"1\":{\"111\":1}}],[\"经过训练的网络也能给出合适的输出\",{\"1\":{\"110\":1}}],[\"错分样本的占比\",{\"1\":{\"111\":1}}],[\"错误率\",{\"1\":{\"111\":2}}],[\"错误率及误差概念\",{\"1\":{\"111\":1}}],[\"误差\",{\"1\":{\"111\":3}}],[\"扩大训练集\",{\"1\":{\"111\":1}}],[\"扩大感受野\",{\"1\":{\"78\":1}}],[\"神经网络计算加权输入并应用激活函数得到输出\",{\"1\":{\"114\":1}}],[\"神经网络的学习过程主要基于反向传播算法\",{\"1\":{\"114\":1}}],[\"神经网络的训练轮数等\",{\"1\":{\"111\":1}}],[\"神经网络根据是否存在网络回路\",{\"1\":{\"114\":1}}],[\"神经网络分类\",{\"1\":{\"114\":1}}],[\"神经网络\",{\"0\":{\"114\":1}}],[\"神经网络等\",{\"1\":{\"110\":1}}],[\"增时符\",{\"1\":{\"181\":1}}],[\"增长\",{\"1\":{\"168\":2}}],[\"增加了车辆换道规则\",{\"1\":{\"150\":1}}],[\"增加模型复杂度\",{\"1\":{\"111\":1}}],[\"增加新特征\",{\"1\":{\"111\":1}}],[\"增广训练集\",{\"1\":{\"111\":1}}],[\"让一些神经元以一定的概率不工作\",{\"1\":{\"111\":1}}],[\"让我顺利解决了很多问题\",{\"1\":{\"1\":1}}],[\"7444\",{\"1\":{\"148\":1}}],[\"7668\",{\"1\":{\"148\":1}}],[\"7\",{\"1\":{\"111\":1,\"112\":1,\"143\":1,\"160\":1,\"222\":2,\"224\":3}}],[\"约束模型特征\",{\"1\":{\"111\":1}}],[\"清洗数据\",{\"1\":{\"111\":1}}],[\"权值学习迭代次数过多\",{\"1\":{\"111\":1}}],[\"不要捏的太紧\",{\"1\":{\"177\":1}}],[\"不重要的\",{\"1\":{\"168\":1}}],[\"不含边界的距离矩阵\",{\"1\":{\"161\":1}}],[\"不含边界距离矩阵\",{\"1\":{\"160\":1}}],[\"不适合不规则形状和不同密度的簇\",{\"1\":{\"116\":1}}],[\"不需要\",{\"1\":{\"116\":1}}],[\"不需要预先指定簇数\",{\"1\":{\"116\":1}}],[\"不具有很好的可伸缩性\",{\"1\":{\"116\":1}}],[\"不能撤消已做的处理\",{\"1\":{\"116\":1}}],[\"不能很好地处理大量多类特征或变量\",{\"1\":{\"115\":1}}],[\"不能划分\",{\"1\":{\"113\":1}}],[\"不受其影响\",{\"1\":{\"115\":1}}],[\"不同位置音符代表不同音高\",{\"1\":{\"172\":1}}],[\"不同方法并非只适用于一个规模\",{\"1\":{\"154\":1}}],[\"不同\",{\"1\":{\"115\":1}}],[\"不同之处\",{\"1\":{\"112\":1}}],[\"不同点\",{\"1\":{\"111\":1,\"115\":1,\"117\":1}}],[\"不足\",{\"1\":{\"112\":1}}],[\"不匹配\",{\"1\":{\"111\":1}}],[\"不想走出舒适圈\",{\"1\":{\"1\":1}}],[\"与不占有\",{\"1\":{\"157\":1}}],[\"与行人图像\",{\"1\":{\"154\":1}}],[\"与加入随机慢化概率s效果类似\",{\"1\":{\"150\":1}}],[\"与\",{\"1\":{\"115\":1,\"160\":2,\"161\":1}}],[\"与没有隐藏层的效果相当\",{\"1\":{\"114\":1}}],[\"与模型复杂度\",{\"1\":{\"111\":1}}],[\"与图像对应元素进行点乘相加的操作\",{\"1\":{\"78\":1}}],[\"欠拟合风险小\",{\"1\":{\"113\":1}}],[\"欠拟合应对\",{\"1\":{\"111\":1}}],[\"欠拟合原因\",{\"1\":{\"111\":1}}],[\"欠拟合\",{\"1\":{\"111\":1}}],[\"欠拟合定义\",{\"1\":{\"111\":1}}],[\"导致泛化性能下降\",{\"1\":{\"111\":1}}],[\"导数趋于\",{\"1\":{\"78\":1}}],[\"样本集较大时\",{\"1\":{\"116\":1}}],[\"样本真实输出与预测输出之间的差异\",{\"1\":{\"111\":1}}],[\"样本噪声过多\",{\"1\":{\"111\":1}}],[\"样本\",{\"1\":{\"111\":1}}],[\"潜在\",{\"1\":{\"111\":1}}],[\"将误差反向传播到每个参数\",{\"1\":{\"114\":1}}],[\"将训练样本本身的特点当做所有\",{\"1\":{\"111\":1}}],[\"将数据对象进行特征\",{\"1\":{\"110\":1}}],[\"太好\",{\"1\":{\"111\":1}}],[\"过拟合原因\",{\"1\":{\"111\":1}}],[\"过拟合\",{\"1\":{\"111\":3}}],[\"过程中还遇到很多莫名其妙的小\",{\"1\":{\"1\":1}}],[\"二者都使用了极大似然估计对训练样本进行建模\",{\"1\":{\"112\":1}}],[\"二者在求解超参数的过程中都使用梯度下降的方法\",{\"1\":{\"112\":1}}],[\"二\",{\"0\":{\"111\":1}}],[\"为我本人购买后的学习过程的记录\",{\"1\":{\"180\":1}}],[\"为交通规划提供数据支持\",{\"1\":{\"167\":1}}],[\"为指标\",{\"1\":{\"158\":1}}],[\"为位置为\",{\"1\":{\"158\":2}}],[\"为元胞动态势能\",{\"1\":{\"158\":1}}],[\"为元胞静态势能\",{\"1\":{\"158\":1}}],[\"为什么要引入对偶问题\",{\"1\":{\"115\":1}}],[\"为什么通常要进行标准化处理\",{\"1\":{\"110\":1}}],[\"为了让我们的模型在测试集表现得更好\",{\"1\":{\"110\":1}}],[\"该模型将模拟的道路环境扩展为双车道\",{\"1\":{\"150\":1}}],[\"该游戏采用标准\",{\"1\":{\"133\":1}}],[\"该能力称为泛化能力\",{\"1\":{\"110\":1}}],[\"该规律不仅适用于训练数据\",{\"1\":{\"110\":1}}],[\"确认网络的实际预测能力\",{\"1\":{\"110\":1}}],[\"然后在\",{\"1\":{\"161\":1}}],[\"然后决定怎么调整我们的超参数\",{\"1\":{\"110\":1}}],[\"然后再来调整参数\",{\"1\":{\"110\":1}}],[\"监控模型是否正常\",{\"1\":{\"110\":1}}],[\"监督学习和非监督学习常常结合使用\",{\"1\":{\"110\":1}}],[\"监督学习和非监督学习是机器学习中两种不同的学习方式\",{\"1\":{\"110\":1}}],[\"监督学习需要已知的输入和输出数据\",{\"1\":{\"110\":1}}],[\"监督学习不具备\",{\"1\":{\"110\":1}}],[\"监督学习关注与标签或已知输出的误差\",{\"1\":{\"110\":1}}],[\"监督学习有反馈\",{\"1\":{\"110\":1}}],[\"监督学习通常用于分类\",{\"1\":{\"110\":1}}],[\"监督学习\",{\"1\":{\"110\":2,\"114\":1}}],[\"选择新位置已占\",{\"1\":{\"161\":1}}],[\"选择下一位置\",{\"1\":{\"161\":1}}],[\"选择样本点投影具有最大方差的方向\",{\"1\":{\"117\":1}}],[\"选择分类性能最好的投影方向\",{\"1\":{\"117\":1}}],[\"选择合适的核函数和参数是一个挑战\",{\"1\":{\"115\":1}}],[\"选择超参数\",{\"1\":{\"110\":1}}],[\"选择特征等算法相关的选择的依据\",{\"1\":{\"110\":1}}],[\"等阻力\",{\"1\":{\"154\":1}}],[\"等类型\",{\"1\":{\"129\":1}}],[\"等\",{\"1\":{\"110\":1,\"154\":3}}],[\"等函数\",{\"1\":{\"78\":1}}],[\"网络节点数\",{\"1\":{\"110\":1}}],[\"网络层数\",{\"1\":{\"110\":1}}],[\"快速调参\",{\"1\":{\"110\":1}}],[\"调整网络权重\",{\"1\":{\"110\":1}}],[\"调参去拟合测试集合\",{\"1\":{\"110\":1}}],[\"调参是不可避免地一部分\",{\"1\":{\"110\":1}}],[\"调参\",{\"1\":{\"109\":1}}],[\"各数据集的作用\",{\"1\":{\"110\":1}}],[\"用1代表有车占有\",{\"1\":{\"139\":1}}],[\"用0代表空\",{\"1\":{\"139\":1}}],[\"用函数表示如下\",{\"1\":{\"133\":1}}],[\"用来评估模最终模型的泛化能力\",{\"1\":{\"110\":1}}],[\"用于模型拟合的数据样本\",{\"1\":{\"110\":1}}],[\"即1s一拍\",{\"1\":{\"181\":1}}],[\"即这个音要发多长时间\",{\"1\":{\"181\":1}}],[\"即后面的不弹\",{\"1\":{\"174\":1}}],[\"即小节\",{\"1\":{\"169\":1}}],[\"即进行位置更新\",{\"1\":{\"161\":1}}],[\"即原胞\",{\"1\":{\"161\":1}}],[\"即可得到选择\",{\"1\":{\"158\":1}}],[\"即该处元胞潜能为\",{\"1\":{\"158\":1}}],[\"即wave\",{\"1\":{\"140\":1}}],[\"即对于一条道路上连续的三个cell\",{\"1\":{\"139\":1}}],[\"即\",{\"1\":{\"139\":1,\"154\":1}}],[\"即按一定方式对空间划分\",{\"1\":{\"129\":1}}],[\"即过拟合\",{\"1\":{\"115\":1}}],[\"即存在一个超平面可以将不同类别的样本完全分开\",{\"1\":{\"115\":1}}],[\"即模式在该空间中有最佳的可分离性\",{\"1\":{\"112\":1}}],[\"即刻画了学习问题本身的难度\",{\"1\":{\"111\":1}}],[\"即刻画了学习法本身的拟合能力\",{\"1\":{\"111\":1}}],[\"即刻画了数据扰动所造成的影响\",{\"1\":{\"111\":2}}],[\"即用于训练的样本集合\",{\"1\":{\"110\":1}}],[\"即将变为sjtuer\",{\"1\":{\"3\":1}}],[\"独立成分分析\",{\"1\":{\"110\":1}}],[\"朴素贝叶斯\",{\"1\":{\"110\":1}}],[\"决策树各自优缺点\",{\"1\":{\"115\":1}}],[\"决策树中剪枝方式分为哪两种\",{\"1\":{\"113\":1}}],[\"决策树三种导致递归返回的情况\",{\"1\":{\"113\":1}}],[\"决策树\",{\"0\":{\"113\":1},\"1\":{\"110\":1,\"115\":1,\"124\":1}}],[\"决定写\",{\"1\":{\"2\":1}}],[\"逻辑回归的性能不是很好\",{\"1\":{\"115\":1}}],[\"逻辑回归的缺点\",{\"1\":{\"115\":1}}],[\"逻辑回归的优点\",{\"1\":{\"115\":1}}],[\"逻辑回归广泛的应用于工业问题上\",{\"1\":{\"115\":1}}],[\"逻辑回归解决的是分类问题\",{\"1\":{\"112\":1}}],[\"逻辑回归和线性回归的异同\",{\"1\":{\"112\":1}}],[\"逻辑回归\",{\"1\":{\"110\":1,\"115\":1}}],[\"举例\",{\"1\":{\"110\":1}}],[\"变成了1\",{\"1\":{\"169\":1}}],[\"变分自编码器等\",{\"1\":{\"110\":1}}],[\"变量\",{\"1\":{\"110\":1}}],[\"变换太缓慢\",{\"1\":{\"78\":1}}],[\"并进行归一化处理\",{\"1\":{\"161\":1}}],[\"并行性\",{\"1\":{\"132\":1}}],[\"并以负梯度方向为搜索方向\",{\"1\":{\"114\":1}}],[\"并严格划分训练集与测试集的界限\",{\"1\":{\"111\":1}}],[\"并分别给出监督和非监督学习的两种算法\",{\"1\":{\"110\":1}}],[\"并且减少了参数的相互依存关系\",{\"1\":{\"78\":1}}],[\"区别\",{\"1\":{\"110\":2,\"116\":2}}],[\"使之能够更真实准确地模拟出道路上交通流的运行状况\",{\"1\":{\"150\":1}}],[\"使得在高维特征空间中数据变得线性可分\",{\"1\":{\"115\":1}}],[\"使得所有的特征具有同样的尺度\",{\"1\":{\"110\":1}}],[\"使用最小二乘法求解线性回归时我们认为因变量服从正态分布\",{\"1\":{\"112\":1}}],[\"使系统行为从环境中获得的累积奖励值最大的一种机器学习方法\",{\"1\":{\"110\":1}}],[\"使不断改善自身的性能\",{\"1\":{\"110\":1}}],[\"是由曲风所决定的\",{\"1\":{\"181\":1}}],[\"是五线谱\",{\"1\":{\"181\":1}}],[\"是为了让最外围元胞能够有像内部元胞一样的邻域条件所创建的虚拟元胞\",{\"1\":{\"131\":1}}],[\"是否为邻居\",{\"1\":{\"130\":1}}],[\"是模型迭代的直接参与者\",{\"1\":{\"128\":1}}],[\"是自动机理论\",{\"1\":{\"127\":1}}],[\"是无监督的降维方法\",{\"1\":{\"117\":1}}],[\"是有监督的降维方法\",{\"1\":{\"117\":1}}],[\"是指机器学习算法对新鲜样本的适应能力\",{\"1\":{\"110\":1}}],[\"是指从环境状态到行为映射的学习\",{\"1\":{\"110\":1}}],[\"是不可行地\",{\"1\":{\"110\":1}}],[\"是一种基于ca模型的用于交通仿真的理论模型\",{\"1\":{\"138\":1}}],[\"是一种在没有标签或目标的情况下\",{\"1\":{\"110\":1}}],[\"是一种通过使用已知输出来训练模型的学习方式\",{\"1\":{\"110\":1}}],[\"异常检测等操作\",{\"1\":{\"110\":1}}],[\"问题描述\",{\"0\":{\"155\":1}}],[\"问题\",{\"1\":{\"110\":1}}],[\"和信号灯状态\",{\"1\":{\"154\":1}}],[\"和线性判别分析\",{\"1\":{\"117\":1}}],[\"和层次聚类通常需要额外的后处理步骤来处理噪声点\",{\"1\":{\"116\":1}}],[\"和\",{\"1\":{\"114\":1,\"115\":3,\"158\":1}}],[\"和后剪枝\",{\"1\":{\"113\":1}}],[\"和原始数据集\",{\"1\":{\"111\":1}}],[\"和输出之间的映射关系\",{\"1\":{\"110\":1}}],[\"和回归\",{\"1\":{\"110\":1}}],[\"和抽象\",{\"1\":{\"78\":1}}],[\"以右手肘与琴箱交点为支点\",{\"1\":{\"177\":1}}],[\"以帮助开发更为智能的决策制定算法\",{\"1\":{\"167\":1}}],[\"以多大的速度\",{\"1\":{\"154\":1}}],[\"以及音符不同时值带来的节奏\",{\"1\":{\"172\":1}}],[\"以及信号状态\",{\"1\":{\"154\":1}}],[\"以及一些环境阻碍的力repulsive\",{\"1\":{\"154\":1}}],[\"以及基本图\",{\"1\":{\"148\":1}}],[\"以个体为研究对象\",{\"1\":{\"154\":1}}],[\"以整个人群为研究对象\",{\"1\":{\"154\":1}}],[\"以下内容源于\",{\"1\":{\"150\":1}}],[\"以一个单位加速\",{\"1\":{\"144\":1}}],[\"以找到最佳的分类效果\",{\"1\":{\"115\":1}}],[\"以达到抽取分类信息和压缩特征空间维数的效果\",{\"1\":{\"112\":1}}],[\"以提高机器学习的效果和性能\",{\"1\":{\"110\":1}}],[\"以便对数据进行聚类\",{\"1\":{\"110\":1}}],[\"以预测新的输入数据的输出\",{\"1\":{\"110\":1}}],[\"以获取新的知识或技能\",{\"1\":{\"110\":1}}],[\"算法将数据划分为\",{\"1\":{\"116\":1}}],[\"算法的基本思想及算法流程\",{\"1\":{\"112\":1}}],[\"算法只能使用输入数据进行学习\",{\"1\":{\"110\":1}}],[\"算法通过学习这些数据\",{\"1\":{\"110\":1}}],[\"算激活函数时\",{\"1\":{\"78\":1}}],[\"定义为\",{\"1\":{\"158\":1}}],[\"定义\",{\"0\":{\"127\":1},\"1\":{\"110\":1,\"111\":1,\"140\":1}}],[\"简谱音符表示\",{\"1\":{\"181\":1}}],[\"简谱等记谱法的最重要元素\",{\"1\":{\"181\":1}}],[\"简谱在音符那课已经讲了很多\",{\"1\":{\"173\":1}}],[\"简谱\",{\"0\":{\"171\":1,\"173\":1},\"1\":{\"173\":1,\"181\":1}}],[\"简称ns\",{\"1\":{\"138\":1}}],[\"简单且高效\",{\"1\":{\"116\":1}}],[\"简单明了\",{\"1\":{\"115\":1}}],[\"简要介绍卷积概念及其作用\",{\"1\":{\"114\":1}}],[\"简要说明监督学习和非监督学习之间的区别\",{\"1\":{\"110\":1}}],[\"简述神经网络中梯度下降方法的原理和作用\",{\"1\":{\"114\":1}}],[\"简述神经网络的学习过程\",{\"1\":{\"114\":1}}],[\"简述\",{\"1\":{\"112\":1}}],[\"简述监督学习\",{\"1\":{\"110\":1}}],[\"简介\",{\"0\":{\"0\":1}}],[\"关系见下\",{\"1\":{\"110\":1}}],[\"表皮的\",{\"1\":{\"168\":1}}],[\"表面的\",{\"1\":{\"168\":1}}],[\"表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界\",{\"1\":{\"111\":1}}],[\"表征了参数每次更新的幅度\",{\"1\":{\"110\":1}}],[\"表示后面音符时值加给前面的音符\",{\"1\":{\"174\":1}}],[\"表示边界条件\",{\"1\":{\"160\":1}}],[\"表示\",{\"1\":{\"110\":1}}],[\"也是增时符与减时符的应用\",{\"1\":{\"181\":1}}],[\"也就是第一个\",{\"1\":{\"169\":1}}],[\"也就是通过验证集我们可以选择超参数\",{\"1\":{\"110\":1}}],[\"也可以是3d坐标\",{\"1\":{\"154\":1}}],[\"也可以同理做出上述图表\",{\"1\":{\"148\":1}}],[\"也称为标签或目标\",{\"1\":{\"110\":1}}],[\"也叫步长\",{\"1\":{\"110\":1}}],[\"也适用于未知数据\",{\"1\":{\"110\":1}}],[\"学习器把训练样本学习的\",{\"1\":{\"111\":1}}],[\"学习的目的是学到隐含在数据背后的规律\",{\"1\":{\"110\":1}}],[\"学习率\",{\"1\":{\"110\":2}}],[\"学习率太大和太小的可能影响\",{\"1\":{\"110\":1}}],[\"学习率这种参数叫什么\",{\"1\":{\"110\":1}}],[\"学过的知识都记不清了\",{\"1\":{\"2\":1}}],[\"利用学到的模型进行预测\",{\"1\":{\"110\":1}}],[\"测试误差\",{\"1\":{\"111\":1}}],[\"测试集上\",{\"1\":{\"111\":1}}],[\"测试集的作用\",{\"1\":{\"110\":1}}],[\"测试集\",{\"1\":{\"110\":2}}],[\"测试\",{\"1\":{\"110\":1}}],[\"称为泛化能力\",{\"1\":{\"110\":1}}],[\"目标是从数据中发现模式和结构\",{\"1\":{\"110\":1}}],[\"目标是学习输入\",{\"1\":{\"110\":1}}],[\"目标是找到输入数据之间的相似性和区别\",{\"1\":{\"110\":1}}],[\"目标\",{\"1\":{\"110\":1}}],[\"目前的系统往往在特定场景下表现良好\",{\"1\":{\"167\":1}}],[\"目前博客内容主要为过去做过的一些项目\",{\"1\":{\"4\":1}}],[\"目前小小的愿望清单\",{\"1\":{\"3\":1}}],[\"目前先尝试一个\",{\"1\":{\"2\":1}}],[\"目前还是处于入门小白阶段\",{\"1\":{\"1\":1}}],[\"训练时间开销大\",{\"1\":{\"113\":1}}],[\"训练样本过少\",{\"1\":{\"111\":1}}],[\"训练样本的一般性质尚未学好\",{\"1\":{\"111\":1}}],[\"训练集上的效果高度优于测试集\",{\"1\":{\"115\":1}}],[\"训练集上\",{\"1\":{\"111\":1}}],[\"训练集与测试集的特征分布不一致\",{\"1\":{\"111\":1}}],[\"训练集的数量级\",{\"1\":{\"111\":1}}],[\"训练集的作用\",{\"1\":{\"110\":1}}],[\"训练集\",{\"1\":{\"110\":2}}],[\"训练数据包括输入数据和对应的输出数据\",{\"1\":{\"110\":1}}],[\"训练\",{\"1\":{\"110\":1,\"111\":1}}],[\"重复出现\",{\"1\":{\"168\":1}}],[\"重复因而不必要的\",{\"1\":{\"168\":1}}],[\"重新组织已有的知识结构\",{\"1\":{\"110\":1}}],[\"重要\",{\"1\":{\"32\":1}}],[\"希望机器通过学习的手段\",{\"1\":{\"110\":1}}],[\"人\",{\"1\":{\"168\":1}}],[\"人群中每个个体有着相同的行为特征\",{\"1\":{\"154\":1}}],[\"人工智能是想要达成的目标\",{\"1\":{\"110\":1}}],[\"人格\",{\"1\":{\"3\":1}}],[\"绪论\",{\"0\":{\"110\":1}}],[\"量纲\",{\"1\":{\"109\":1}}],[\"拟合了不具代表性的特征\",{\"1\":{\"111\":1}}],[\"拟合模型\",{\"1\":{\"110\":1}}],[\"拟合\",{\"1\":{\"109\":1}}],[\"振荡\",{\"1\":{\"109\":1}}],[\"收敛\",{\"1\":{\"109\":1}}],[\"幅度\",{\"1\":{\"109\":1}}],[\"∀u∈n\",{\"1\":{\"101\":1}}],[\"∀s∈sunder∀π\",{\"1\":{\"28\":1}}],[\"∀s∈s\",{\"1\":{\"23\":4,\"28\":1,\"29\":1,\"30\":1}}],[\"α\",{\"1\":{\"133\":2}}],[\"α12​wh2​+α13​wh3​\",{\"1\":{\"98\":1}}],[\"α12​=exp\",{\"1\":{\"98\":1}}],[\"α13​=exp\",{\"1\":{\"98\":1}}],[\"αij​=softmaxj​\",{\"1\":{\"97\":1}}],[\"αi\",{\"1\":{\"53\":1}}],[\"∥\",{\"1\":{\"96\":1,\"101\":1}}],[\"×f\",{\"1\":{\"96\":1}}],[\"σ\",{\"1\":{\"93\":1,\"98\":1,\"101\":1,\"229\":1}}],[\"ρ\",{\"1\":{\"89\":1}}],[\"z\",{\"1\":{\"213\":1}}],[\"zeros\",{\"1\":{\"160\":2,\"161\":1}}],[\"zero\",{\"1\":{\"104\":1}}],[\"zv​=hv\",{\"1\":{\"102\":1}}],[\"zip\",{\"1\":{\"84\":1}}],[\"zhao\",{\"1\":{\"5\":1}}],[\"缓解了过拟合问题的发生\",{\"1\":{\"78\":1}}],[\"这段代码思想为用一个\",{\"1\":{\"161\":1}}],[\"这里用\",{\"1\":{\"161\":1}}],[\"这里因为有四个入口\",{\"1\":{\"161\":1}}],[\"这里设置了\",{\"1\":{\"160\":1}}],[\"这里以位置为\",{\"1\":{\"158\":1}}],[\"这里主要考虑为出口与障碍物\",{\"1\":{\"158\":1}}],[\"这里引入元胞潜能\",{\"1\":{\"158\":1}}],[\"这里不一一展示\",{\"1\":{\"148\":1}}],[\"这些提示用于指导视频内容的生成\",{\"1\":{\"167\":1}}],[\"这些力的合力即为行人朝着什么方向\",{\"1\":{\"154\":1}}],[\"这些特征的量纲和数量级都是不一样的\",{\"1\":{\"110\":1}}],[\"这类模型主要是基于物理规则\",{\"1\":{\"154\":1}}],[\"这三种\",{\"1\":{\"154\":1}}],[\"这也是反映了拥堵总是在没有任何外部原因\",{\"1\":{\"144\":1}}],[\"这反应了司机希望开得越快越好\",{\"1\":{\"144\":1}}],[\"这一点很重要\",{\"1\":{\"115\":1}}],[\"这个超平面需要满足离其最近的点到其的距离最大化\",{\"1\":{\"115\":1}}],[\"这是两者最本质的区别\",{\"1\":{\"112\":1}}],[\"这种情况就是最原始的感知机\",{\"1\":{\"114\":1}}],[\"这种情况会造成信息丢失\",{\"1\":{\"78\":1}}],[\"这种方式可以保持数据分布的一致性条件\",{\"1\":{\"111\":1}}],[\"这两种方法最大的不同点在于每次划分过程中每个样本点是否只有一次被划入训练集或测试集的机会\",{\"1\":{\"111\":1}}],[\"这时需要对数据进行标准化处理\",{\"1\":{\"110\":1}}],[\"这样的stop\",{\"1\":{\"140\":1}}],[\"这样子时间代价较高\",{\"1\":{\"110\":1}}],[\"这样就造成了网络的稀疏性\",{\"1\":{\"78\":1}}],[\"这相当于作弊\",{\"1\":{\"110\":1}}],[\"会使一部分神经元的输出为\",{\"1\":{\"78\":1}}],[\"接近饱和区时\",{\"1\":{\"78\":1}}],[\"函数反向传播时\",{\"1\":{\"78\":1}}],[\"第三维为速度\",{\"1\":{\"154\":1}}],[\"第三章\",{\"1\":{\"112\":2}}],[\"第三\",{\"1\":{\"78\":1}}],[\"第二个时步为演化更新时步\",{\"1\":{\"150\":1}}],[\"第二\",{\"1\":{\"78\":1}}],[\"第一个时步为车辆换道时步\",{\"1\":{\"150\":1}}],[\"第一\",{\"1\":{\"78\":1}}],[\"整个过程的计算量节省很多\",{\"1\":{\"78\":1}}],[\"而另一条车道上的驾驶条件可以满足驾驶员对速度的要求时\",{\"1\":{\"150\":1}}],[\"而层次聚类较慢\",{\"1\":{\"116\":1}}],[\"而层次聚类和\",{\"1\":{\"116\":1}}],[\"而\",{\"1\":{\"115\":1,\"117\":3}}],[\"而逻辑回归考虑全局\",{\"1\":{\"115\":1}}],[\"而线性回归解决的是回归问题\",{\"1\":{\"112\":1}}],[\"而且方差较小\",{\"1\":{\"111\":1}}],[\"而且复习过程中也苦于没有地方整理\",{\"1\":{\"2\":1}}],[\"而不需要预先定义的目标\",{\"1\":{\"110\":1}}],[\"而监督学习的标签是正确的\",{\"1\":{\"110\":1}}],[\"而深度学习\",{\"1\":{\"110\":1}}],[\"而机器学习是想要达成目标的手段\",{\"1\":{\"110\":1}}],[\"而采用\",{\"1\":{\"78\":1}}],[\"反复标记\",{\"1\":{\"174\":1}}],[\"反复出现的节奏片段\",{\"1\":{\"169\":1}}],[\"反复出现\",{\"1\":{\"168\":2}}],[\"反应人流变化的方向与源头\",{\"1\":{\"161\":1}}],[\"反应平台实时状态\",{\"1\":{\"160\":1}}],[\"反应了元胞有人占据\",{\"1\":{\"158\":1}}],[\"反应交通系统越来越拥堵\",{\"1\":{\"148\":1}}],[\"反应静止车辆启动较慢\",{\"1\":{\"146\":1}}],[\"反映在车长\",{\"1\":{\"146\":1}}],[\"反映驾驶员的不完美驾驶行为\",{\"1\":{\"144\":1}}],[\"反向传播求误差梯度时\",{\"1\":{\"78\":1}}],[\"反卷积\",{\"1\":{\"78\":1}}],[\"指更新参数步幅\",{\"1\":{\"110\":1}}],[\"指数运算\",{\"1\":{\"78\":1}}],[\"指南\",{\"0\":{\"4\":1}}],[\"采用固定型\",{\"1\":{\"157\":1}}],[\"采用基本\",{\"1\":{\"157\":1}}],[\"采用深度学习等方法预测行人的轨迹\",{\"1\":{\"154\":1}}],[\"采用\",{\"1\":{\"78\":1}}],[\"采用原因\",{\"1\":{\"78\":1}}],[\"正六边形\",{\"1\":{\"129\":1}}],[\"正方形\",{\"1\":{\"129\":1}}],[\"正则化来解决\",{\"1\":{\"115\":1}}],[\"正则化\",{\"1\":{\"111\":1,\"119\":2}}],[\"正\",{\"1\":{\"78\":1}}],[\"负为\",{\"1\":{\"78\":1}}],[\"激活函数是神经网络的一个重要组成部分\",{\"1\":{\"114\":1}}],[\"激活函数\",{\"1\":{\"78\":2,\"114\":1}}],[\"的平台上\",{\"1\":{\"161\":1}}],[\"的计算\",{\"1\":{\"161\":2}}],[\"的元胞周围空元胞数目\",{\"1\":{\"158\":1}}],[\"的元胞周边的非障碍数目\",{\"1\":{\"158\":1}}],[\"的元胞距出口的距离\",{\"1\":{\"158\":1}}],[\"的形式\",{\"1\":{\"154\":1}}],[\"的格子\",{\"1\":{\"133\":2}}],[\"的维数\",{\"1\":{\"117\":1}}],[\"的维度\",{\"1\":{\"115\":1}}],[\"的缺点\",{\"1\":{\"115\":1}}],[\"的优点\",{\"1\":{\"115\":1}}],[\"的损失函数就自带正则\",{\"1\":{\"115\":1}}],[\"的原理\",{\"1\":{\"115\":1}}],[\"的微分值\",{\"1\":{\"114\":1}}],[\"的一般性质\",{\"1\":{\"111\":1}}],[\"的卷积\",{\"1\":{\"78\":1}}],[\"的想法\",{\"1\":{\"1\":1}}],[\"缩小图像\",{\"1\":{\"78\":1}}],[\"转置卷积\",{\"1\":{\"78\":1}}],[\"实现非线性\",{\"1\":{\"78\":1}}],[\"在应急响应规划中\",{\"1\":{\"167\":1}}],[\"在复杂环境中做出快速决策的\",{\"1\":{\"167\":1}}],[\"在现实场景\",{\"1\":{\"144\":1}}],[\"在拥堵\",{\"1\":{\"140\":1}}],[\"在外围补上固定不变的\",{\"1\":{\"131\":1}}],[\"在二维空间下\",{\"1\":{\"130\":1}}],[\"在处理噪声点时比较鲁棒\",{\"1\":{\"116\":1}}],[\"在对偶问题下\",{\"1\":{\"115\":1}}],[\"在原始问题下\",{\"1\":{\"115\":1}}],[\"在解决非线性问题时\",{\"1\":{\"115\":1}}],[\"在反向传播中\",{\"1\":{\"114\":1}}],[\"在前向传播中\",{\"1\":{\"114\":1}}],[\"在神经网络中\",{\"1\":{\"114\":1}}],[\"在自变量和超参数确定的情况下逻辑回归可看作广义的线性模型在因变量下服从二元分布的一个特殊情况\",{\"1\":{\"112\":1}}],[\"在训练样本上都存在较大的经验误差\",{\"1\":{\"111\":1}}],[\"在实际问题中\",{\"1\":{\"110\":1}}],[\"在实际应用中\",{\"1\":{\"110\":1}}],[\"在数据处理时\",{\"1\":{\"110\":1}}],[\"在非监督学习中\",{\"1\":{\"110\":1}}],[\"在监督学习中\",{\"1\":{\"110\":1}}],[\"在机器学习中\",{\"1\":{\"110\":1}}],[\"在\",{\"1\":{\"78\":1,\"160\":2}}],[\"在一定程度上防止过拟合\",{\"1\":{\"78\":1}}],[\"在此表示十分感谢\",{\"1\":{\"1\":1}}],[\"进一步的改进点\",{\"0\":{\"150\":1}}],[\"进而减少计算量和参数个数\",{\"1\":{\"78\":1}}],[\"进入到梦中情组\",{\"1\":{\"3\":1}}],[\"从下往上依次是mi\",{\"1\":{\"172\":1}}],[\"从driver角度\",{\"1\":{\"140\":1}}],[\"从概念上就可以理解元胞就好似生物体的细胞\",{\"1\":{\"128\":1}}],[\"从数据中发现模式或结构的学习方式\",{\"1\":{\"110\":1}}],[\"从中学习出规律\",{\"1\":{\"110\":1}}],[\"从而锻炼手指展开能力\",{\"1\":{\"178\":1}}],[\"从而实现每一次迭代的整体更新\",{\"1\":{\"161\":1}}],[\"从而引起全局的变化\",{\"1\":{\"132\":1}}],[\"从而推广到非线性分类问题\",{\"1\":{\"115\":1}}],[\"从而增强测试评估的稳定性和可靠性\",{\"1\":{\"111\":1}}],[\"从而评价模型在数据集上的表现\",{\"1\":{\"111\":1}}],[\"从而无法完成深层网络的训练\",{\"1\":{\"78\":1}}],[\"从而使模型可以抽取更加广范围的特征\",{\"1\":{\"78\":1}}],[\"从encoder\",{\"1\":{\"49\":1}}],[\"下面将分别做进一步阐述\",{\"1\":{\"127\":1}}],[\"下面将针对这方面详细展开论述\",{\"1\":{\"111\":1}}],[\"下面来\",{\"1\":{\"1\":1}}],[\"下采样\",{\"1\":{\"78\":2}}],[\"后附点同理\",{\"1\":{\"169\":1}}],[\"后面跟一个强拍\",{\"1\":{\"169\":1}}],[\"后剪枝过程是在生成完全的决策树之后\",{\"1\":{\"113\":1}}],[\"后剪枝\",{\"1\":{\"113\":1}}],[\"后查看模型的训练效果及我们的网络是否出现异常\",{\"1\":{\"110\":1}}],[\"后\",{\"1\":{\"78\":1}}],[\"尺度变换\",{\"1\":{\"78\":1}}],[\"那么经过伸缩\",{\"1\":{\"78\":1}}],[\"输出的结果仍将为\",{\"1\":{\"78\":1}}],[\"输入为\",{\"1\":{\"78\":1}}],[\"旋转不变性和尺度不变性\",{\"1\":{\"78\":1}}],[\"池化的作用是什么\",{\"1\":{\"114\":1}}],[\"池化相当于在空间范围内做了维度约减\",{\"1\":{\"78\":1}}],[\"池化操作是模型更加关注是否存在某些特征而不是特征具体的位置\",{\"1\":{\"78\":1}}],[\"池化层的引入是仿照人的视觉系统对视觉输入对象进行降维\",{\"1\":{\"78\":1}}],[\"池化\",{\"1\":{\"78\":1}}],[\"任何全连接层都能被转换为一个等价卷积层\",{\"1\":{\"78\":1}}],[\"其规律同音符\",{\"1\":{\"182\":1}}],[\"其他谱的音符表示\",{\"1\":{\"181\":1}}],[\"其他四种音阶\",{\"1\":{\"178\":1}}],[\"其他道路使用者状态\",{\"1\":{\"154\":1}}],[\"其定义如下\",{\"1\":{\"158\":1}}],[\"其在城市规划\",{\"1\":{\"154\":1}}],[\"其元胞规则为\",{\"1\":{\"133\":1}}],[\"其状态可能是死亡\",{\"1\":{\"133\":1}}],[\"其中\",{\"1\":{\"116\":1,\"158\":2,\"181\":1}}],[\"其中不变形性包括\",{\"1\":{\"78\":1}}],[\"其采用的是有放回的随机抽样方法\",{\"1\":{\"111\":1}}],[\"其能更好地捕获图像的空间特征\",{\"1\":{\"78\":1}}],[\"其实很早就萌生过写\",{\"1\":{\"1\":1}}],[\"提供应急响应与安全管理案例\",{\"1\":{\"167\":1}}],[\"提供虚拟测试的平台\",{\"1\":{\"167\":1}}],[\"提供新的开发算法思路\",{\"1\":{\"167\":1}}],[\"提供高质量的真实数据用于训练与模拟\",{\"1\":{\"167\":1}}],[\"提取图像的特征\",{\"1\":{\"78\":1}}],[\"提示\",{\"1\":{\"30\":2,\"32\":1,\"34\":1,\"61\":1}}],[\"本系列课程源于高峰老师课程\",{\"1\":{\"180\":1}}],[\"本文不定期更新前沿科技简介\",{\"1\":{\"165\":1}}],[\"本文探讨基于\",{\"1\":{\"153\":1}}],[\"本节介绍ns模型基本内容\",{\"1\":{\"136\":1}}],[\"本节简单介绍一下元胞自动机模型\",{\"1\":{\"126\":1}}],[\"本节整理机器学习的基本问题\",{\"1\":{\"108\":1}}],[\"本节整理卷积方面基本概念\",{\"1\":{\"77\":1}}],[\"本质上是其\",{\"1\":{\"115\":1}}],[\"本科\",{\"1\":{\"4\":1}}],[\"卷积前后图像尺寸之间的关系是什么\",{\"1\":{\"114\":1}}],[\"卷积本质上也是一种对数据维度的变换\",{\"1\":{\"78\":1}}],[\"卷积核\",{\"1\":{\"78\":1}}],[\"卷积就是用一个可移动的窗口\",{\"1\":{\"78\":1}}],[\"卷积\",{\"0\":{\"77\":1},\"1\":{\"78\":1}}],[\"∇θ​logp\",{\"1\":{\"75\":1}}],[\"∇θ​l=e\",{\"1\":{\"72\":1}}],[\"∇θ​l=∇θ​e\",{\"1\":{\"72\":1}}],[\"∇θ​d\",{\"1\":{\"72\":1}}],[\"⋅\",{\"1\":{\"69\":3,\"101\":1}}],[\"⋅valuei​\",{\"1\":{\"60\":1}}],[\"μ\",{\"1\":{\"69\":2,\"71\":2,\"72\":2}}],[\"μ∗\",{\"1\":{\"69\":1}}],[\"θ\",{\"1\":{\"69\":4,\"71\":1,\"72\":2,\"75\":2,\"114\":1,\"203\":1}}],[\"6线的固弦锥上\",{\"1\":{\"177\":1}}],[\"6758\",{\"1\":{\"148\":1}}],[\"60\",{\"1\":{\"61\":1}}],[\"6\",{\"0\":{\"124\":1},\"1\":{\"61\":1,\"104\":1,\"111\":1,\"160\":2,\"161\":1,\"218\":1,\"221\":3,\"222\":5,\"223\":5,\"224\":1}}],[\"j∈z\",{\"1\":{\"213\":1}}],[\"joint\",{\"1\":{\"198\":2}}],[\"job\",{\"1\":{\"49\":2}}],[\"j+neigh\",{\"1\":{\"161\":1}}],[\"j=h+1\",{\"1\":{\"161\":1}}],[\"j=1m​\",{\"1\":{\"190\":1}}],[\"j=1\",{\"1\":{\"161\":2,\"195\":1}}],[\"j​+k3​di\",{\"1\":{\"158\":1}}],[\"j​+k2​oi\",{\"1\":{\"158\":1}}],[\"j​+ko​oi\",{\"1\":{\"158\":1}}],[\"j​+kd​di\",{\"1\":{\"158\":1}}],[\"j​=kl​li\",{\"1\":{\"158\":1}}],[\"j​=0\",{\"1\":{\"158\":1}}],[\"j​=ei\",{\"1\":{\"158\":2}}],[\"j​\",{\"1\":{\"158\":11,\"161\":3}}],[\"j​exp\",{\"1\":{\"158\":2}}],[\"judgment\",{\"1\":{\"144\":1}}],[\"just\",{\"1\":{\"8\":1,\"23\":1,\"38\":1,\"39\":2,\"41\":2,\"43\":1,\"44\":1,\"46\":1,\"49\":1,\"196\":1,\"214\":1,\"216\":1,\"226\":1}}],[\"jam\",{\"0\":{\"140\":1},\"1\":{\"140\":2,\"141\":1,\"144\":1}}],[\"jacobian\",{\"1\":{\"82\":1}}],[\"j\",{\"1\":{\"53\":1,\"96\":2,\"104\":3,\"158\":5,\"160\":2,\"161\":11,\"192\":1,\"195\":1,\"198\":1,\"213\":2}}],[\"4下面一个横线代表八分音符\",{\"1\":{\"181\":1}}],[\"4弦位置\",{\"1\":{\"177\":1}}],[\"4拍\",{\"1\":{\"169\":1}}],[\"4拍=3\",{\"1\":{\"169\":1}}],[\"4356\",{\"1\":{\"148\":1}}],[\"43\",{\"1\":{\"148\":1}}],[\"46\",{\"1\":{\"148\":3}}],[\"4693\",{\"1\":{\"148\":1}}],[\"4292\",{\"1\":{\"148\":1}}],[\"4248\",{\"1\":{\"148\":1}}],[\"42\",{\"1\":{\"148\":3}}],[\"44\",{\"1\":{\"148\":1}}],[\"41​0\",{\"1\":{\"93\":2}}],[\"4100\",{\"1\":{\"93\":2}}],[\"410\",{\"1\":{\"93\":4}}],[\"404\",{\"1\":{\"231\":1}}],[\"40\",{\"1\":{\"61\":1,\"127\":1,\"148\":3,\"185\":1,\"197\":2}}],[\"4\",{\"0\":{\"122\":1,\"141\":1,\"149\":1,\"191\":1,\"197\":1,\"218\":1,\"225\":1,\"226\":1,\"227\":1},\"1\":{\"49\":1,\"61\":1,\"104\":1,\"111\":3,\"115\":3,\"116\":1,\"117\":1,\"144\":1,\"160\":3,\"161\":1,\"168\":1,\"198\":2,\"208\":1,\"213\":1,\"215\":1,\"221\":4,\"222\":5,\"223\":7,\"224\":1,\"229\":1}}],[\"yes\",{\"1\":{\"224\":1}}],[\"yet\",{\"1\":{\"215\":1,\"218\":1,\"224\":1}}],[\"y+2\",{\"1\":{\"161\":6}}],[\"y+1\",{\"1\":{\"161\":4}}],[\"y=h+2\",{\"1\":{\"160\":1}}],[\"y=x\",{\"1\":{\"78\":1}}],[\"y=x+b\",{\"1\":{\"67\":1}}],[\"y0​\",{\"1\":{\"130\":2}}],[\"y\",{\"1\":{\"66\":3,\"67\":1,\"130\":2,\"160\":11,\"161\":15}}],[\"yt−1​\",{\"1\":{\"59\":5}}],[\"yt​∣y<t​\",{\"1\":{\"59\":1}}],[\"yt​\",{\"1\":{\"49\":1,\"59\":1}}],[\"yielding\",{\"1\":{\"228\":1}}],[\"yield\",{\"1\":{\"222\":1}}],[\"yields\",{\"1\":{\"207\":1,\"219\":1,\"222\":1,\"223\":2,\"226\":1}}],[\"yi−1​\",{\"1\":{\"49\":1}}],[\"yi​=g\",{\"1\":{\"49\":1}}],[\"yn​\",{\"1\":{\"49\":1}}],[\"y3​\",{\"1\":{\"49\":2}}],[\"y2​\",{\"1\":{\"49\":2}}],[\"y1​\",{\"1\":{\"49\":2}}],[\"your\",{\"1\":{\"61\":1,\"104\":2,\"196\":2}}],[\"yourself\",{\"1\":{\"37\":1}}],[\"you\",{\"1\":{\"5\":1,\"21\":1,\"23\":1,\"28\":1,\"61\":2,\"82\":10,\"196\":1,\"221\":3,\"223\":1}}],[\"如有侵权请联系我\",{\"1\":{\"180\":1}}],[\"如预训练\",{\"1\":{\"166\":1}}],[\"如车辆坐标\",{\"1\":{\"154\":1}}],[\"如动作\",{\"1\":{\"154\":1}}],[\"如道路结构\",{\"1\":{\"154\":1}}],[\"如来自人行道边界\",{\"1\":{\"154\":1}}],[\"如使人达到向着目的地前进\",{\"1\":{\"154\":1}}],[\"如流体动力学模型\",{\"1\":{\"154\":1}}],[\"如研究拥堵\",{\"1\":{\"154\":1}}],[\"如stca模型\",{\"1\":{\"150\":1}}],[\"如当s\",{\"1\":{\"148\":1}}],[\"如事故\",{\"1\":{\"144\":1}}],[\"如下图\",{\"1\":{\"177\":3,\"182\":1}}],[\"如下图time\",{\"1\":{\"140\":1}}],[\"如下图所示\",{\"1\":{\"139\":1}}],[\"如司机过度刹车或者与其他车里的太近\",{\"1\":{\"140\":1}}],[\"如图一所示\",{\"1\":{\"130\":1}}],[\"如\",{\"1\":{\"128\":1}}],[\"如决策树的扩展分支\",{\"1\":{\"111\":1}}],[\"如池化与步长为\",{\"1\":{\"78\":1}}],[\"如果选择的位置被占\",{\"1\":{\"161\":1}}],[\"如果上下和前面三个位置共\",{\"1\":{\"161\":1}}],[\"如果位置\",{\"1\":{\"161\":1}}],[\"如果你还不了解\",{\"1\":{\"153\":1}}],[\"如果某一步没有很好地选择合并或分裂的决定\",{\"1\":{\"116\":1}}],[\"如果不考虑核函数\",{\"1\":{\"115\":1}}],[\"如果不使用激活函数\",{\"1\":{\"114\":1}}],[\"如果没有设置验证集\",{\"1\":{\"110\":1}}],[\"如果把测试集当验证集\",{\"1\":{\"110\":1}}],[\"如果原先的神经元在最大池化操作后输出\",{\"1\":{\"78\":1}}],[\"如果将输入右移一位得到\",{\"1\":{\"78\":1}}],[\"如果之前有所记录就很便于回忆\",{\"1\":{\"2\":1}}],[\"如何从rnn起步\",{\"1\":{\"49\":1}}],[\"ε\",{\"1\":{\"46\":11}}],[\"≃xˉ=j=1∑n​nxj​​\",{\"1\":{\"43\":1}}],[\"→g1∗​\",{\"1\":{\"221\":1}}],[\"→gk−1∗​\",{\"1\":{\"221\":1}}],[\"→gk∗​\",{\"1\":{\"221\":2}}],[\"→maxv\",{\"1\":{\"144\":1}}],[\"→minv\",{\"1\":{\"144\":2}}],[\"→\",{\"1\":{\"140\":4,\"197\":3,\"221\":2,\"227\":1}}],[\"→vk+1​\",{\"1\":{\"38\":1}}],[\"→π\",{\"1\":{\"38\":1}}],[\"→q\",{\"1\":{\"38\":1}}],[\"5669\",{\"1\":{\"148\":1}}],[\"5524\",{\"1\":{\"148\":1}}],[\"5m\",{\"1\":{\"143\":1}}],[\"5​​​101​011​​\",{\"1\":{\"93\":1}}],[\"5​​\",{\"1\":{\"93\":1}}],[\"5​123​​\",{\"1\":{\"61\":1}}],[\"50​0\",{\"1\":{\"93\":2}}],[\"50\",{\"1\":{\"61\":1}}],[\"52\",{\"1\":{\"61\":1}}],[\"5128\",{\"1\":{\"148\":1}}],[\"51\",{\"1\":{\"61\":1}}],[\"5×\",{\"1\":{\"43\":1}}],[\"5×1+0\",{\"1\":{\"43\":1}}],[\"5\",{\"0\":{\"43\":1,\"123\":1,\"150\":1,\"192\":1,\"228\":1,\"229\":1},\"1\":{\"32\":1,\"43\":1,\"49\":1,\"61\":5,\"78\":6,\"104\":1,\"111\":2,\"115\":1,\"143\":1,\"160\":1,\"161\":4,\"199\":2,\"213\":1,\"221\":1,\"222\":5,\"223\":4,\"224\":4}}],[\"5xk​\",{\"1\":{\"32\":1}}],[\"5x\",{\"1\":{\"32\":1}}],[\"−1\",{\"1\":{\"43\":1,\"98\":3,\"144\":1,\"213\":2}}],[\"−1rπ​\",{\"1\":{\"24\":2}}],[\"−f\",{\"1\":{\"32\":1}}],[\"∣p∣∣i∣k\",{\"1\":{\"227\":2}}],[\"∣v∣+∣e∣\",{\"1\":{\"216\":2}}],[\"∣x−x0​∣+∣y−y0​∣<=r\",{\"1\":{\"130\":1}}],[\"∣x−x0​∣<=r\",{\"1\":{\"130\":1}}],[\"∣y−y0​∣<=r\",{\"1\":{\"130\":1}}],[\"∣∣≤γ∣∣x1​−x2​∣∣\",{\"1\":{\"32\":1}}],[\"∣∣f\",{\"1\":{\"32\":1}}],[\"∣s\",{\"1\":{\"13\":1,\"23\":3,\"25\":2,\"29\":1,\"30\":1,\"34\":1,\"39\":1}}],[\"xd​\",{\"1\":{\"222\":1}}],[\"xb​\",{\"1\":{\"222\":1}}],[\"xe​\",{\"1\":{\"222\":1}}],[\"xc​\",{\"1\":{\"222\":1}}],[\"x=xi​\",{\"1\":{\"222\":1}}],[\"x5​\",{\"1\":{\"221\":7}}],[\"x4​=b\",{\"1\":{\"221\":1}}],[\"x4​\",{\"1\":{\"221\":5}}],[\"xf​∈xg​\",{\"1\":{\"219\":1}}],[\"xf​\",{\"1\":{\"219\":4,\"221\":2}}],[\"xk\",{\"1\":{\"221\":2}}],[\"xk−1​\",{\"1\":{\"221\":1,\"222\":2}}],[\"xk​∈xanduk​∈u\",{\"1\":{\"219\":1}}],[\"xk​\",{\"1\":{\"219\":6,\"221\":7,\"222\":4}}],[\"xk+1​\",{\"1\":{\"219\":3,\"221\":2}}],[\"xk+1​=0\",{\"1\":{\"32\":1}}],[\"xnew\",{\"1\":{\"218\":1}}],[\"xn​\",{\"1\":{\"43\":1,\"49\":2}}],[\"xg\",{\"1\":{\"224\":2,\"227\":1}}],[\"xg​=d\",{\"1\":{\"221\":1}}],[\"xg​\",{\"1\":{\"216\":2,\"218\":2,\"219\":1,\"221\":1,\"222\":1,\"224\":4,\"227\":1}}],[\"xg​∈x\",{\"1\":{\"212\":1}}],[\"xg=\",{\"1\":{\"213\":1}}],[\"xi​=a\",{\"1\":{\"221\":1}}],[\"xi​\",{\"1\":{\"215\":1,\"218\":2,\"219\":1,\"221\":1,\"222\":4,\"227\":1}}],[\"xi​∈x\",{\"1\":{\"212\":1}}],[\"xi\",{\"1\":{\"215\":1,\"217\":2,\"224\":3}}],[\"xi=\",{\"1\":{\"213\":1}}],[\"xy\",{\"1\":{\"161\":4}}],[\"x+1\",{\"1\":{\"161\":4}}],[\"x+2\",{\"1\":{\"161\":6}}],[\"x0​\",{\"1\":{\"130\":2}}],[\"x0​=10\",{\"1\":{\"32\":1}}],[\"x∈r3×2\",{\"1\":{\"93\":1,\"98\":1}}],[\"x∈rn×f\",{\"1\":{\"92\":1,\"95\":1,\"100\":1}}],[\"x∈xg​\",{\"1\":{\"215\":1,\"223\":1}}],[\"x∈x\",{\"1\":{\"32\":1,\"212\":1,\"213\":1,\"227\":2}}],[\"x3​\",{\"1\":{\"49\":2,\"221\":4}}],[\"xt​\",{\"1\":{\"49\":1}}],[\"x→0\",{\"1\":{\"32\":1}}],[\"x→x\",{\"1\":{\"32\":1}}],[\"x=n+2\",{\"1\":{\"160\":1}}],[\"x=​101​011​​\",{\"1\":{\"93\":1,\"98\":1}}],[\"x=−1\",{\"1\":{\"43\":1}}],[\"x=+1\",{\"1\":{\"43\":1}}],[\"x=0\",{\"1\":{\"32\":1,\"43\":1}}],[\"x=f\",{\"1\":{\"32\":1}}],[\"x2​=2\",{\"1\":{\"32\":1}}],[\"x2​\",{\"1\":{\"32\":1,\"43\":1,\"49\":2,\"221\":2}}],[\"x1​=5\",{\"1\":{\"32\":1}}],[\"x1​\",{\"1\":{\"32\":1,\"43\":1,\"49\":2,\"219\":1,\"221\":4,\"222\":2}}],[\"x\",{\"1\":{\"32\":1,\"43\":5,\"59\":1,\"65\":1,\"66\":2,\"67\":3,\"130\":2,\"160\":8,\"161\":10,\"212\":7,\"213\":5,\"215\":18,\"216\":4,\"217\":36,\"218\":6,\"219\":3,\"222\":1,\"223\":18,\"224\":15,\"227\":5,\"229\":1}}],[\"​>dsafe\",{\"1\":{\"150\":1}}],[\"​>dn​⋯​\",{\"1\":{\"150\":1}}],[\"​s<2∨s>3s=2s=3​\",{\"1\":{\"133\":1}}],[\"​∥hn\",{\"1\":{\"101\":1}}],[\"​=σ\",{\"1\":{\"101\":1}}],[\"​=σ​j∈ni​∑​αij​whj​​\",{\"1\":{\"98\":1}}],[\"​=aggregate\",{\"1\":{\"101\":1}}],[\"​=relu\",{\"1\":{\"98\":1}}],[\"​=πmax​a∑​π\",{\"1\":{\"29\":1,\"30\":1,\"34\":1}}],[\"​\",{\"1\":{\"34\":1,\"93\":1,\"97\":1,\"98\":3,\"101\":3,\"102\":1,\"150\":1}}],[\"​​\",{\"1\":{\"23\":1,\"25\":1,\"29\":1,\"30\":1}}],[\"≥vπ​\",{\"1\":{\"28\":1}}],[\"≥vπ2​​\",{\"1\":{\"28\":1}}],[\"35\",{\"1\":{\"197\":2}}],[\"3处代表四分音符\",{\"1\":{\"181\":1}}],[\"3处\",{\"1\":{\"177\":1}}],[\"3个音符\",{\"1\":{\"169\":1}}],[\"3d\",{\"1\":{\"154\":1}}],[\"38\",{\"1\":{\"148\":1}}],[\"32\",{\"1\":{\"148\":1}}],[\"34\",{\"1\":{\"148\":1}}],[\"37\",{\"1\":{\"148\":1}}],[\"3903\",{\"1\":{\"148\":1}}],[\"39\",{\"1\":{\"148\":1}}],[\"3的时空位置图\",{\"1\":{\"148\":1}}],[\"330\",{\"1\":{\"93\":2}}],[\"3035\",{\"1\":{\"148\":1}}],[\"30\",{\"1\":{\"61\":1,\"197\":4}}],[\"3\",{\"0\":{\"27\":1,\"121\":1,\"140\":1,\"145\":1,\"146\":1,\"147\":1,\"148\":2,\"149\":1,\"150\":1,\"190\":1,\"196\":1,\"208\":1,\"217\":1,\"219\":1,\"220\":1,\"221\":1,\"222\":1,\"223\":1,\"224\":2},\"1\":{\"38\":1,\"49\":1,\"55\":1,\"58\":1,\"61\":5,\"78\":1,\"86\":1,\"93\":3,\"98\":3,\"104\":1,\"110\":3,\"111\":6,\"112\":3,\"113\":1,\"115\":5,\"116\":1,\"117\":2,\"133\":2,\"144\":1,\"148\":5,\"160\":2,\"161\":14,\"177\":1,\"198\":1,\"199\":1,\"208\":2,\"213\":2,\"216\":2,\"221\":2,\"222\":3,\"223\":1,\"224\":1,\"226\":2,\"227\":1,\"229\":1}}],[\"=0\",{\"1\":{\"25\":1}}],[\"∑​vπ​\",{\"1\":{\"25\":1,\"39\":1}}],[\"∑​pπ​\",{\"1\":{\"23\":1}}],[\"∑​p\",{\"1\":{\"23\":2,\"25\":1,\"29\":1,\"30\":1,\"34\":1}}],[\"⊤∈rn\",{\"1\":{\"23\":2}}],[\"+ck−1∗​\",{\"1\":{\"222\":1}}],[\"+l\",{\"1\":{\"223\":1}}],[\"+li​\",{\"1\":{\"222\":1}}],[\"+lf​\",{\"1\":{\"219\":1,\"221\":1}}],[\"+g∗\",{\"1\":{\"223\":1}}],[\"+g3∗​\",{\"1\":{\"221\":2}}],[\"+g5∗​\",{\"1\":{\"221\":4}}],[\"+gk+1∗​\",{\"1\":{\"221\":1}}],[\"+ϵσi∈\",{\"1\":{\"200\":1}}],[\"+o\",{\"1\":{\"161\":1}}],[\"+obstacle\",{\"1\":{\"161\":7}}],[\"+d\",{\"1\":{\"161\":1}}],[\"+sm\",{\"1\":{\"161\":7}}],[\"+2\",{\"1\":{\"160\":4}}],[\"+exp\",{\"1\":{\"98\":2}}],[\"+\",{\"1\":{\"82\":1,\"104\":3,\"154\":1,\"161\":5,\"216\":1,\"224\":2}}],[\"+bo​\",{\"1\":{\"59\":1}}],[\"+γs\",{\"1\":{\"23\":1}}],[\"+γe\",{\"1\":{\"23\":1}}],[\"+1\",{\"1\":{\"18\":1,\"34\":1,\"144\":1,\"161\":4}}],[\"k1​li\",{\"1\":{\"158\":1}}],[\"kd​\",{\"1\":{\"158\":1}}],[\"ks​\",{\"1\":{\"158\":1}}],[\"ks​si\",{\"1\":{\"158\":1}}],[\"kj逐渐减小\",{\"1\":{\"148\":1}}],[\"kc先增加后减小\",{\"1\":{\"148\":1}}],[\"kai\",{\"1\":{\"138\":1}}],[\"k−1\",{\"1\":{\"101\":2}}],[\"kept\",{\"1\":{\"196\":1}}],[\"keeps\",{\"1\":{\"144\":1}}],[\"keep\",{\"1\":{\"81\":1,\"214\":1}}],[\"kernel\",{\"1\":{\"78\":1,\"123\":1}}],[\"keyi​\",{\"1\":{\"60\":1}}],[\"keys\",{\"1\":{\"53\":1}}],[\"key\",{\"0\":{\"51\":1,\"52\":1,\"90\":1,\"92\":1,\"95\":1,\"100\":1,\"186\":1},\"1\":{\"8\":1,\"20\":1,\"21\":2,\"40\":1,\"49\":1,\"51\":1,\"52\":1,\"53\":2,\"57\":1,\"60\":1,\"61\":11,\"67\":1,\"87\":1,\"199\":1,\"223\":1,\"224\":1}}],[\"k=3\",{\"1\":{\"221\":1}}],[\"k=4\",{\"1\":{\"221\":2,\"222\":1}}],[\"k=0对应的速度\",{\"1\":{\"141\":1}}],[\"k=m\",{\"1\":{\"111\":1}}],[\"k=​0\",{\"1\":{\"61\":1}}],[\"k=1\",{\"1\":{\"38\":1}}],[\"kinds\",{\"1\":{\"215\":1}}],[\"kind\",{\"1\":{\"49\":1,\"226\":1}}],[\"k\",{\"0\":{\"122\":1},\"1\":{\"44\":1,\"52\":1,\"53\":1,\"54\":1,\"61\":6,\"101\":7,\"102\":2,\"116\":10,\"117\":1,\"120\":4,\"121\":1,\"122\":4,\"161\":4,\"199\":1,\"216\":2,\"219\":2,\"221\":1,\"222\":1,\"223\":2,\"227\":2}}],[\"kth\",{\"1\":{\"44\":1}}],[\"knowledge\",{\"1\":{\"48\":1}}],[\"know\",{\"1\":{\"25\":1,\"27\":1,\"30\":3,\"38\":2,\"43\":1,\"48\":1,\"223\":1}}],[\"known\",{\"1\":{\"23\":1,\"29\":1,\"67\":2,\"223\":1}}],[\"k→∞\",{\"1\":{\"24\":1}}],[\"k+1\",{\"1\":{\"24\":1,\"38\":1,\"39\":1,\"219\":1}}],[\"qm逐渐减小\",{\"1\":{\"148\":1}}],[\"qk曲线极值\",{\"1\":{\"141\":1}}],[\"qk​\",{\"1\":{\"34\":1}}],[\"quite\",{\"1\":{\"226\":1}}],[\"quickly\",{\"1\":{\"83\":1}}],[\"quality\",{\"1\":{\"167\":2,\"203\":1}}],[\"queue\",{\"1\":{\"216\":1,\"217\":1,\"218\":1,\"224\":2}}],[\"queryt​\",{\"1\":{\"60\":2}}],[\"query\",{\"0\":{\"52\":1},\"1\":{\"52\":2,\"53\":2,\"58\":1,\"60\":2,\"61\":10}}],[\"questions\",{\"0\":{\"118\":1},\"1\":{\"28\":1}}],[\"q=​135​246​​\",{\"1\":{\"61\":1}}],[\"qπk​​\",{\"1\":{\"44\":2}}],[\"qπ​\",{\"1\":{\"25\":3,\"39\":1}}],[\"q3\",{\"1\":{\"39\":1}}],[\"q2\",{\"1\":{\"39\":1}}],[\"q1\",{\"1\":{\"39\":1}}],[\"q\",{\"1\":{\"22\":1,\"25\":1,\"29\":1,\"30\":1,\"38\":1,\"44\":1,\"45\":1,\"52\":1,\"53\":1,\"54\":1,\"61\":4,\"215\":4,\"216\":5,\"217\":11,\"218\":1,\"224\":4}}],[\"h+1\",{\"1\":{\"161\":1}}],[\"h+2\",{\"1\":{\"160\":3,\"161\":1}}],[\"h+4\",{\"1\":{\"160\":2}}],[\"h=30\",{\"1\":{\"160\":1}}],[\"hyperplane\",{\"1\":{\"123\":4}}],[\"hurdle\",{\"1\":{\"160\":11,\"161\":3}}],[\"hu\",{\"1\":{\"101\":1}}],[\"huge\",{\"1\":{\"87\":1}}],[\"hn\",{\"1\":{\"101\":1}}],[\"hv\",{\"1\":{\"101\":3}}],[\"h1\",{\"1\":{\"98\":1}}],[\"hj​\",{\"1\":{\"96\":1}}],[\"hold\",{\"1\":{\"226\":3}}],[\"hopes\",{\"1\":{\"225\":1}}],[\"horizontal\",{\"1\":{\"49\":1}}],[\"however\",{\"1\":{\"49\":1,\"53\":1,\"87\":1,\"94\":1,\"168\":1,\"200\":1,\"223\":1,\"226\":1,\"229\":1}}],[\"how\",{\"0\":{\"197\":1},\"1\":{\"8\":1,\"21\":1,\"24\":2,\"27\":1,\"28\":1,\"30\":4,\"44\":1,\"45\":2,\"82\":1,\"199\":1,\"207\":1,\"215\":2,\"223\":4,\"228\":1}}],[\"h\",{\"1\":{\"49\":1,\"53\":2,\"69\":2,\"71\":1,\"72\":2,\"93\":5,\"104\":8,\"160\":4,\"161\":1}}],[\"ht​=rnn\",{\"1\":{\"59\":1}}],[\"ht​=f\",{\"1\":{\"49\":1}}],[\"ht​\",{\"1\":{\"58\":1,\"59\":5}}],[\"ht−1​\",{\"1\":{\"49\":1,\"59\":3}}],[\"hierarchical\",{\"1\":{\"207\":4}}],[\"hit\",{\"1\":{\"195\":1}}],[\"hi\",{\"1\":{\"98\":2}}],[\"hi​\",{\"1\":{\"53\":1,\"96\":1}}],[\"historical\",{\"1\":{\"49\":1}}],[\"highest\",{\"1\":{\"59\":1}}],[\"higher\",{\"1\":{\"48\":1,\"82\":2,\"123\":1,\"197\":1,\"224\":1}}],[\"high\",{\"0\":{\"74\":1},\"1\":{\"49\":1,\"83\":1,\"167\":2,\"198\":1,\"199\":1,\"203\":1}}],[\"hidden\",{\"1\":{\"48\":1,\"49\":3,\"57\":1,\"58\":3,\"59\":9}}],[\"heuristics\",{\"1\":{\"229\":1}}],[\"heuristic\",{\"1\":{\"216\":1}}],[\"helicopter\",{\"1\":{\"207\":1}}],[\"helps\",{\"1\":{\"81\":1,\"119\":1,\"120\":1,\"194\":1,\"196\":2,\"199\":1,\"203\":1}}],[\"heavy\",{\"1\":{\"140\":2}}],[\"head\",{\"1\":{\"43\":2,\"154\":1}}],[\"hence\",{\"1\":{\"46\":1,\"185\":1,\"221\":1}}],[\"here\",{\"1\":{\"21\":1,\"23\":1,\"24\":1,\"28\":1,\"32\":1,\"39\":1,\"43\":1,\"44\":2,\"49\":3,\"82\":1,\"104\":1,\"199\":1,\"205\":1,\"215\":1,\"219\":1,\"221\":2,\"222\":1,\"223\":1,\"226\":4}}],[\"handling\",{\"1\":{\"196\":1}}],[\"handled\",{\"1\":{\"223\":1}}],[\"handle\",{\"1\":{\"166\":1,\"167\":1}}],[\"having\",{\"1\":{\"185\":1}}],[\"have\",{\"1\":{\"23\":1,\"45\":2,\"46\":1,\"83\":1,\"86\":1,\"88\":2,\"89\":4,\"199\":2,\"215\":4,\"216\":1,\"224\":1,\"225\":1,\"226\":2}}],[\"hard\",{\"1\":{\"83\":1}}],[\"has\",{\"1\":{\"30\":1,\"40\":2,\"45\":1,\"46\":1,\"83\":1,\"87\":1,\"134\":2,\"143\":1,\"144\":1,\"185\":1,\"189\":1,\"190\":1,\"191\":1,\"192\":1,\"193\":2,\"196\":1,\"197\":8,\"198\":1,\"199\":1,\"207\":1,\"213\":1,\"215\":2,\"217\":1,\"218\":1,\"222\":1,\"223\":1,\"225\":1,\"226\":1,\"227\":7}}],[\"vs\",{\"1\":{\"223\":1}}],[\"vocabulary\",{\"0\":{\"168\":1}}],[\"von\",{\"1\":{\"127\":1,\"130\":2}}],[\"vn​+1\",{\"1\":{\"150\":1}}],[\"vdr中p是v的函数\",{\"1\":{\"150\":1}}],[\"vdr\",{\"1\":{\"150\":1}}],[\"vf先不变后急剧减小\",{\"1\":{\"148\":1}}],[\"vc先急剧减小后基本不变\",{\"1\":{\"148\":1}}],[\"vmax​\",{\"1\":{\"144\":1,\"150\":1}}],[\"vt+1\",{\"1\":{\"133\":1}}],[\"vh​+ws​\",{\"1\":{\"53\":1}}],[\"violated\",{\"1\":{\"229\":1}}],[\"vij+1​\",{\"1\":{\"189\":1}}],[\"vij​\",{\"1\":{\"189\":1}}],[\"vik​\",{\"1\":{\"189\":1}}],[\"vi2​\",{\"1\":{\"189\":1}}],[\"vi1​\",{\"1\":{\"189\":1}}],[\"virtual\",{\"1\":{\"167\":1}}],[\"virtually\",{\"1\":{\"89\":1}}],[\"vision\",{\"1\":{\"166\":1}}],[\"visited\",{\"1\":{\"214\":1,\"215\":7,\"217\":11}}],[\"visiting\",{\"1\":{\"203\":1}}],[\"visit\",{\"1\":{\"45\":2,\"46\":2,\"203\":1}}],[\"visits\",{\"1\":{\"45\":2}}],[\"visual\",{\"1\":{\"154\":1}}],[\"visualize\",{\"1\":{\"104\":2}}],[\"vice\",{\"1\":{\"89\":1}}],[\"view\",{\"0\":{\"218\":1},\"1\":{\"69\":1}}],[\"via\",{\"1\":{\"59\":1,\"89\":1}}],[\"vi\",{\"1\":{\"40\":3}}],[\"videos\",{\"1\":{\"5\":1,\"167\":2}}],[\"vu\",{\"1\":{\"38\":1}}],[\"vk+1​\",{\"1\":{\"34\":1}}],[\"vk+1​=f\",{\"1\":{\"34\":1,\"38\":1}}],[\"vk​\",{\"1\":{\"34\":2,\"38\":5,\"39\":1}}],[\"vk​→vπ​=\",{\"1\":{\"24\":1}}],[\"v=0时的密度\",{\"1\":{\"141\":1}}],[\"v=​103050​204060​​\",{\"1\":{\"61\":1}}],[\"v=f\",{\"1\":{\"33\":1,\"38\":1}}],[\"v=πmax​\",{\"1\":{\"29\":1,\"33\":1}}],[\"velocity\",{\"1\":{\"143\":2,\"144\":2,\"146\":1,\"150\":1,\"154\":2,\"207\":1}}],[\"vehicles\",{\"1\":{\"146\":1,\"154\":1,\"185\":2}}],[\"vehicle\",{\"1\":{\"140\":2,\"143\":5,\"144\":5,\"146\":2,\"154\":2}}],[\"vertex\",{\"1\":{\"189\":2,\"207\":5,\"218\":1,\"221\":2,\"229\":1}}],[\"vertices\",{\"1\":{\"87\":1,\"188\":2,\"189\":1,\"229\":1}}],[\"verify\",{\"1\":{\"185\":1}}],[\"versa\",{\"1\":{\"89\":1}}],[\"versions\",{\"1\":{\"220\":1}}],[\"version\",{\"1\":{\"23\":2,\"200\":1}}],[\"verb\",{\"1\":{\"88\":1}}],[\"very\",{\"1\":{\"49\":1,\"83\":1,\"110\":1}}],[\"vectors\",{\"1\":{\"96\":1,\"123\":2,\"199\":6,\"213\":1}}],[\"vector\",{\"0\":{\"123\":1},\"1\":{\"29\":1,\"34\":1,\"53\":1,\"58\":2,\"59\":6,\"78\":1,\"82\":2,\"87\":1,\"88\":1,\"89\":2,\"96\":1,\"104\":2,\"110\":1,\"123\":1,\"195\":1,\"199\":1,\"200\":1}}],[\"v\",{\"1\":{\"29\":5,\"30\":3,\"33\":3,\"35\":1,\"38\":1,\"52\":1,\"53\":1,\"55\":1,\"61\":4,\"87\":2,\"101\":6,\"102\":1,\"144\":4,\"188\":2,\"198\":4,\"218\":4}}],[\"vπk​​\",{\"1\":{\"44\":1}}],[\"vπk​​=rπk​​+γpπk​​vπk​​\",{\"1\":{\"39\":1}}],[\"vπ∗​\",{\"1\":{\"28\":1}}],[\"vπ1​​\",{\"1\":{\"28\":1}}],[\"vπ​=\",{\"1\":{\"23\":1}}],[\"vπ​=rπ​+γpπ​vpi​∀s∈s\",{\"1\":{\"23\":1}}],[\"vπ​\",{\"1\":{\"23\":9,\"24\":3,\"25\":2}}],[\"v2​\",{\"1\":{\"24\":1}}],[\"v1​\",{\"1\":{\"24\":1,\"38\":1}}],[\"v0​\",{\"1\":{\"24\":1,\"38\":2}}],[\"vaguely\",{\"1\":{\"227\":1}}],[\"valid\",{\"1\":{\"196\":1}}],[\"validated\",{\"1\":{\"120\":1}}],[\"validation\",{\"0\":{\"120\":1},\"1\":{\"81\":1,\"120\":4}}],[\"valued\",{\"1\":{\"207\":1,\"226\":1}}],[\"values\",{\"1\":{\"23\":1,\"24\":1,\"25\":1,\"44\":4,\"45\":1,\"55\":1,\"93\":1,\"124\":2,\"195\":2,\"197\":1,\"226\":1,\"229\":2}}],[\"value\",{\"0\":{\"22\":1,\"24\":1,\"25\":1,\"37\":1,\"38\":1,\"40\":1,\"52\":1,\"221\":1,\"222\":1},\"1\":{\"17\":2,\"22\":7,\"23\":4,\"24\":1,\"25\":18,\"28\":1,\"29\":1,\"30\":2,\"37\":1,\"38\":14,\"39\":7,\"40\":7,\"41\":2,\"43\":3,\"44\":4,\"45\":4,\"52\":1,\"60\":1,\"61\":10,\"81\":2,\"88\":1,\"124\":1,\"195\":1,\"198\":2,\"199\":1,\"219\":1,\"220\":3,\"221\":3,\"222\":1,\"223\":3,\"224\":5,\"226\":1,\"228\":1,\"229\":1}}],[\"various\",{\"1\":{\"154\":1,\"166\":1,\"185\":1,\"207\":1}}],[\"variety\",{\"1\":{\"89\":1}}],[\"variations\",{\"1\":{\"226\":1}}],[\"variate\",{\"0\":{\"63\":1,\"68\":1},\"1\":{\"67\":1,\"74\":2,\"203\":1}}],[\"variances\",{\"1\":{\"121\":1}}],[\"variance\",{\"0\":{\"73\":1,\"74\":1},\"1\":{\"65\":1,\"74\":1,\"82\":1,\"84\":1,\"111\":1,\"119\":1,\"121\":1,\"122\":1,\"203\":2}}],[\"variant\",{\"1\":{\"44\":1,\"45\":1}}],[\"variable\",{\"1\":{\"43\":1,\"65\":1,\"66\":1,\"67\":1,\"124\":1,\"223\":4}}],[\"variables\",{\"1\":{\"21\":1,\"229\":4}}],[\"var\",{\"1\":{\"66\":1}}],[\"vanishing\",{\"1\":{\"49\":1}}],[\"γ<1\",{\"1\":{\"32\":1}}],[\"γ\",{\"1\":{\"17\":2,\"35\":1,\"69\":1}}],[\"2∣p∣∣i∣k\",{\"1\":{\"227\":1}}],[\"25\",{\"1\":{\"197\":2}}],[\"2mm与琴弦接触\",{\"1\":{\"177\":1}}],[\"2或2\",{\"1\":{\"177\":1}}],[\"2拍+1\",{\"1\":{\"169\":1}}],[\"2×n\",{\"1\":{\"161\":1}}],[\"2d\",{\"1\":{\"154\":1,\"213\":1}}],[\"2417\",{\"1\":{\"148\":1}}],[\"2326\",{\"1\":{\"148\":1}}],[\"2006\",{\"1\":{\"205\":1}}],[\"2024\",{\"1\":{\"184\":2}}],[\"20\",{\"1\":{\"61\":1,\"127\":1,\"160\":2,\"197\":2}}],[\"2\",{\"0\":{\"20\":1,\"74\":1,\"82\":1,\"120\":1,\"139\":1,\"142\":1,\"143\":1,\"144\":2,\"147\":1,\"189\":1,\"195\":1,\"207\":1,\"210\":1,\"211\":1,\"212\":1,\"213\":2,\"214\":2,\"215\":2,\"216\":3,\"217\":2,\"218\":2,\"219\":1,\"220\":1,\"221\":1,\"222\":2,\"223\":2,\"224\":1,\"225\":1,\"226\":1,\"227\":2,\"228\":1,\"229\":1},\"1\":{\"17\":1,\"18\":2,\"23\":1,\"38\":2,\"44\":2,\"49\":8,\"54\":1,\"58\":2,\"59\":1,\"61\":14,\"78\":1,\"86\":1,\"89\":1,\"93\":3,\"98\":2,\"104\":7,\"110\":3,\"111\":6,\"112\":4,\"113\":1,\"115\":6,\"116\":1,\"117\":2,\"132\":1,\"133\":1,\"144\":2,\"148\":5,\"160\":10,\"161\":24,\"167\":1,\"168\":1,\"177\":1,\"188\":1,\"193\":1,\"195\":1,\"197\":4,\"198\":2,\"208\":2,\"212\":1,\"213\":2,\"216\":2,\"218\":1,\"219\":3,\"221\":3,\"222\":4,\"223\":8,\"224\":3,\"226\":2,\"227\":2}}],[\"0799\",{\"1\":{\"148\":1}}],[\"0448\",{\"1\":{\"148\":1}}],[\"05\",{\"1\":{\"148\":1}}],[\"03\",{\"1\":{\"148\":1}}],[\"0972\",{\"1\":{\"148\":1}}],[\"01\",{\"1\":{\"148\":1}}],[\"0\",{\"1\":{\"17\":2,\"18\":1,\"35\":1,\"61\":2,\"78\":4,\"84\":1,\"93\":3,\"104\":5,\"116\":1,\"128\":1,\"133\":1,\"134\":1,\"144\":1,\"148\":12,\"157\":1,\"158\":2,\"160\":6,\"213\":9,\"221\":2,\"222\":1,\"223\":6,\"224\":1,\"227\":5}}],[\"πk​\",{\"1\":{\"219\":2}}],[\"πk+1​=argπmax​rπ​+γpπ​vπk​​\",{\"1\":{\"39\":1}}],[\"πtj​\",{\"1\":{\"200\":2}}],[\"πn\",{\"1\":{\"198\":2}}],[\"π2\",{\"1\":{\"198\":2}}],[\"π2​\",{\"1\":{\"28\":1}}],[\"π1\",{\"1\":{\"198\":2}}],[\"π1​\",{\"1\":{\"28\":1}}],[\"π=\",{\"1\":{\"198\":2}}],[\"πi​\",{\"1\":{\"198\":2}}],[\"πi\",{\"1\":{\"189\":3,\"190\":2,\"200\":1}}],[\"π0​\",{\"1\":{\"44\":1}}],[\"π∗\",{\"1\":{\"28\":1}}],[\"π\",{\"1\":{\"11\":1,\"21\":1,\"22\":2,\"25\":1,\"29\":3,\"35\":1,\"39\":2,\"198\":8,\"199\":1}}],[\"n×n\",{\"1\":{\"161\":1}}],[\"n+1\",{\"1\":{\"161\":2}}],[\"n+2\",{\"1\":{\"160\":3,\"161\":1}}],[\"n+4\",{\"1\":{\"160\":2}}],[\"n=zeros\",{\"1\":{\"160\":1}}],[\"n=16\",{\"1\":{\"160\":1}}],[\"ni\",{\"1\":{\"158\":3}}],[\"ni​\",{\"1\":{\"97\":1}}],[\"ns\",{\"0\":{\"145\":1},\"1\":{\"138\":1}}],[\"ns模型中p是固定不变的\",{\"1\":{\"150\":1}}],[\"ns模型\",{\"0\":{\"138\":1}}],[\"navigating\",{\"1\":{\"193\":1}}],[\"naturally\",{\"1\":{\"185\":1}}],[\"natural\",{\"1\":{\"166\":1}}],[\"nagel\",{\"0\":{\"138\":1},\"1\":{\"138\":4}}],[\"name\",{\"1\":{\"49\":1,\"196\":1,\"226\":2}}],[\"namely\",{\"1\":{\"23\":1,\"29\":1,\"39\":1,\"44\":2,\"46\":1,\"57\":2,\"58\":2,\"198\":1,\"220\":1,\"227\":1}}],[\"namaly\",{\"1\":{\"39\":1}}],[\"nv​\",{\"1\":{\"130\":1}}],[\"nm​\",{\"1\":{\"130\":1}}],[\"n^2\",{\"1\":{\"116\":1}}],[\"numpy\",{\"1\":{\"104\":2}}],[\"num\",{\"1\":{\"104\":17}}],[\"number\",{\"1\":{\"44\":1,\"46\":1,\"81\":1,\"92\":1,\"95\":1,\"100\":1,\"104\":1,\"144\":1,\"200\":1,\"207\":2,\"216\":1,\"219\":2,\"223\":4,\"226\":1,\"227\":2}}],[\"numbers\",{\"1\":{\"43\":1}}],[\"nxgraph\",{\"1\":{\"104\":1}}],[\"nx\",{\"1\":{\"104\":3}}],[\"n2\",{\"1\":{\"87\":2}}],[\"n\",{\"1\":{\"61\":6,\"87\":1,\"92\":1,\"95\":1,\"100\":1,\"101\":1,\"104\":14,\"111\":1,\"116\":1,\"160\":4,\"161\":16,\"188\":2,\"226\":1}}],[\"nn\",{\"1\":{\"61\":7,\"104\":6}}],[\"noise\",{\"1\":{\"111\":1}}],[\"noun\",{\"1\":{\"88\":1}}],[\"node\",{\"1\":{\"87\":3,\"88\":2,\"89\":24,\"91\":2,\"92\":2,\"93\":1,\"95\":1,\"96\":1,\"97\":1,\"98\":4,\"99\":2,\"100\":3,\"101\":2,\"102\":1,\"104\":15,\"124\":3,\"198\":6}}],[\"nodes\",{\"1\":{\"86\":1,\"87\":5,\"88\":2,\"89\":2,\"92\":1,\"93\":1,\"94\":1,\"95\":2,\"96\":1,\"98\":2,\"99\":1,\"100\":1,\"104\":4,\"199\":1}}],[\"nonlinear\",{\"1\":{\"96\":1,\"98\":1,\"101\":1}}],[\"nonempty\",{\"1\":{\"212\":1,\"226\":3}}],[\"none\",{\"1\":{\"82\":1}}],[\"non\",{\"0\":{\"71\":1},\"1\":{\"122\":1,\"123\":1,\"190\":1,\"191\":1,\"196\":2,\"199\":3,\"200\":1,\"203\":2}}],[\"no\",{\"1\":{\"45\":1,\"104\":1,\"122\":1,\"191\":1,\"193\":1,\"195\":1,\"197\":1,\"198\":1,\"199\":1,\"207\":1,\"223\":4,\"224\":3,\"229\":2}}],[\"now\",{\"1\":{\"39\":2,\"227\":3}}],[\"normalization\",{\"1\":{\"115\":1}}],[\"normalize\",{\"1\":{\"98\":1}}],[\"normalized\",{\"0\":{\"97\":1},\"1\":{\"54\":1,\"92\":1,\"93\":2,\"97\":1}}],[\"normally\",{\"1\":{\"82\":1}}],[\"normal\",{\"1\":{\"18\":2}}],[\"notion\",{\"1\":{\"227\":1}}],[\"notice\",{\"1\":{\"21\":1}}],[\"notably\",{\"1\":{\"219\":1,\"224\":1}}],[\"notation\",{\"1\":{\"69\":1,\"168\":1}}],[\"nothing\",{\"1\":{\"215\":1}}],[\"notes\",{\"1\":{\"29\":1}}],[\"note\",{\"1\":{\"25\":1,\"38\":1,\"39\":2,\"226\":1,\"227\":1}}],[\"notebook\",{\"1\":{\"5\":1}}],[\"not\",{\"1\":{\"12\":1,\"18\":1,\"23\":1,\"28\":1,\"35\":1,\"38\":1,\"39\":1,\"43\":2,\"44\":1,\"45\":4,\"46\":5,\"61\":1,\"83\":1,\"104\":1,\"110\":1,\"144\":1,\"146\":1,\"190\":1,\"191\":1,\"196\":1,\"200\":1,\"215\":3,\"217\":5,\"218\":1,\"221\":1,\"223\":2,\"224\":3,\"225\":1,\"226\":3,\"227\":1,\"229\":2,\"231\":1}}],[\"negated\",{\"1\":{\"226\":1}}],[\"negative\",{\"1\":{\"12\":1,\"226\":8,\"227\":5}}],[\"necessarily\",{\"1\":{\"190\":1}}],[\"necessary\",{\"0\":{\"196\":1},\"1\":{\"82\":1,\"227\":1}}],[\"neigh\",{\"1\":{\"160\":1,\"161\":4}}],[\"neighbor\",{\"1\":{\"96\":1,\"101\":1,\"134\":1}}],[\"neighbors\",{\"1\":{\"94\":1,\"97\":1,\"98\":2,\"100\":2,\"101\":1,\"134\":1}}],[\"neighborhood\",{\"1\":{\"91\":1,\"99\":1,\"100\":1}}],[\"neighboring\",{\"1\":{\"89\":2,\"94\":1}}],[\"neumann\",{\"1\":{\"127\":1,\"130\":2}}],[\"neural\",{\"0\":{\"86\":1},\"1\":{\"49\":4,\"86\":4,\"89\":1,\"91\":1,\"94\":1,\"203\":1}}],[\"net\",{\"1\":{\"104\":2,\"222\":1}}],[\"networkx\",{\"1\":{\"104\":2}}],[\"networks\",{\"1\":{\"49\":3,\"86\":4}}],[\"network\",{\"0\":{\"75\":1,\"86\":1},\"1\":{\"49\":5,\"69\":3,\"71\":1,\"74\":2,\"82\":1,\"89\":2,\"91\":2,\"94\":2,\"203\":10}}],[\"nerural\",{\"1\":{\"49\":1}}],[\"nested\",{\"1\":{\"39\":1}}],[\"needs\",{\"1\":{\"25\":1,\"226\":1}}],[\"need\",{\"1\":{\"21\":1,\"29\":2,\"30\":1,\"39\":1,\"46\":2,\"61\":1,\"104\":1,\"193\":1,\"221\":2,\"222\":2,\"223\":1,\"224\":1,\"226\":1,\"229\":1}}],[\"never\",{\"1\":{\"18\":1}}],[\"nearest\",{\"1\":{\"122\":1}}],[\"near\",{\"1\":{\"17\":2,\"83\":1}}],[\"next\",{\"1\":{\"12\":1,\"14\":1,\"58\":1,\"59\":2,\"215\":3,\"223\":1,\"227\":2}}],[\"newly\",{\"1\":{\"198\":1}}],[\"new\",{\"1\":{\"10\":1,\"23\":1,\"38\":3,\"40\":1,\"49\":1,\"89\":1,\"121\":1,\"198\":2,\"203\":1,\"207\":1,\"212\":1,\"218\":1,\"221\":1,\"222\":1,\"224\":1}}],[\"=2\",{\"1\":{\"221\":1}}],[\"=4+0=4\",{\"1\":{\"221\":1}}],[\"=∞+0=∞\",{\"1\":{\"221\":1}}],[\"=∞\",{\"1\":{\"219\":1,\"221\":1,\"222\":2}}],[\"=u1\",{\"1\":{\"222\":1}}],[\"=u4​min​l\",{\"1\":{\"221\":1}}],[\"=u4min​l\",{\"1\":{\"221\":1}}],[\"=uk−1min​l\",{\"1\":{\"222\":1}}],[\"=ukmin​l\",{\"1\":{\"221\":1}}],[\"=uk\",{\"1\":{\"221\":1}}],[\"=uforallx∈x\",{\"1\":{\"213\":1}}],[\"=utanh\",{\"1\":{\"53\":1}}],[\"=f\",{\"1\":{\"212\":1,\"215\":1}}],[\"=g\",{\"1\":{\"200\":1}}],[\"=∑i∈i​gi​\",{\"1\":{\"198\":1}}],[\"=∑k∈ni​​exp\",{\"1\":{\"97\":1}}],[\"=j=1∑k−1​cost\",{\"1\":{\"189\":1}}],[\"=1+1=2\",{\"1\":{\"221\":1}}],[\"=1+0=1\",{\"1\":{\"221\":1}}],[\"=1\",{\"1\":{\"161\":1}}],[\"=1e10\",{\"1\":{\"161\":1}}],[\"=1−γγ​\",{\"1\":{\"17\":1}}],[\"=border\",{\"1\":{\"161\":1}}],[\"=inf\",{\"1\":{\"161\":1}}],[\"=i=1∑n​similarity\",{\"1\":{\"60\":1}}],[\"=li​\",{\"1\":{\"222\":1}}],[\"=l\",{\"1\":{\"161\":1,\"221\":4}}],[\"=sumk=1k​l\",{\"1\":{\"219\":1}}],[\"=sqrt\",{\"1\":{\"161\":1}}],[\"=softmax\",{\"1\":{\"59\":1}}],[\"=⎩⎨⎧​0\",{\"1\":{\"133\":1}}],[\"=xf​\",{\"1\":{\"221\":1}}],[\"=x+u\",{\"1\":{\"213\":1}}],[\"=x\",{\"1\":{\"93\":1}}],[\"=x∑​p\",{\"1\":{\"43\":1}}],[\"=σ\",{\"1\":{\"93\":1}}],[\"==0\",{\"1\":{\"161\":2}}],[\"==\",{\"1\":{\"61\":3,\"104\":2}}],[\"=htws\",{\"1\":{\"53\":1}}],[\"=dk​​qkt​\",{\"1\":{\"53\":1}}],[\"=0\",{\"1\":{\"43\":1,\"160\":12,\"161\":1,\"219\":1,\"222\":1}}],[\"=amax​q\",{\"1\":{\"38\":1}}],[\"=amax​qk​\",{\"1\":{\"34\":1}}],[\"=argamax​q\",{\"1\":{\"38\":1,\"39\":1}}],[\"=a∑​π\",{\"1\":{\"23\":2,\"25\":1}}],[\"=πmax​rπ​+γpπ​vk​\",{\"1\":{\"38\":1}}],[\"=πmax​rπ​+γpπ​v\",{\"1\":{\"38\":1}}],[\"=πmax​\",{\"1\":{\"34\":1}}],[\"=πmax​a∑​π\",{\"1\":{\"29\":1,\"30\":1,\"34\":1}}],[\"=relu​​0\",{\"1\":{\"93\":1}}],[\"=relu\",{\"1\":{\"93\":1}}],[\"=r∑​p\",{\"1\":{\"25\":1,\"39\":1}}],[\"=rπ​+γpπ​vk​\",{\"1\":{\"24\":1}}],[\"=rπ​+γpπ​vπ​\",{\"1\":{\"24\":1}}],[\"=rπ​\",{\"1\":{\"23\":1}}],[\"=e\",{\"1\":{\"25\":1,\"66\":1,\"75\":1}}],[\"=meanofimmediaterewarda∑​π\",{\"1\":{\"23\":1}}],[\"=\",{\"1\":{\"9\":1,\"10\":1,\"18\":2,\"24\":1,\"61\":21,\"82\":3,\"84\":4,\"93\":1,\"104\":31,\"111\":1,\"130\":2,\"148\":2,\"160\":12,\"161\":18,\"199\":1,\"213\":3,\"215\":2,\"216\":1,\"217\":6,\"224\":2,\"226\":1,\"227\":1}}],[\"$u\",{\"1\":{\"223\":1}}],[\"$q\",{\"1\":{\"44\":1}}],[\"$v\",{\"1\":{\"35\":1}}],[\"$st\",{\"1\":{\"21\":1}}],[\"$\",{\"1\":{\"9\":2,\"10\":2,\"11\":2,\"44\":1,\"160\":4,\"223\":1}}],[\"d=sm\",{\"1\":{\"161\":1}}],[\"dn\",{\"1\":{\"150\":2}}],[\"dn​<δtmin\",{\"1\":{\"150\":1}}],[\"d等人在单车道元胞自动机nasch模型的基础上提出了双车道元胞自动机stca模型\",{\"1\":{\"150\":1}}],[\"daily\",{\"0\":{\"235\":1}}],[\"dangerous\",{\"1\":{\"140\":1}}],[\"datasets\",{\"1\":{\"166\":1,\"167\":1}}],[\"dataset\",{\"1\":{\"120\":2,\"122\":1}}],[\"data\",{\"1\":{\"45\":2,\"49\":2,\"78\":1,\"86\":2,\"87\":1,\"91\":1,\"94\":1,\"111\":1,\"119\":1,\"120\":1,\"121\":4,\"122\":2,\"123\":4,\"124\":2}}],[\"dbscan\",{\"1\":{\"116\":8}}],[\"driving\",{\"1\":{\"144\":1}}],[\"drive\",{\"1\":{\"144\":2}}],[\"drivers\",{\"1\":{\"144\":4}}],[\"dropout\",{\"1\":{\"111\":1}}],[\"draw\",{\"1\":{\"104\":1}}],[\"drastic\",{\"1\":{\"83\":1}}],[\"duplicate\",{\"1\":{\"215\":1,\"217\":3}}],[\"during\",{\"1\":{\"94\":1,\"95\":1,\"99\":1,\"199\":2,\"215\":1}}],[\"due\",{\"1\":{\"24\":1}}],[\"d~=diag\",{\"1\":{\"93\":1}}],[\"d~\",{\"1\":{\"92\":1}}],[\"dynamic\",{\"1\":{\"81\":1,\"154\":1,\"158\":1,\"224\":2}}],[\"dynamically\",{\"1\":{\"48\":1,\"94\":1}}],[\"d\",{\"1\":{\"61\":2,\"69\":1,\"72\":1,\"82\":4,\"92\":1,\"104\":9,\"111\":2,\"144\":1,\"160\":1,\"221\":15,\"222\":3,\"223\":1}}],[\"dk​\",{\"1\":{\"53\":1}}],[\"dirt\",{\"1\":{\"226\":1}}],[\"direction\",{\"1\":{\"185\":1,\"223\":1}}],[\"directions\",{\"1\":{\"168\":2,\"189\":1,\"213\":1}}],[\"directionality\",{\"1\":{\"87\":1}}],[\"directed\",{\"1\":{\"87\":1,\"218\":1}}],[\"directly\",{\"1\":{\"8\":1,\"21\":1,\"44\":3,\"78\":1,\"229\":1}}],[\"dijkstra\",{\"0\":{\"224\":1},\"1\":{\"216\":1,\"220\":1,\"224\":4}}],[\"di\",{\"1\":{\"158\":2,\"161\":1}}],[\"diverse\",{\"1\":{\"166\":1}}],[\"diverge\",{\"1\":{\"110\":1}}],[\"divided\",{\"1\":{\"120\":1,\"143\":1}}],[\"difficult\",{\"1\":{\"203\":1,\"229\":1}}],[\"difficulty\",{\"1\":{\"111\":1}}],[\"difficuty\",{\"1\":{\"146\":1}}],[\"differ\",{\"1\":{\"103\":1}}],[\"differs\",{\"1\":{\"53\":1}}],[\"differences\",{\"0\":{\"40\":1},\"1\":{\"40\":1,\"44\":1,\"199\":1,\"223\":1,\"224\":1}}],[\"difference\",{\"1\":{\"25\":1,\"37\":1,\"46\":1,\"203\":1}}],[\"differential\",{\"1\":{\"207\":2,\"208\":1}}],[\"differentiable\",{\"1\":{\"203\":2}}],[\"differentiability\",{\"0\":{\"71\":1}}],[\"differentiation\",{\"1\":{\"82\":1}}],[\"differentiating\",{\"1\":{\"82\":1}}],[\"different\",{\"1\":{\"22\":2,\"30\":1,\"46\":1,\"48\":1,\"67\":1,\"120\":2,\"123\":1,\"191\":1,\"193\":2,\"199\":1,\"216\":1,\"223\":2,\"226\":2,\"229\":1}}],[\"diagonal\",{\"1\":{\"104\":2}}],[\"diag\",{\"1\":{\"104\":1}}],[\"diagram所示\",{\"1\":{\"140\":1}}],[\"diagram\",{\"0\":{\"141\":1},\"1\":{\"49\":1,\"140\":1,\"141\":1,\"148\":2}}],[\"dii​=∑j​aij​\",{\"1\":{\"92\":1}}],[\"dim2=2\",{\"1\":{\"104\":1}}],[\"dim1=1\",{\"1\":{\"104\":1}}],[\"dim=\",{\"1\":{\"61\":1,\"104\":1}}],[\"dim=0\",{\"1\":{\"61\":3,\"104\":2}}],[\"dim\",{\"1\":{\"61\":7}}],[\"dimension\",{\"1\":{\"53\":1,\"61\":5,\"78\":1,\"92\":1,\"95\":1,\"100\":1,\"104\":2,\"109\":1}}],[\"dimensionality\",{\"1\":{\"121\":2}}],[\"dimensional\",{\"1\":{\"49\":1,\"123\":1,\"213\":1}}],[\"disjoint\",{\"1\":{\"190\":1}}],[\"dis\",{\"1\":{\"161\":6}}],[\"discuss\",{\"1\":{\"220\":1}}],[\"discusses\",{\"1\":{\"219\":1,\"228\":1}}],[\"discrete\",{\"0\":{\"210\":1,\"211\":1,\"213\":1,\"219\":1,\"225\":1},\"1\":{\"134\":1,\"143\":1,\"168\":2,\"207\":2,\"213\":1,\"219\":1,\"225\":1,\"226\":1}}],[\"disconnected\",{\"1\":{\"87\":1}}],[\"discount\",{\"1\":{\"17\":1,\"35\":1}}],[\"discounted\",{\"0\":{\"17\":1},\"1\":{\"17\":2,\"22\":1,\"23\":1}}],[\"discarded\",{\"1\":{\"82\":1,\"195\":1}}],[\"disturbances\",{\"1\":{\"140\":2}}],[\"distinct\",{\"1\":{\"122\":1,\"154\":1}}],[\"distinguished\",{\"1\":{\"227\":1}}],[\"distinguish\",{\"1\":{\"18\":1,\"219\":1,\"223\":1}}],[\"distance\",{\"1\":{\"69\":1,\"123\":1,\"143\":1,\"144\":1,\"216\":2,\"219\":1}}],[\"distributions\",{\"1\":{\"21\":1}}],[\"distribution\",{\"0\":{\"13\":1},\"1\":{\"21\":1,\"43\":1,\"59\":3}}],[\"display\",{\"1\":{\"11\":1}}],[\"done\",{\"1\":{\"223\":1}}],[\"don\",{\"1\":{\"196\":1,\"223\":1,\"227\":1}}],[\"dominance\",{\"0\":{\"193\":1,\"195\":1,\"196\":1},\"1\":{\"194\":1,\"196\":2}}],[\"dominates\",{\"1\":{\"191\":1,\"195\":2,\"196\":1}}],[\"dominated\",{\"1\":{\"17\":2,\"191\":1,\"196\":4,\"199\":1,\"200\":1}}],[\"dot\",{\"1\":{\"53\":1,\"78\":1}}],[\"downloading\",{\"1\":{\"205\":1}}],[\"down\",{\"1\":{\"49\":1,\"168\":2,\"213\":1}}],[\"doesn\",{\"1\":{\"82\":1,\"87\":1}}],[\"does\",{\"1\":{\"28\":1,\"44\":1,\"45\":1,\"46\":1,\"224\":1,\"225\":1,\"226\":1}}],[\"do\",{\"1\":{\"10\":1,\"21\":1,\"25\":1,\"40\":2,\"43\":2,\"45\":5,\"46\":1,\"172\":1,\"215\":1,\"217\":2,\"224\":1}}],[\"degrades\",{\"1\":{\"224\":1}}],[\"degree\",{\"1\":{\"92\":2,\"93\":1,\"104\":1,\"111\":1}}],[\"denote\",{\"1\":{\"219\":2,\"227\":2}}],[\"denotes\",{\"1\":{\"213\":1,\"222\":1}}],[\"denoted\",{\"1\":{\"10\":1,\"11\":2,\"22\":1,\"43\":1,\"189\":1,\"190\":1,\"199\":1,\"223\":1}}],[\"density\",{\"1\":{\"141\":2}}],[\"developed\",{\"1\":{\"229\":1}}],[\"develop\",{\"1\":{\"185\":1,\"229\":2}}],[\"develped\",{\"1\":{\"138\":1}}],[\"device\",{\"1\":{\"207\":1}}],[\"device=device\",{\"1\":{\"81\":1}}],[\"deviation\",{\"1\":{\"109\":1,\"111\":1}}],[\"dead\",{\"1\":{\"134\":1,\"215\":3}}],[\"depth\",{\"1\":{\"207\":1,\"216\":6}}],[\"depicts\",{\"1\":{\"111\":3}}],[\"dependency\",{\"1\":{\"223\":1}}],[\"dependent\",{\"1\":{\"150\":1}}],[\"depending\",{\"1\":{\"81\":1}}],[\"depends\",{\"1\":{\"12\":1,\"14\":1,\"25\":2,\"190\":1,\"223\":2,\"229\":1}}],[\"demonstrated\",{\"1\":{\"216\":1}}],[\"demonstrate\",{\"1\":{\"203\":1}}],[\"demonstrates\",{\"1\":{\"49\":1}}],[\"demention\",{\"1\":{\"78\":1}}],[\"defining\",{\"1\":{\"216\":1}}],[\"definition\",{\"0\":{\"195\":1},\"1\":{\"22\":1,\"25\":1,\"43\":1,\"199\":1,\"221\":1}}],[\"define\",{\"1\":{\"84\":1,\"93\":1,\"98\":1,\"226\":2}}],[\"defined\",{\"1\":{\"67\":1,\"123\":1,\"219\":2,\"226\":1,\"229\":1}}],[\"def\",{\"1\":{\"61\":2,\"104\":2}}],[\"deepening\",{\"1\":{\"216\":1}}],[\"deep\",{\"1\":{\"49\":1,\"86\":1,\"166\":1,\"203\":1}}],[\"decide\",{\"1\":{\"229\":1}}],[\"decisions\",{\"1\":{\"207\":1}}],[\"decision\",{\"0\":{\"8\":1,\"124\":1},\"1\":{\"8\":1,\"89\":1,\"110\":1,\"124\":3,\"197\":1,\"206\":1,\"208\":1}}],[\"decades\",{\"1\":{\"207\":1}}],[\"decrements\",{\"1\":{\"168\":2,\"213\":1}}],[\"decrement\",{\"1\":{\"168\":2}}],[\"decreasing\",{\"1\":{\"81\":1,\"190\":1}}],[\"decelerates\",{\"1\":{\"146\":1}}],[\"decelerate\",{\"1\":{\"140\":1}}],[\"decoding\",{\"0\":{\"59\":1}}],[\"decoder\",{\"1\":{\"48\":1,\"49\":6,\"58\":2,\"59\":5}}],[\"decompose\",{\"1\":{\"45\":1}}],[\"derivatives\",{\"1\":{\"82\":1}}],[\"derivative\",{\"0\":{\"106\":1},\"1\":{\"71\":1,\"84\":3,\"203\":2}}],[\"deriving\",{\"1\":{\"37\":1,\"43\":1}}],[\"derived\",{\"1\":{\"25\":1,\"226\":1}}],[\"derive\",{\"1\":{\"23\":1,\"39\":1,\"40\":2,\"58\":1}}],[\"destroy\",{\"1\":{\"229\":1}}],[\"destination\",{\"1\":{\"226\":1}}],[\"deserve\",{\"1\":{\"194\":1}}],[\"desirable\",{\"1\":{\"226\":2}}],[\"desire\",{\"1\":{\"144\":1}}],[\"designed\",{\"1\":{\"86\":1,\"91\":1,\"94\":1}}],[\"design\",{\"0\":{\"75\":1},\"1\":{\"35\":1,\"49\":1,\"89\":1,\"154\":1}}],[\"descent\",{\"1\":{\"110\":2}}],[\"describe\",{\"1\":{\"23\":1,\"87\":1}}],[\"described\",{\"1\":{\"7\":1}}],[\"description\",{\"0\":{\"142\":1},\"1\":{\"8\":1,\"69\":1}}],[\"determine\",{\"0\":{\"35\":1},\"1\":{\"81\":1,\"203\":1,\"215\":1,\"218\":1,\"227\":1}}],[\"determined\",{\"1\":{\"25\":1,\"59\":1,\"207\":1}}],[\"deterministic\",{\"1\":{\"22\":1,\"28\":1,\"38\":1,\"46\":2}}],[\"details\",{\"1\":{\"49\":1,\"168\":1}}],[\"detail\",{\"1\":{\"24\":1,\"45\":1}}],[\"delving\",{\"1\":{\"21\":1}}],[\"p=zeros\",{\"1\":{\"160\":1}}],[\"pp\",{\"1\":{\"160\":1,\"161\":8}}],[\"p称为随机慢化概率\",{\"1\":{\"144\":1}}],[\"physical\",{\"1\":{\"207\":1}}],[\"physics\",{\"1\":{\"154\":1}}],[\"physicists\",{\"1\":{\"138\":1}}],[\"phantom\",{\"0\":{\"140\":1},\"1\":{\"144\":1}}],[\"phase\",{\"0\":{\"57\":1,\"59\":1},\"1\":{\"154\":2}}],[\"pca\",{\"0\":{\"121\":1},\"1\":{\"117\":4,\"121\":1}}],[\"p10\",{\"1\":{\"112\":1}}],[\"p1\",{\"1\":{\"112\":1}}],[\"plt\",{\"1\":{\"104\":4}}],[\"pla\",{\"0\":{\"239\":1}}],[\"plans\",{\"0\":{\"214\":1,\"220\":1,\"223\":1,\"229\":1},\"1\":{\"207\":2,\"216\":2,\"222\":1,\"223\":5,\"224\":1,\"228\":1,\"229\":6}}],[\"plan\",{\"1\":{\"206\":1,\"207\":17,\"219\":2,\"223\":6,\"226\":2,\"229\":11}}],[\"planned\",{\"1\":{\"207\":1}}],[\"planner\",{\"1\":{\"185\":2}}],[\"planning\",{\"0\":{\"168\":1,\"206\":1,\"207\":1,\"210\":1,\"211\":1,\"213\":1,\"219\":1,\"225\":1,\"228\":1},\"1\":{\"154\":2,\"205\":1,\"206\":1,\"207\":3,\"208\":3,\"217\":1,\"219\":2,\"223\":5,\"224\":2,\"225\":1,\"226\":4}}],[\"platforms\",{\"1\":{\"167\":1}}],[\"platform\",{\"1\":{\"160\":3,\"161\":5}}],[\"platform=ones\",{\"1\":{\"160\":1}}],[\"plateaus\",{\"1\":{\"81\":1,\"83\":1}}],[\"places\",{\"1\":{\"227\":1}}],[\"placecap\",{\"1\":{\"226\":1}}],[\"placed\",{\"1\":{\"7\":1}}],[\"place\",{\"1\":{\"7\":2,\"226\":2,\"227\":2}}],[\"pyplot\",{\"1\":{\"104\":1}}],[\"pytorch\",{\"0\":{\"80\":1},\"1\":{\"82\":3}}],[\"psychological\",{\"1\":{\"154\":1}}],[\"ps\",{\"1\":{\"84\":3}}],[\"pseudocode\",{\"1\":{\"38\":1,\"39\":1,\"41\":1,\"44\":1}}],[\"past\",{\"1\":{\"223\":1}}],[\"pass\",{\"1\":{\"82\":1,\"89\":1}}],[\"passing\",{\"0\":{\"89\":1},\"1\":{\"82\":1,\"89\":6,\"203\":1}}],[\"passed\",{\"1\":{\"54\":1,\"58\":1,\"59\":2,\"89\":1,\"218\":1}}],[\"paper\",{\"0\":{\"186\":1,\"237\":1},\"1\":{\"184\":2,\"185\":1,\"194\":1,\"201\":1,\"203\":1}}],[\"pathπi\",{\"1\":{\"189\":1}}],[\"paths\",{\"0\":{\"189\":1},\"1\":{\"185\":1,\"190\":1,\"196\":1,\"199\":1,\"224\":1}}],[\"path\",{\"0\":{\"184\":1,\"238\":1},\"1\":{\"185\":2,\"189\":2,\"190\":4,\"192\":1,\"198\":3,\"207\":3,\"218\":1,\"223\":1,\"224\":2}}],[\"patterns\",{\"1\":{\"89\":1,\"154\":1}}],[\"pattern\",{\"1\":{\"49\":1}}],[\"pageinfo\",{\"0\":{\"64\":1}}],[\"pay\",{\"1\":{\"48\":1}}],[\"pairs\",{\"1\":{\"45\":3,\"46\":1,\"213\":1,\"227\":1,\"229\":1}}],[\"pair\",{\"1\":{\"44\":2,\"45\":4,\"46\":2,\"49\":1,\"227\":1}}],[\"pareto\",{\"0\":{\"191\":1,\"193\":1,\"194\":1,\"197\":1},\"1\":{\"185\":2,\"191\":3,\"192\":1,\"194\":2,\"196\":1,\"197\":1,\"199\":1,\"200\":3}}],[\"parallel\",{\"1\":{\"143\":1}}],[\"params\",{\"1\":{\"84\":2}}],[\"parameter\",{\"1\":{\"83\":1,\"104\":2,\"110\":1,\"203\":1}}],[\"parameters\",{\"0\":{\"84\":1},\"1\":{\"46\":1,\"53\":1,\"59\":1,\"69\":2,\"71\":1,\"78\":1,\"81\":1,\"83\":2,\"84\":2,\"104\":1,\"119\":1,\"166\":1}}],[\"paramount\",{\"1\":{\"44\":1}}],[\"partial\",{\"0\":{\"229\":1},\"1\":{\"226\":4,\"229\":15}}],[\"partitions\",{\"1\":{\"122\":1}}],[\"partitioning\",{\"1\":{\"120\":1}}],[\"particularly\",{\"1\":{\"82\":1}}],[\"particular\",{\"0\":{\"216\":1},\"1\":{\"45\":1,\"229\":2}}],[\"parts\",{\"1\":{\"48\":1,\"87\":1,\"88\":1,\"134\":1}}],[\"part\",{\"1\":{\"21\":1,\"23\":1,\"67\":1,\"104\":2,\"208\":4,\"223\":1,\"227\":1}}],[\"pπk​​\",{\"1\":{\"39\":1}}],[\"pπ​\",{\"1\":{\"23\":1,\"39\":1}}],[\"pπ​∈rn×n\",{\"1\":{\"23\":1}}],[\"people\",{\"1\":{\"160\":1}}],[\"pedestrians\",{\"1\":{\"154\":1}}],[\"pedestrian\",{\"0\":{\"153\":1},\"1\":{\"154\":3}}],[\"penalties\",{\"1\":{\"119\":1}}],[\"penalty\",{\"1\":{\"119\":1}}],[\"per\",{\"1\":{\"207\":1}}],[\"period也逐渐变小\",{\"1\":{\"148\":1}}],[\"periodic\",{\"1\":{\"147\":1}}],[\"period\",{\"1\":{\"140\":1}}],[\"perceptron\",{\"1\":{\"114\":1}}],[\"perturbations\",{\"1\":{\"111\":1}}],[\"permutation\",{\"1\":{\"87\":4}}],[\"performs\",{\"1\":{\"216\":1}}],[\"performed\",{\"1\":{\"207\":1}}],[\"performing\",{\"1\":{\"82\":1}}],[\"perform\",{\"1\":{\"78\":1,\"87\":1,\"216\":1,\"222\":1}}],[\"performance\",{\"1\":{\"46\":1,\"111\":1,\"120\":3,\"224\":1,\"227\":1,\"229\":1}}],[\"perspective\",{\"0\":{\"8\":1}}],[\"pe\",{\"1\":{\"39\":1}}],[\"purpose\",{\"1\":{\"213\":1,\"229\":1}}],[\"puzzle\",{\"1\":{\"207\":1,\"213\":1}}],[\"published\",{\"1\":{\"184\":1,\"205\":1}}],[\"putting\",{\"1\":{\"226\":2}}],[\"put\",{\"1\":{\"49\":1,\"226\":1}}],[\"pu\",{\"1\":{\"38\":1}}],[\"pdf\",{\"1\":{\"37\":1}}],[\"p\",{\"1\":{\"13\":2,\"21\":2,\"22\":2,\"25\":1,\"29\":2,\"35\":2,\"39\":2,\"59\":1,\"75\":1,\"81\":1,\"84\":2,\"144\":1,\"148\":1,\"226\":1,\"227\":2}}],[\"po\",{\"1\":{\"161\":2}}],[\"po=1\",{\"1\":{\"160\":1}}],[\"potential\",{\"1\":{\"158\":3,\"221\":1}}],[\"potentially\",{\"1\":{\"83\":1,\"88\":1}}],[\"pow\",{\"1\":{\"104\":1}}],[\"powerful\",{\"1\":{\"48\":1}}],[\"pooling\",{\"1\":{\"89\":1,\"100\":1,\"101\":1}}],[\"pooled\",{\"1\":{\"89\":1}}],[\"points\",{\"0\":{\"186\":1},\"1\":{\"122\":1,\"123\":1}}],[\"point\",{\"1\":{\"25\":1,\"32\":2,\"83\":1,\"122\":1,\"213\":1,\"214\":1,\"215\":1}}],[\"policies\",{\"1\":{\"24\":1,\"44\":1,\"46\":1}}],[\"policyπ\",{\"1\":{\"21\":1}}],[\"policy\",{\"0\":{\"11\":1,\"27\":1,\"28\":1,\"35\":1,\"37\":1,\"39\":1,\"40\":1,\"41\":1},\"1\":{\"7\":2,\"8\":1,\"11\":3,\"16\":1,\"18\":2,\"21\":1,\"22\":3,\"24\":5,\"27\":2,\"28\":8,\"29\":1,\"30\":1,\"37\":3,\"38\":4,\"39\":11,\"40\":7,\"41\":6,\"43\":2,\"44\":12,\"45\":6,\"46\":13}}],[\"position\",{\"1\":{\"207\":2}}],[\"positive\",{\"1\":{\"46\":1,\"226\":8,\"227\":5}}],[\"possibly\",{\"1\":{\"215\":1,\"227\":1,\"229\":1}}],[\"possible\",{\"1\":{\"22\":1,\"23\":1,\"25\":2,\"59\":1,\"144\":1,\"185\":1,\"188\":1,\"191\":1,\"196\":1,\"199\":1,\"207\":1,\"213\":1,\"215\":1,\"227\":2,\"229\":1}}],[\"possess\",{\"1\":{\"166\":1}}],[\"pose\",{\"1\":{\"154\":1}}],[\"postpruning\",{\"1\":{\"113\":1}}],[\"postion\",{\"1\":{\"23\":1,\"198\":1}}],[\"postive\",{\"1\":{\"12\":1}}],[\"pixel\",{\"1\":{\"88\":1}}],[\"pictures\",{\"1\":{\"59\":1}}],[\"picture\",{\"1\":{\"49\":1,\"89\":1,\"207\":1}}],[\"pi\",{\"1\":{\"11\":1,\"39\":1,\"40\":3,\"44\":1}}],[\"prune\",{\"1\":{\"224\":1}}],[\"pruning\",{\"1\":{\"196\":1}}],[\"pr\",{\"2\":{\"202\":1,\"204\":1,\"209\":1,\"230\":1}}],[\"principal\",{\"1\":{\"121\":4}}],[\"principle\",{\"0\":{\"121\":1,\"122\":1,\"123\":1,\"124\":1},\"1\":{\"110\":1,\"123\":1}}],[\"print\",{\"1\":{\"61\":3,\"104\":3}}],[\"prioritizing\",{\"1\":{\"199\":1}}],[\"prioritize\",{\"1\":{\"199\":2,\"224\":1}}],[\"priority\",{\"1\":{\"197\":1,\"217\":1,\"218\":1,\"224\":2}}],[\"priori\",{\"1\":{\"67\":1}}],[\"practice\",{\"1\":{\"46\":1}}],[\"predicate\",{\"1\":{\"226\":7,\"227\":2}}],[\"predicates\",{\"1\":{\"226\":8,\"227\":3}}],[\"predicts\",{\"1\":{\"124\":1}}],[\"predict\",{\"1\":{\"88\":2,\"203\":1}}],[\"predicting\",{\"1\":{\"88\":1}}],[\"predictions\",{\"1\":{\"88\":1}}],[\"prediction\",{\"1\":{\"88\":2,\"91\":1,\"111\":1,\"203\":2}}],[\"predecessor\",{\"1\":{\"223\":1}}],[\"press\",{\"1\":{\"205\":1}}],[\"presented\",{\"1\":{\"214\":1,\"215\":1}}],[\"presents\",{\"1\":{\"203\":1,\"216\":1}}],[\"preserving\",{\"1\":{\"121\":1}}],[\"preserve\",{\"1\":{\"87\":1}}],[\"preset\",{\"1\":{\"104\":1}}],[\"preference\",{\"1\":{\"197\":1}}],[\"preferences\",{\"1\":{\"197\":1}}],[\"precondition\",{\"1\":{\"229\":3}}],[\"preconditions\",{\"1\":{\"226\":4}}],[\"preclude\",{\"1\":{\"168\":1}}],[\"preceeding\",{\"1\":{\"143\":1}}],[\"precisely\",{\"1\":{\"83\":1}}],[\"precise\",{\"1\":{\"43\":1}}],[\"prepruning\",{\"1\":{\"113\":1}}],[\"prevent\",{\"1\":{\"119\":1,\"168\":1}}],[\"prevents\",{\"1\":{\"82\":1}}],[\"preventing\",{\"1\":{\"82\":1}}],[\"previously\",{\"1\":{\"49\":1,\"59\":1,\"227\":1}}],[\"previous\",{\"1\":{\"14\":1,\"21\":1,\"49\":1,\"59\":3,\"199\":1,\"223\":1}}],[\"pre\",{\"1\":{\"82\":1,\"166\":1}}],[\"preliminary\",{\"1\":{\"48\":1}}],[\"preliminaries\",{\"0\":{\"32\":1,\"49\":1},\"1\":{\"29\":1,\"57\":1}}],[\"preocess\",{\"1\":{\"8\":1}}],[\"pros\",{\"1\":{\"225\":1}}],[\"proot​\",{\"1\":{\"198\":2}}],[\"proof\",{\"0\":{\"67\":1,\"72\":1},\"1\":{\"23\":1,\"221\":1}}],[\"prompts\",{\"1\":{\"167\":2}}],[\"projection\",{\"1\":{\"121\":1}}],[\"progress\",{\"1\":{\"81\":1}}],[\"program\",{\"1\":{\"61\":1}}],[\"programming\",{\"1\":{\"11\":1,\"224\":1}}],[\"provided\",{\"1\":{\"203\":1}}],[\"provides\",{\"1\":{\"81\":1,\"199\":1}}],[\"provide\",{\"1\":{\"74\":1,\"167\":1,\"224\":1}}],[\"prove\",{\"1\":{\"46\":1}}],[\"product\",{\"1\":{\"53\":1,\"82\":1}}],[\"produces\",{\"1\":{\"212\":1,\"221\":1,\"226\":1}}],[\"produce\",{\"1\":{\"49\":1,\"54\":1,\"59\":2,\"104\":1,\"167\":1,\"224\":1}}],[\"properties\",{\"1\":{\"226\":1}}],[\"property\",{\"0\":{\"14\":1,\"33\":1},\"1\":{\"14\":1,\"87\":1}}],[\"propotion\",{\"1\":{\"148\":1}}],[\"proposed\",{\"1\":{\"89\":1,\"203\":1}}],[\"proposition\",{\"1\":{\"41\":1}}],[\"propagation\",{\"0\":{\"93\":1,\"101\":1}}],[\"propagating\",{\"1\":{\"86\":1}}],[\"propagates\",{\"1\":{\"224\":1}}],[\"propagate\",{\"1\":{\"82\":1}}],[\"procedure\",{\"1\":{\"34\":2,\"38\":1}}],[\"processing\",{\"1\":{\"49\":1,\"166\":1}}],[\"processes\",{\"1\":{\"44\":1,\"57\":1}}],[\"process\",{\"0\":{\"8\":1},\"1\":{\"8\":1,\"12\":1,\"21\":1,\"39\":1,\"40\":4,\"49\":2,\"57\":1,\"59\":1,\"86\":1,\"89\":2,\"91\":1,\"94\":1,\"104\":1,\"122\":1,\"199\":1,\"203\":2,\"221\":1,\"226\":1}}],[\"prob\",{\"1\":{\"161\":2}}],[\"prob=zeros\",{\"1\":{\"160\":1}}],[\"probabilitiepi\",{\"1\":{\"158\":1}}],[\"probability\",{\"0\":{\"13\":1},\"1\":{\"11\":1,\"13\":5,\"21\":1,\"30\":1,\"46\":2,\"59\":4,\"144\":1,\"146\":1}}],[\"probably\",{\"1\":{\"43\":1}}],[\"problems\",{\"1\":{\"30\":1,\"88\":3,\"207\":2,\"217\":1,\"219\":1,\"224\":3,\"225\":2,\"226\":1}}],[\"problem\",{\"0\":{\"69\":1,\"80\":1,\"187\":1,\"203\":1,\"212\":1},\"1\":{\"24\":1,\"43\":1,\"45\":1,\"49\":1,\"69\":1,\"88\":1,\"89\":1,\"111\":1,\"185\":1,\"191\":1,\"192\":3,\"193\":1,\"200\":3,\"203\":4,\"207\":1,\"225\":1,\"226\":2,\"227\":1,\"229\":1}}],[\"15\",{\"1\":{\"199\":1,\"208\":1}}],[\"1534\",{\"1\":{\"148\":1}}],[\"1拍\",{\"1\":{\"181\":1}}],[\"1s\",{\"1\":{\"181\":1}}],[\"1e10\",{\"1\":{\"161\":1}}],[\"11\",{\"1\":{\"160\":1,\"207\":1}}],[\"115\",{\"1\":{\"148\":1}}],[\"14\",{\"1\":{\"160\":2,\"224\":1}}],[\"140\",{\"1\":{\"148\":1}}],[\"1×4\",{\"1\":{\"155\":1}}],[\"1×1\",{\"1\":{\"155\":3}}],[\"16\",{\"1\":{\"160\":1}}],[\"16×30\",{\"1\":{\"155\":1}}],[\"1647\",{\"1\":{\"148\":1}}],[\"13\",{\"1\":{\"160\":1,\"208\":1}}],[\"1377\",{\"1\":{\"148\":1}}],[\"131\",{\"1\":{\"148\":1}}],[\"138\",{\"1\":{\"148\":1}}],[\"1702\",{\"1\":{\"148\":1}}],[\"1782\",{\"1\":{\"148\":1}}],[\"12\",{\"1\":{\"160\":1,\"199\":1,\"208\":1}}],[\"1241\",{\"1\":{\"148\":1}}],[\"127\",{\"1\":{\"148\":1}}],[\"126\",{\"1\":{\"148\":3}}],[\"122\",{\"1\":{\"148\":1}}],[\"19\",{\"1\":{\"148\":1}}],[\"1970\",{\"1\":{\"133\":1}}],[\"184\",{\"0\":{\"139\":1},\"1\":{\"139\":2}}],[\"1−1​−11​\",{\"1\":{\"93\":2,\"98\":1}}],[\"1+2+3=3+1+2=2+1+3\",{\"1\":{\"87\":1}}],[\"101\",{\"1\":{\"227\":1}}],[\"10100\",{\"1\":{\"225\":1}}],[\"100\",{\"1\":{\"213\":4}}],[\"10分钟掌握图神经网络及其经典模型\",{\"1\":{\"86\":1}}],[\"10\",{\"1\":{\"61\":1,\"160\":1,\"199\":2,\"224\":3}}],[\"1\",{\"0\":{\"7\":1,\"71\":1,\"81\":1,\"119\":1,\"137\":1,\"138\":2,\"139\":1,\"140\":1,\"141\":1,\"143\":1,\"146\":1,\"188\":1,\"194\":1,\"205\":1,\"206\":2,\"207\":1,\"208\":1,\"211\":1,\"212\":2,\"213\":1,\"215\":1,\"220\":1,\"221\":2,\"222\":1,\"226\":1,\"229\":1},\"1\":{\"17\":3,\"18\":1,\"23\":1,\"34\":1,\"38\":1,\"39\":2,\"44\":1,\"49\":2,\"53\":1,\"54\":1,\"58\":1,\"59\":1,\"61\":6,\"67\":1,\"78\":2,\"82\":3,\"86\":1,\"89\":1,\"93\":2,\"98\":6,\"104\":4,\"110\":3,\"111\":6,\"112\":3,\"113\":1,\"115\":6,\"116\":1,\"117\":3,\"127\":1,\"128\":1,\"133\":2,\"134\":1,\"144\":2,\"146\":1,\"148\":6,\"157\":1,\"158\":1,\"160\":29,\"161\":31,\"167\":1,\"168\":1,\"177\":1,\"181\":1,\"188\":1,\"193\":1,\"197\":5,\"198\":1,\"207\":1,\"208\":2,\"212\":1,\"213\":5,\"216\":3,\"217\":8,\"219\":1,\"221\":4,\"222\":4,\"223\":8,\"224\":1,\"226\":2,\"227\":5}}],[\"rsj\",{\"1\":{\"184\":1}}],[\"r=1\",{\"1\":{\"157\":1}}],[\"r=0\",{\"1\":{\"148\":12}}],[\"rbf\",{\"1\":{\"115\":1}}],[\"rug\",{\"1\":{\"226\":1}}],[\"rubik\",{\"1\":{\"213\":1}}],[\"rules\",{\"1\":{\"134\":2,\"197\":1,\"218\":1}}],[\"rule\",{\"0\":{\"93\":1,\"101\":1,\"139\":1},\"1\":{\"139\":2}}],[\"runtime\",{\"1\":{\"185\":1}}],[\"runs\",{\"1\":{\"49\":1}}],[\"running\",{\"1\":{\"49\":1,\"216\":2}}],[\"run\",{\"1\":{\"44\":1,\"198\":1}}],[\"rid\",{\"1\":{\"196\":1}}],[\"ridge\",{\"1\":{\"119\":1}}],[\"richer\",{\"1\":{\"89\":1}}],[\"right\",{\"1\":{\"34\":1,\"45\":1,\"168\":2,\"213\":1}}],[\"rightside\",{\"1\":{\"30\":1}}],[\"rnns\",{\"1\":{\"49\":9,\"59\":1,\"86\":1}}],[\"r+s\",{\"1\":{\"25\":1,\"39\":1}}],[\"r+γs\",{\"1\":{\"23\":1,\"25\":1,\"29\":1,\"30\":1,\"34\":1}}],[\"rπ​+γpπ​vk​\",{\"1\":{\"34\":1}}],[\"rπ​+γpπ​v\",{\"1\":{\"29\":1,\"33\":1}}],[\"rπ​\",{\"1\":{\"23\":2}}],[\"rπ​=\",{\"1\":{\"23\":1}}],[\"r​​+meanoffuturerewardγa∑​π\",{\"1\":{\"23\":1}}],[\"r∑​p\",{\"1\":{\"23\":2,\"25\":1,\"29\":1,\"30\":1,\"34\":1}}],[\"range\",{\"1\":{\"104\":2}}],[\"randint\",{\"1\":{\"104\":2}}],[\"randn\",{\"1\":{\"104\":1}}],[\"randomization\",{\"1\":{\"144\":2,\"150\":1}}],[\"randomly\",{\"1\":{\"104\":1,\"122\":1}}],[\"random\",{\"1\":{\"21\":1,\"43\":1,\"65\":1,\"66\":1,\"67\":1}}],[\"rateγ∈\",{\"1\":{\"17\":1}}],[\"rate\",{\"0\":{\"17\":1,\"83\":1},\"1\":{\"35\":1,\"81\":7,\"83\":3,\"110\":4}}],[\"rather\",{\"1\":{\"14\":1,\"48\":1,\"49\":1}}],[\"rt+1​∣st​=s\",{\"1\":{\"23\":1}}],[\"rt+1​∣st​\",{\"1\":{\"21\":1}}],[\"robotics\",{\"1\":{\"207\":1}}],[\"robots\",{\"1\":{\"184\":1}}],[\"robot\",{\"1\":{\"168\":2,\"206\":2,\"207\":4,\"213\":2}}],[\"robustness\",{\"1\":{\"120\":1}}],[\"road\",{\"1\":{\"154\":2,\"195\":1}}],[\"rooted\",{\"1\":{\"207\":1}}],[\"root\",{\"1\":{\"124\":1,\"198\":1,\"207\":1}}],[\"rounded\",{\"1\":{\"93\":1}}],[\"routing\",{\"1\":{\"69\":1}}],[\"route\",{\"1\":{\"7\":1,\"207\":1}}],[\"role\",{\"1\":{\"88\":1,\"222\":1}}],[\"roles\",{\"1\":{\"17\":1}}],[\"row\",{\"1\":{\"78\":1}}],[\"r∣s\",{\"1\":{\"13\":1,\"22\":1,\"23\":2,\"25\":2,\"29\":2,\"30\":1,\"34\":1,\"35\":1,\"39\":1}}],[\"r\",{\"1\":{\"13\":1,\"18\":2,\"21\":1,\"35\":1,\"82\":1,\"148\":1}}],[\"rl\",{\"1\":{\"7\":2,\"21\":1,\"24\":1,\"27\":1},\"2\":{\"6\":1,\"19\":1,\"26\":1,\"36\":1,\"42\":1,\"47\":1}}],[\"revealed\",{\"1\":{\"214\":1}}],[\"revisited\",{\"0\":{\"224\":1}}],[\"revision\",{\"0\":{\"21\":1},\"1\":{\"21\":1}}],[\"review\",{\"1\":{\"86\":1}}],[\"rerun\",{\"1\":{\"198\":1}}],[\"reports\",{\"1\":{\"218\":1}}],[\"repeating\",{\"1\":{\"223\":1}}],[\"repeat\",{\"1\":{\"223\":1}}],[\"repeatedly\",{\"1\":{\"207\":1}}],[\"repelem\",{\"1\":{\"160\":1}}],[\"replanning\",{\"1\":{\"198\":1}}],[\"replace\",{\"1\":{\"84\":1}}],[\"repmat\",{\"1\":{\"160\":1}}],[\"representing\",{\"1\":{\"188\":2,\"196\":1,\"227\":1}}],[\"represented\",{\"1\":{\"188\":1,\"195\":1,\"207\":1}}],[\"represents\",{\"1\":{\"22\":1,\"52\":3,\"53\":1,\"59\":1,\"124\":3,\"185\":1,\"195\":1,\"199\":1,\"207\":1,\"221\":3,\"223\":2,\"229\":1}}],[\"represent\",{\"1\":{\"11\":1,\"34\":1,\"87\":2,\"110\":2,\"207\":1,\"221\":1}}],[\"representations\",{\"1\":{\"89\":3,\"225\":1,\"226\":4}}],[\"representation\",{\"0\":{\"226\":1,\"227\":1},\"1\":{\"11\":2,\"49\":3,\"87\":2,\"89\":3,\"99\":1,\"227\":4,\"228\":1,\"229\":1}}],[\"re\",{\"1\":{\"82\":5,\"172\":1,\"196\":1}}],[\"retaining\",{\"1\":{\"226\":1}}],[\"retain\",{\"1\":{\"82\":3,\"84\":1}}],[\"returns\",{\"1\":{\"22\":1,\"44\":1}}],[\"return=0×γ+0×γ2+0×γ3+1×γ4+1×γ5\",{\"1\":{\"17\":1}}],[\"return=0+0+0+1=1\",{\"1\":{\"16\":1}}],[\"return\",{\"0\":{\"16\":1},\"1\":{\"16\":2,\"17\":3,\"22\":3,\"23\":1,\"25\":2,\"45\":2,\"61\":1,\"82\":1,\"104\":1,\"215\":2,\"217\":5,\"218\":1}}],[\"rewritten\",{\"1\":{\"72\":1}}],[\"rewardr\",{\"1\":{\"21\":1}}],[\"rewards\",{\"1\":{\"16\":1,\"17\":3,\"18\":1,\"22\":1}}],[\"reward\",{\"0\":{\"12\":1},\"1\":{\"8\":1,\"12\":6,\"13\":2,\"16\":1,\"21\":1,\"23\":5,\"25\":3,\"34\":1,\"35\":1}}],[\"regardless\",{\"1\":{\"223\":1}}],[\"regarded\",{\"1\":{\"33\":1,\"45\":1,\"198\":1}}],[\"regularization\",{\"0\":{\"119\":1},\"1\":{\"111\":1,\"119\":1}}],[\"regularity\",{\"1\":{\"72\":1}}],[\"regression\",{\"1\":{\"110\":2,\"123\":1,\"124\":1}}],[\"reducing\",{\"1\":{\"81\":1,\"83\":2,\"119\":1,\"203\":1}}],[\"reduction\",{\"0\":{\"73\":1},\"1\":{\"121\":1,\"124\":1}}],[\"reduces\",{\"1\":{\"144\":2,\"192\":1}}],[\"reduced\",{\"1\":{\"81\":1,\"220\":1}}],[\"reducelronplateau\",{\"1\":{\"81\":3}}],[\"reduce\",{\"1\":{\"65\":1,\"78\":1,\"81\":2,\"100\":1,\"121\":1}}],[\"redundant\",{\"1\":{\"60\":1,\"168\":1,\"223\":1}}],[\"required\",{\"1\":{\"46\":1,\"229\":1}}],[\"requires\",{\"1\":{\"46\":1,\"87\":1}}],[\"require\",{\"1\":{\"46\":1,\"82\":1,\"99\":1}}],[\"requirements\",{\"1\":{\"67\":1}}],[\"requirement\",{\"1\":{\"46\":1}}],[\"remove\",{\"1\":{\"226\":1}}],[\"removecap≺σ​insert\",{\"1\":{\"229\":1}}],[\"removecap\",{\"1\":{\"226\":1,\"229\":4}}],[\"removed\",{\"1\":{\"46\":1}}],[\"remaining\",{\"1\":{\"120\":1,\"223\":1}}],[\"remains\",{\"1\":{\"87\":1}}],[\"remarks\",{\"1\":{\"22\":1}}],[\"reliable\",{\"1\":{\"120\":1}}],[\"releases\",{\"1\":{\"82\":1}}],[\"relevant\",{\"1\":{\"48\":1,\"94\":1}}],[\"relu\",{\"1\":{\"49\":1,\"78\":4,\"93\":3,\"98\":1,\"101\":1,\"104\":1}}],[\"rely\",{\"1\":{\"43\":1,\"223\":1}}],[\"relation\",{\"1\":{\"196\":1,\"229\":1}}],[\"relational\",{\"1\":{\"86\":1}}],[\"relationships\",{\"1\":{\"86\":1,\"87\":1,\"203\":1}}],[\"relationship\",{\"1\":{\"22\":1,\"23\":1}}],[\"relative\",{\"1\":{\"54\":1}}],[\"related\",{\"1\":{\"5\":1}}],[\"references\",{\"1\":{\"229\":1}}],[\"reference\",{\"1\":{\"229\":1}}],[\"referred\",{\"1\":{\"206\":1}}],[\"refers\",{\"1\":{\"43\":1,\"87\":1,\"223\":1,\"226\":1}}],[\"refinement\",{\"1\":{\"207\":3}}],[\"refine\",{\"1\":{\"83\":2,\"207\":1,\"229\":1}}],[\"reflecting\",{\"1\":{\"81\":1}}],[\"reflect\",{\"1\":{\"23\":1,\"144\":1,\"146\":1}}],[\"reflects\",{\"1\":{\"8\":1,\"81\":1,\"144\":1}}],[\"recycled\",{\"1\":{\"222\":1}}],[\"recall\",{\"1\":{\"207\":1,\"226\":1,\"229\":1}}],[\"recurrence\",{\"1\":{\"168\":1,\"221\":1,\"222\":1}}],[\"recurrent\",{\"1\":{\"49\":4}}],[\"recurring\",{\"1\":{\"168\":4}}],[\"recursively\",{\"1\":{\"124\":1}}],[\"recognition\",{\"1\":{\"166\":1}}],[\"record\",{\"0\":{\"80\":1}}],[\"recommand\",{\"1\":{\"21\":1,\"23\":1,\"37\":1}}],[\"receives\",{\"1\":{\"49\":1}}],[\"receive\",{\"1\":{\"21\":1,\"222\":1}}],[\"resources\",{\"1\":{\"193\":1}}],[\"resolved\",{\"1\":{\"229\":1}}],[\"resolve\",{\"1\":{\"185\":1,\"215\":1,\"217\":3}}],[\"research\",{\"1\":{\"89\":1,\"226\":1}}],[\"results\",{\"0\":{\"201\":1},\"1\":{\"61\":2,\"203\":1,\"221\":1,\"223\":1}}],[\"result\",{\"1\":{\"43\":3,\"58\":1,\"87\":1,\"104\":1,\"111\":1,\"193\":1,\"226\":1}}],[\"resulting\",{\"1\":{\"18\":1,\"122\":1,\"124\":1,\"207\":1,\"223\":1}}],[\"respectively\",{\"1\":{\"198\":1,\"221\":1}}],[\"respective\",{\"1\":{\"185\":1}}],[\"respect\",{\"1\":{\"9\":1,\"82\":4,\"190\":1,\"196\":1}}],[\"reason\",{\"1\":{\"224\":1}}],[\"reasons\",{\"1\":{\"140\":1,\"144\":1}}],[\"rearranged\",{\"1\":{\"87\":1}}],[\"real\",{\"1\":{\"40\":1,\"44\":1,\"144\":1,\"167\":2,\"219\":1}}],[\"read\",{\"1\":{\"37\":1}}],[\"reading\",{\"0\":{\"5\":1,\"118\":1},\"1\":{\"21\":1}}],[\"reaching\",{\"1\":{\"144\":1}}],[\"reached\",{\"1\":{\"83\":1,\"223\":1}}],[\"reaches\",{\"1\":{\"18\":1,\"223\":1,\"226\":1}}],[\"reach\",{\"1\":{\"7\":1,\"216\":1,\"223\":1,\"226\":1}}],[\"reinforcement\",{\"0\":{\"7\":1},\"1\":{\"5\":1,\"7\":1,\"8\":1}}],[\"cσ​\",{\"1\":{\"229\":1}}],[\"c∗\",{\"1\":{\"223\":2}}],[\"c₅\",{\"1\":{\"222\":1}}],[\"c₄\",{\"1\":{\"222\":1}}],[\"c₃\",{\"1\":{\"222\":1}}],[\"c₂\",{\"1\":{\"222\":1}}],[\"c₁\",{\"1\":{\"222\":1}}],[\"c3∗​\",{\"1\":{\"222\":1}}],[\"ck−1∗​\",{\"1\":{\"222\":1}}],[\"ck∗​\",{\"1\":{\"222\":4}}],[\"c4∗​\",{\"1\":{\"222\":2}}],[\"cmpnn\",{\"1\":{\"203\":1}}],[\"cbs\",{\"0\":{\"198\":1,\"199\":2,\"200\":1},\"1\":{\"197\":1,\"199\":7,\"200\":6}}],[\"c大调mi音阶\",{\"1\":{\"178\":1}}],[\"circumstances\",{\"1\":{\"215\":1,\"221\":1}}],[\"cifar\",{\"1\":{\"88\":1}}],[\"cities\",{\"1\":{\"87\":1,\"203\":3}}],[\"city\",{\"1\":{\"69\":1,\"203\":2}}],[\"cnns\",{\"1\":{\"86\":1}}],[\"criterion\",{\"1\":{\"207\":1}}],[\"critically\",{\"1\":{\"229\":1}}],[\"critical\",{\"1\":{\"141\":2}}],[\"creation\",{\"1\":{\"167\":1}}],[\"creates\",{\"1\":{\"89\":1}}],[\"create\",{\"1\":{\"82\":3,\"104\":1,\"124\":1}}],[\"crowds\",{\"1\":{\"154\":1}}],[\"crosswalk\",{\"1\":{\"154\":1}}],[\"cross\",{\"0\":{\"120\":1},\"1\":{\"120\":2}}],[\"crucial\",{\"1\":{\"21\":1}}],[\"c−e\",{\"1\":{\"67\":1}}],[\"ct​\",{\"1\":{\"59\":6}}],[\"c=f\",{\"1\":{\"49\":1}}],[\"c\",{\"1\":{\"49\":3,\"67\":6,\"75\":1,\"197\":2,\"216\":3,\"221\":12,\"222\":8,\"223\":1,\"224\":9}}],[\"children\",{\"1\":{\"207\":2}}],[\"check\",{\"1\":{\"198\":2,\"218\":1}}],[\"choice\",{\"1\":{\"229\":1}}],[\"choices\",{\"1\":{\"100\":1}}],[\"chose\",{\"1\":{\"197\":1}}],[\"chosen\",{\"1\":{\"124\":1}}],[\"choose=zeros\",{\"1\":{\"160\":1}}],[\"choose\",{\"0\":{\"197\":1},\"1\":{\"38\":1,\"89\":2,\"161\":2,\"197\":2,\"198\":1}}],[\"challenging\",{\"1\":{\"226\":1}}],[\"challenge\",{\"0\":{\"71\":1,\"74\":1}}],[\"characterized\",{\"1\":{\"143\":1}}],[\"characteristic\",{\"1\":{\"46\":1}}],[\"changing\",{\"1\":{\"78\":1,\"223\":1}}],[\"changed\",{\"1\":{\"223\":1}}],[\"changes\",{\"1\":{\"83\":1,\"111\":1,\"134\":1,\"207\":1,\"223\":2}}],[\"change\",{\"1\":{\"46\":1,\"111\":1,\"122\":1,\"221\":1}}],[\"chain\",{\"1\":{\"16\":2,\"49\":1}}],[\"chapter\",{\"0\":{\"7\":1,\"20\":1,\"27\":1,\"43\":1,\"205\":1,\"210\":1},\"1\":{\"20\":1,\"24\":1,\"27\":1,\"37\":2,\"38\":1,\"43\":1,\"45\":1,\"208\":4}}],[\"clever\",{\"1\":{\"224\":1}}],[\"clear\",{\"1\":{\"140\":1,\"160\":1}}],[\"clearly\",{\"1\":{\"59\":1}}],[\"clearer\",{\"1\":{\"25\":1}}],[\"clc\",{\"1\":{\"160\":1}}],[\"clusters\",{\"1\":{\"122\":2}}],[\"clustering\",{\"1\":{\"122\":1}}],[\"cluster\",{\"0\":{\"116\":1},\"1\":{\"122\":1}}],[\"classical\",{\"1\":{\"216\":1}}],[\"classification\",{\"1\":{\"88\":2,\"91\":2,\"123\":1,\"124\":1}}],[\"classes\",{\"1\":{\"123\":1}}],[\"class\",{\"1\":{\"49\":1,\"61\":1,\"86\":1,\"104\":1,\"124\":1,\"224\":1}}],[\"click\",{\"1\":{\"49\":1}}],[\"closer\",{\"1\":{\"229\":1}}],[\"closest\",{\"1\":{\"123\":1}}],[\"closed\",{\"1\":{\"24\":2}}],[\"close\",{\"1\":{\"17\":2,\"140\":1,\"207\":1}}],[\"cube\",{\"1\":{\"213\":1}}],[\"cumbersome\",{\"1\":{\"168\":2}}],[\"cumulative\",{\"1\":{\"22\":1}}],[\"cutting\",{\"0\":{\"165\":1}}],[\"currently\",{\"1\":{\"52\":1,\"229\":1}}],[\"current\",{\"1\":{\"12\":1,\"14\":1,\"21\":1,\"23\":2,\"39\":2,\"58\":1,\"59\":5,\"111\":1,\"134\":1,\"212\":1,\"216\":2,\"221\":1,\"223\":1,\"224\":1}}],[\"cambridge\",{\"1\":{\"205\":1}}],[\"cat\",{\"1\":{\"160\":2}}],[\"ca\",{\"1\":{\"127\":1,\"128\":1,\"132\":1,\"153\":2,\"154\":1}}],[\"causal\",{\"1\":{\"229\":3}}],[\"caused\",{\"1\":{\"111\":1}}],[\"causing\",{\"1\":{\"49\":1}}],[\"cap\",{\"1\":{\"226\":16,\"227\":1,\"229\":2}}],[\"capacity\",{\"1\":{\"141\":1}}],[\"capable\",{\"1\":{\"99\":1,\"166\":1}}],[\"capture\",{\"1\":{\"49\":1,\"78\":1}}],[\"cases\",{\"1\":{\"200\":1}}],[\"case\",{\"1\":{\"82\":1,\"192\":2,\"216\":1,\"217\":2,\"218\":3,\"222\":2,\"224\":1}}],[\"carlo\",{\"0\":{\"43\":1,\"44\":1,\"45\":1,\"46\":1},\"1\":{\"43\":1}}],[\"cares\",{\"1\":{\"199\":1}}],[\"care\",{\"1\":{\"25\":1,\"45\":1,\"227\":1}}],[\"calling\",{\"1\":{\"82\":1}}],[\"called\",{\"1\":{\"18\":2,\"24\":1,\"46\":1,\"48\":1,\"49\":1,\"58\":1,\"121\":1,\"185\":1,\"194\":1,\"197\":1,\"216\":1,\"221\":1,\"226\":3}}],[\"calculat\",{\"1\":{\"49\":1}}],[\"calculating\",{\"1\":{\"44\":1,\"53\":1,\"82\":1,\"121\":1}}],[\"calculation\",{\"0\":{\"58\":1},\"1\":{\"39\":1,\"61\":1,\"104\":1}}],[\"calculates\",{\"1\":{\"223\":1}}],[\"calculated\",{\"1\":{\"29\":1,\"49\":1}}],[\"calculate\",{\"1\":{\"21\":1,\"35\":1,\"38\":2,\"39\":1,\"43\":2,\"45\":1,\"58\":2,\"60\":1,\"104\":1,\"221\":1,\"222\":1}}],[\"candidate\",{\"1\":{\"224\":1}}],[\"cannot\",{\"1\":{\"44\":1,\"46\":1,\"144\":1,\"216\":2}}],[\"can\",{\"1\":{\"5\":1,\"7\":3,\"16\":2,\"18\":3,\"21\":1,\"22\":2,\"23\":3,\"24\":2,\"25\":4,\"29\":1,\"32\":1,\"33\":3,\"38\":2,\"39\":1,\"43\":2,\"45\":6,\"46\":7,\"49\":4,\"59\":3,\"61\":2,\"78\":2,\"82\":1,\"83\":1,\"87\":3,\"88\":1,\"89\":2,\"104\":1,\"111\":1,\"121\":1,\"123\":1,\"134\":1,\"140\":1,\"143\":1,\"168\":1,\"188\":1,\"190\":1,\"191\":1,\"193\":1,\"195\":2,\"196\":1,\"197\":1,\"199\":1,\"200\":1,\"206\":1,\"207\":8,\"213\":1,\"215\":3,\"219\":1,\"220\":1,\"221\":8,\"222\":2,\"223\":5,\"224\":1,\"226\":4,\"227\":8,\"229\":1}}],[\"covering\",{\"1\":{\"224\":1}}],[\"covariance\",{\"1\":{\"121\":1}}],[\"copyright\",{\"1\":{\"205\":1}}],[\"copying\",{\"1\":{\"198\":1}}],[\"coordinating\",{\"1\":{\"185\":1}}],[\"coordination\",{\"0\":{\"184\":1},\"1\":{\"185\":2}}],[\"coordinates\",{\"1\":{\"121\":1,\"143\":1,\"154\":2,\"213\":1}}],[\"coordinate\",{\"1\":{\"121\":1,\"168\":2,\"213\":1}}],[\"cooperative\",{\"0\":{\"184\":1,\"192\":1,\"199\":1},\"1\":{\"185\":1,\"192\":1,\"200\":1}}],[\"collision\",{\"1\":{\"144\":1,\"185\":3,\"189\":1,\"207\":1}}],[\"collection\",{\"1\":{\"207\":1}}],[\"collect\",{\"1\":{\"45\":1}}],[\"collected\",{\"1\":{\"16\":1,\"45\":1}}],[\"color=\",{\"1\":{\"104\":3}}],[\"coefficient\",{\"1\":{\"96\":1}}],[\"coefficients\",{\"0\":{\"97\":1},\"1\":{\"95\":1,\"97\":1,\"98\":1}}],[\"costly\",{\"1\":{\"229\":1}}],[\"costs\",{\"0\":{\"189\":1},\"1\":{\"189\":1,\"190\":2,\"197\":1,\"223\":1,\"224\":1}}],[\"cost\",{\"1\":{\"69\":1,\"81\":6,\"83\":2,\"189\":1,\"190\":2,\"191\":2,\"197\":8,\"198\":1,\"199\":1,\"216\":3,\"219\":4,\"220\":2,\"221\":2,\"222\":3,\"223\":24,\"224\":10}}],[\"codes\",{\"1\":{\"104\":1}}],[\"code\",{\"0\":{\"61\":1,\"104\":1},\"1\":{\"61\":3}}],[\"coherent\",{\"1\":{\"49\":1}}],[\"corresponds\",{\"1\":{\"227\":2}}],[\"corresponding\",{\"1\":{\"24\":1,\"25\":2,\"44\":1,\"45\":1,\"78\":1,\"198\":1,\"213\":1,\"226\":2}}],[\"correcting\",{\"1\":{\"224\":3}}],[\"core\",{\"1\":{\"48\":1}}],[\"coin\",{\"1\":{\"43\":2}}],[\"come\",{\"1\":{\"121\":1,\"216\":1,\"220\":1,\"222\":2,\"223\":3,\"224\":6}}],[\"comes\",{\"1\":{\"28\":1}}],[\"combine\",{\"1\":{\"101\":1}}],[\"combines\",{\"1\":{\"100\":1,\"223\":1}}],[\"combined\",{\"1\":{\"59\":1,\"89\":1}}],[\"combinations\",{\"1\":{\"221\":1,\"226\":2}}],[\"combination\",{\"1\":{\"41\":1,\"203\":1}}],[\"commonly\",{\"1\":{\"124\":1}}],[\"common\",{\"1\":{\"53\":1,\"100\":1,\"120\":1,\"190\":1,\"192\":1,\"226\":1}}],[\"competitive\",{\"1\":{\"229\":1}}],[\"compressed\",{\"1\":{\"227\":1}}],[\"comprise\",{\"1\":{\"49\":1}}],[\"comprised\",{\"1\":{\"21\":1}}],[\"completion\",{\"1\":{\"229\":1}}],[\"completeness\",{\"1\":{\"185\":1}}],[\"complete\",{\"1\":{\"134\":1}}],[\"complementary\",{\"1\":{\"227\":2}}],[\"complexity\",{\"1\":{\"100\":1,\"119\":1}}],[\"complex\",{\"1\":{\"89\":1,\"167\":1}}],[\"compact\",{\"1\":{\"71\":1,\"122\":1}}],[\"comparable\",{\"1\":{\"196\":1}}],[\"comparing\",{\"1\":{\"196\":1,\"199\":1}}],[\"comparison\",{\"1\":{\"40\":3,\"229\":2}}],[\"compared\",{\"1\":{\"78\":1}}],[\"compares\",{\"1\":{\"52\":1,\"199\":1}}],[\"compare\",{\"1\":{\"45\":1,\"193\":1,\"199\":5,\"222\":1}}],[\"computing\",{\"1\":{\"134\":2,\"224\":2}}],[\"computational\",{\"1\":{\"100\":1}}],[\"computation\",{\"1\":{\"71\":1,\"82\":3,\"87\":1}}],[\"computer\",{\"1\":{\"166\":1,\"225\":1}}],[\"computes\",{\"1\":{\"95\":1,\"222\":1}}],[\"computed\",{\"1\":{\"52\":1,\"58\":1,\"82\":2,\"93\":1,\"96\":1,\"98\":1,\"101\":1,\"227\":1}}],[\"compute\",{\"1\":{\"43\":1,\"53\":1,\"55\":1,\"59\":1,\"82\":5,\"93\":1,\"98\":2,\"203\":1,\"220\":1}}],[\"compositional\",{\"1\":{\"203\":1}}],[\"composed\",{\"1\":{\"25\":1}}],[\"component\",{\"1\":{\"69\":1,\"110\":1,\"121\":1,\"199\":1}}],[\"components\",{\"0\":{\"50\":1,\"51\":1,\"92\":1,\"95\":1,\"100\":1},\"1\":{\"21\":2,\"59\":1,\"103\":1,\"121\":3,\"219\":1}}],[\"countably\",{\"1\":{\"207\":1}}],[\"could\",{\"1\":{\"28\":1,\"89\":3,\"218\":2,\"224\":2,\"226\":1,\"227\":2,\"229\":1}}],[\"course\",{\"1\":{\"18\":1,\"78\":1,\"215\":1}}],[\"conflict\",{\"0\":{\"198\":1},\"1\":{\"189\":2,\"193\":1}}],[\"conflicts\",{\"1\":{\"189\":1,\"198\":4,\"229\":1}}],[\"conflicting\",{\"1\":{\"154\":1,\"194\":1}}],[\"conference\",{\"1\":{\"184\":1}}],[\"conway于\",{\"1\":{\"133\":1}}],[\"conatins\",{\"1\":{\"104\":1}}],[\"connects\",{\"1\":{\"198\":1}}],[\"connections\",{\"1\":{\"92\":1}}],[\"connectivity\",{\"1\":{\"88\":1}}],[\"connected\",{\"1\":{\"53\":1,\"59\":1,\"78\":2,\"89\":1,\"185\":1,\"207\":1}}],[\"convolutional\",{\"1\":{\"91\":2}}],[\"convolution\",{\"1\":{\"78\":5}}],[\"convenience\",{\"1\":{\"213\":1,\"219\":1,\"227\":1}}],[\"conveyor\",{\"1\":{\"49\":1}}],[\"converge\",{\"1\":{\"83\":1,\"109\":1,\"110\":2}}],[\"convergent\",{\"1\":{\"44\":2}}],[\"convergence\",{\"1\":{\"41\":1,\"44\":1,\"203\":1}}],[\"convert\",{\"1\":{\"227\":1}}],[\"converts\",{\"1\":{\"200\":1}}],[\"converted\",{\"1\":{\"78\":1,\"221\":1}}],[\"converting\",{\"0\":{\"227\":1},\"1\":{\"18\":1,\"227\":2}}],[\"concrete\",{\"1\":{\"227\":1}}],[\"conclusion\",{\"0\":{\"201\":1}}],[\"conclude\",{\"1\":{\"199\":1}}],[\"concerning\",{\"1\":{\"185\":1}}],[\"concept\",{\"1\":{\"8\":1}}],[\"concepts\",{\"0\":{\"7\":1,\"8\":1,\"15\":1},\"1\":{\"8\":2,\"20\":1,\"21\":2}}],[\"concatenate\",{\"1\":{\"89\":1}}],[\"concatenation\",{\"1\":{\"59\":1,\"96\":1,\"101\":1}}],[\"conditioning\",{\"1\":{\"89\":1}}],[\"conditions\",{\"1\":{\"72\":1,\"220\":1}}],[\"condition\",{\"1\":{\"22\":1,\"46\":1,\"218\":1}}],[\"conditional\",{\"1\":{\"22\":1}}],[\"contain\",{\"1\":{\"198\":1,\"221\":1,\"229\":1}}],[\"containing\",{\"1\":{\"82\":1}}],[\"contains\",{\"1\":{\"29\":1,\"58\":1}}],[\"contribute\",{\"1\":{\"215\":1}}],[\"contributes\",{\"1\":{\"82\":1}}],[\"contributions\",{\"1\":{\"95\":1}}],[\"controller\",{\"1\":{\"206\":1}}],[\"control\",{\"0\":{\"63\":1,\"68\":1},\"1\":{\"67\":1,\"74\":2,\"203\":1}}],[\"contraction\",{\"0\":{\"33\":1},\"1\":{\"32\":3,\"33\":3,\"38\":1}}],[\"content\",{\"1\":{\"49\":1,\"52\":1,\"57\":1}}],[\"contextually\",{\"1\":{\"49\":1}}],[\"context\",{\"1\":{\"21\":1,\"49\":1,\"58\":2,\"59\":6,\"87\":1,\"88\":1,\"89\":2,\"223\":1,\"227\":1}}],[\"continuous\",{\"1\":{\"124\":1,\"207\":2,\"224\":1}}],[\"continue\",{\"1\":{\"40\":1,\"83\":1}}],[\"continuing\",{\"1\":{\"18\":2}}],[\"cons\",{\"1\":{\"225\":1}}],[\"consumed\",{\"1\":{\"219\":1}}],[\"constructed\",{\"1\":{\"229\":2}}],[\"constructs\",{\"1\":{\"216\":1}}],[\"construct\",{\"1\":{\"198\":1,\"229\":1}}],[\"constructing\",{\"1\":{\"89\":1,\"221\":1,\"225\":1}}],[\"constraint\",{\"1\":{\"198\":2,\"208\":1,\"223\":1,\"229\":2}}],[\"constraints\",{\"1\":{\"198\":3,\"207\":1,\"229\":2}}],[\"constrain\",{\"1\":{\"119\":1}}],[\"constant\",{\"1\":{\"67\":1,\"144\":2}}],[\"consistency\",{\"1\":{\"46\":1}}],[\"consistent\",{\"1\":{\"46\":3,\"227\":1}}],[\"consists\",{\"1\":{\"23\":1,\"87\":1,\"189\":1,\"190\":1,\"226\":1}}],[\"considerations\",{\"1\":{\"207\":1}}],[\"considering\",{\"1\":{\"198\":1,\"199\":1}}],[\"considered\",{\"1\":{\"45\":1,\"215\":1,\"226\":1,\"229\":1}}],[\"consider\",{\"1\":{\"25\":1,\"146\":1,\"227\":1}}],[\"considers\",{\"1\":{\"18\":1,\"207\":1}}],[\"consequent\",{\"1\":{\"18\":1}}],[\"centralized\",{\"1\":{\"185\":1}}],[\"centroid\",{\"1\":{\"122\":1}}],[\"centroids\",{\"1\":{\"122\":3}}],[\"certain\",{\"1\":{\"7\":1,\"10\":1,\"11\":1,\"78\":1,\"218\":1,\"226\":1}}],[\"celluar\",{\"1\":{\"154\":1}}],[\"cellular\",{\"0\":{\"126\":1},\"1\":{\"127\":1,\"134\":2,\"138\":1}}],[\"cells\",{\"1\":{\"143\":1,\"144\":2}}],[\"cell\",{\"1\":{\"7\":1,\"49\":5,\"134\":7,\"143\":1,\"144\":1,\"158\":1}}],[\"csuer\",{\"1\":{\"3\":1}}],[\"w=\",{\"1\":{\"98\":1}}],[\"w∈rf\",{\"1\":{\"96\":1}}],[\"wq\",{\"1\":{\"61\":1}}],[\"wv\",{\"1\":{\"61\":3}}],[\"wk\",{\"1\":{\"61\":2}}],[\"would\",{\"1\":{\"82\":1,\"88\":1,\"223\":1,\"224\":1,\"225\":1,\"226\":1,\"227\":1}}],[\"wo​\",{\"1\":{\"59\":1}}],[\"worry\",{\"1\":{\"224\":1}}],[\"work\",{\"1\":{\"217\":1,\"224\":2,\"228\":1}}],[\"workflow\",{\"1\":{\"199\":1}}],[\"workspace\",{\"0\":{\"188\":1}}],[\"works\",{\"1\":{\"89\":1,\"119\":1,\"122\":1,\"124\":1,\"192\":1,\"199\":1,\"229\":1}}],[\"working\",{\"0\":{\"56\":1},\"1\":{\"49\":1}}],[\"word\",{\"1\":{\"52\":1,\"59\":2,\"88\":1}}],[\"words\",{\"1\":{\"41\":1,\"191\":1,\"226\":2,\"229\":1}}],[\"worsening\",{\"1\":{\"199\":1}}],[\"worsen\",{\"1\":{\"193\":1}}],[\"worse\",{\"1\":{\"24\":1,\"191\":1,\"194\":1,\"195\":2,\"196\":1}}],[\"world\",{\"1\":{\"7\":2,\"144\":1,\"167\":1,\"207\":1,\"226\":1,\"227\":1}}],[\"w\",{\"1\":{\"53\":1,\"82\":1,\"93\":3,\"101\":2,\"104\":3,\"114\":1,\"115\":1}}],[\"wxt​\",{\"1\":{\"49\":1}}],[\"written\",{\"1\":{\"29\":1,\"221\":1,\"226\":1}}],[\"write\",{\"1\":{\"23\":1,\"33\":1,\"104\":1}}],[\"wave会造成以下危害\",{\"1\":{\"140\":1}}],[\"wave沿车流末端传播的速度\",{\"1\":{\"140\":1}}],[\"wave\",{\"1\":{\"140\":2,\"148\":2}}],[\"wait\",{\"1\":{\"45\":1}}],[\"want\",{\"1\":{\"25\":1,\"48\":1,\"61\":1,\"82\":1,\"88\":3,\"89\":1,\"223\":1,\"226\":1}}],[\"wants\",{\"1\":{\"12\":1}}],[\"ways\",{\"0\":{\"24\":1},\"1\":{\"207\":1,\"226\":1,\"227\":1}}],[\"way\",{\"1\":{\"18\":1,\"30\":1,\"45\":1,\"53\":1,\"89\":1,\"193\":1,\"199\":1,\"221\":1,\"223\":1,\"224\":1}}],[\"wh1​∥wh3​\",{\"1\":{\"98\":1}}],[\"wh1​∥wh2​\",{\"1\":{\"98\":1}}],[\"whi​∥whj​\",{\"1\":{\"96\":1}}],[\"while\",{\"1\":{\"25\":1,\"40\":1,\"121\":1,\"196\":1,\"200\":1,\"207\":1,\"215\":1,\"217\":2,\"219\":1,\"222\":1,\"224\":1,\"229\":1}}],[\"which\",{\"1\":{\"12\":1,\"21\":1,\"25\":1,\"39\":2,\"43\":1,\"44\":1,\"45\":1,\"52\":1,\"55\":1,\"58\":2,\"82\":3,\"88\":1,\"89\":4,\"99\":1,\"103\":1,\"111\":1,\"119\":1,\"123\":2,\"134\":1,\"143\":1,\"166\":1,\"168\":2,\"185\":2,\"189\":1,\"194\":2,\"196\":1,\"198\":4,\"203\":4,\"213\":5,\"215\":2,\"216\":1,\"218\":2,\"219\":2,\"220\":1,\"221\":3,\"222\":3,\"223\":4,\"224\":1,\"226\":11,\"227\":2,\"229\":7}}],[\"why\",{\"0\":{\"196\":1},\"1\":{\"25\":1,\"44\":1,\"199\":1,\"224\":1}}],[\"wheel\",{\"1\":{\"207\":1}}],[\"whether\",{\"1\":{\"35\":1,\"89\":1,\"215\":2,\"218\":1,\"226\":2}}],[\"where\",{\"1\":{\"23\":2,\"53\":2,\"59\":2,\"67\":1,\"83\":1,\"87\":1,\"88\":3,\"89\":1,\"92\":4,\"95\":2,\"100\":2,\"120\":1,\"123\":1,\"124\":1,\"188\":1,\"190\":1,\"192\":2,\"193\":1,\"194\":1,\"195\":2,\"198\":1,\"199\":3,\"200\":1,\"223\":3}}],[\"whenever\",{\"1\":{\"227\":1}}],[\"when\",{\"0\":{\"197\":1},\"1\":{\"18\":2,\"23\":1,\"44\":1,\"45\":1,\"46\":1,\"81\":1,\"82\":3,\"83\":1,\"87\":2,\"89\":2,\"185\":1,\"199\":1,\"207\":1,\"212\":1,\"217\":2,\"223\":1,\"227\":1}}],[\"what\",{\"0\":{\"119\":1,\"120\":1,\"206\":1},\"1\":{\"10\":1,\"11\":1,\"21\":1,\"22\":1,\"39\":1,\"48\":1,\"49\":1,\"52\":1,\"88\":1}}],[\"whole\",{\"1\":{\"89\":1}}],[\"who\",{\"1\":{\"3\":1}}],[\"wise\",{\"0\":{\"93\":1,\"101\":1}}],[\"window\",{\"1\":{\"78\":1}}],[\"windylab\",{\"1\":{\"5\":1}}],[\"widely\",{\"1\":{\"49\":1,\"91\":1}}],[\"wikipedia\",{\"1\":{\"49\":1}}],[\"will\",{\"1\":{\"8\":1,\"10\":1,\"12\":1,\"18\":1,\"20\":1,\"21\":2,\"23\":2,\"24\":1,\"25\":2,\"27\":1,\"28\":1,\"30\":2,\"37\":2,\"43\":1,\"45\":2,\"46\":1,\"48\":1,\"81\":2,\"88\":1,\"110\":3,\"190\":1,\"200\":1,\"213\":1,\"215\":1,\"216\":4,\"218\":1,\"219\":1,\"220\":1,\"221\":3,\"222\":1,\"223\":2,\"226\":1}}],[\"within\",{\"1\":{\"122\":1,\"190\":1}}],[\"without\",{\"1\":{\"82\":1,\"83\":1,\"119\":1,\"140\":1,\"185\":1,\"191\":2,\"194\":1,\"196\":1,\"198\":1,\"199\":1,\"223\":1,\"224\":1,\"229\":2}}],[\"with\",{\"0\":{\"203\":1},\"1\":{\"8\":1,\"9\":1,\"18\":3,\"21\":1,\"22\":1,\"23\":1,\"29\":1,\"30\":2,\"38\":1,\"45\":1,\"46\":4,\"49\":2,\"58\":1,\"59\":1,\"61\":3,\"67\":2,\"78\":2,\"82\":4,\"84\":1,\"87\":1,\"88\":4,\"89\":1,\"103\":1,\"104\":2,\"144\":2,\"146\":1,\"147\":1,\"154\":1,\"185\":2,\"190\":1,\"191\":1,\"196\":1,\"197\":2,\"198\":3,\"199\":1,\"203\":1,\"212\":1,\"214\":1,\"216\":3,\"217\":1,\"218\":1,\"223\":2,\"224\":1,\"227\":2,\"229\":1}}],[\"weave\",{\"1\":{\"89\":2}}],[\"weigths\",{\"1\":{\"60\":1}}],[\"weight\",{\"1\":{\"93\":1,\"96\":2,\"101\":1,\"200\":1}}],[\"weighted\",{\"0\":{\"55\":1},\"1\":{\"55\":1,\"58\":2,\"60\":1,\"98\":1,\"197\":1}}],[\"weights⋅v\",{\"1\":{\"55\":1}}],[\"weights=softmax\",{\"1\":{\"54\":1}}],[\"weights\",{\"1\":{\"48\":1,\"54\":2,\"55\":1,\"60\":1,\"61\":4,\"197\":1,\"221\":1}}],[\"weigh\",{\"1\":{\"48\":1,\"94\":1,\"95\":1}}],[\"well\",{\"1\":{\"49\":2,\"122\":1,\"221\":1}}],[\"we\",{\"1\":{\"7\":1,\"8\":1,\"11\":2,\"18\":1,\"20\":1,\"21\":3,\"23\":4,\"24\":3,\"25\":3,\"27\":3,\"28\":2,\"29\":2,\"30\":5,\"32\":1,\"33\":1,\"37\":2,\"38\":3,\"39\":4,\"43\":7,\"44\":3,\"45\":6,\"46\":9,\"48\":2,\"49\":3,\"57\":1,\"60\":1,\"61\":2,\"84\":2,\"87\":3,\"88\":5,\"89\":9,\"104\":4,\"121\":1,\"185\":2,\"193\":1,\"199\":3,\"216\":2,\"221\":2,\"222\":5,\"223\":11,\"224\":1,\"225\":1,\"226\":4,\"227\":2}}],[\"westlake\",{\"1\":{\"5\":1}}],[\"ld−c​+c3∗​\",{\"1\":{\"222\":1}}],[\"lda\",{\"1\":{\"112\":1,\"117\":5}}],[\"lb−c​+c3∗​\",{\"1\":{\"222\":1}}],[\"lc−c​=∞\",{\"1\":{\"222\":1}}],[\"lf​\",{\"1\":{\"219\":3,\"222\":1}}],[\"l=zeros\",{\"1\":{\"160\":1}}],[\"l1\",{\"1\":{\"119\":2}}],[\"l2\",{\"1\":{\"115\":1,\"119\":2}}],[\"l+1\",{\"1\":{\"93\":3}}],[\"lrscheduler\",{\"1\":{\"81\":1}}],[\"lr\",{\"1\":{\"81\":2,\"115\":5}}],[\"l\",{\"1\":{\"75\":1,\"93\":5,\"161\":1,\"219\":5,\"221\":1,\"223\":4,\"224\":2,\"229\":3}}],[\"luong\",{\"1\":{\"53\":2}}],[\"lstms\",{\"1\":{\"49\":1}}],[\"lstm\",{\"1\":{\"49\":6,\"59\":1,\"100\":1,\"101\":1}}],[\"llustration\",{\"1\":{\"41\":1}}],[\"latter\",{\"1\":{\"222\":1}}],[\"la−c​\",{\"1\":{\"222\":1}}],[\"lavalle\",{\"1\":{\"205\":2}}],[\"la\",{\"1\":{\"172\":1}}],[\"language\",{\"1\":{\"166\":1}}],[\"lane\",{\"0\":{\"145\":1},\"1\":{\"143\":1,\"147\":1}}],[\"lashlight\",{\"1\":{\"226\":3}}],[\"lasso\",{\"1\":{\"119\":1}}],[\"last\",{\"1\":{\"37\":1,\"38\":1,\"45\":1,\"216\":1}}],[\"labels=true\",{\"1\":{\"104\":1}}],[\"label\",{\"1\":{\"88\":2,\"124\":1,\"224\":3}}],[\"layout\",{\"0\":{\"64\":1},\"1\":{\"104\":1,\"154\":1}}],[\"layers\",{\"1\":{\"102\":1}}],[\"layer\",{\"0\":{\"93\":1,\"101\":1},\"1\":{\"49\":8,\"53\":1,\"59\":1,\"61\":2,\"78\":3,\"88\":1,\"89\":2,\"93\":3,\"101\":1}}],[\"larger\",{\"1\":{\"144\":1,\"203\":1,\"207\":2}}],[\"large\",{\"1\":{\"43\":1,\"46\":3,\"99\":2,\"110\":1,\"217\":1}}],[\"largest\",{\"1\":{\"30\":1,\"38\":2,\"46\":1}}],[\"law\",{\"1\":{\"43\":1,\"154\":1}}],[\"let\",{\"1\":{\"199\":1,\"213\":3,\"219\":1,\"221\":2}}],[\"lexicographically\",{\"1\":{\"199\":4}}],[\"lexicographic\",{\"1\":{\"199\":8}}],[\"levels\",{\"1\":{\"88\":1}}],[\"level\",{\"1\":{\"69\":2,\"88\":6,\"89\":1,\"198\":2,\"199\":1,\"203\":3}}],[\"leveraging\",{\"1\":{\"49\":1,\"185\":1}}],[\"leverages\",{\"1\":{\"203\":1}}],[\"leverage\",{\"1\":{\"43\":1}}],[\"leveraged\",{\"1\":{\"24\":1,\"45\":1}}],[\"len\",{\"1\":{\"61\":3,\"104\":2}}],[\"lengths\",{\"0\":{\"223\":1},\"1\":{\"223\":2}}],[\"length\",{\"0\":{\"220\":1},\"1\":{\"46\":1,\"57\":1,\"75\":1,\"82\":1,\"143\":2,\"146\":1,\"203\":3,\"219\":2,\"223\":11,\"227\":1}}],[\"left\",{\"1\":{\"34\":1,\"168\":2,\"213\":1,\"221\":1,\"229\":1}}],[\"lectures\",{\"1\":{\"25\":1}}],[\"less\",{\"1\":{\"24\":1,\"45\":1,\"224\":1}}],[\"least\",{\"1\":{\"191\":1,\"195\":2,\"196\":1,\"223\":1,\"224\":1}}],[\"leaf\",{\"1\":{\"124\":2}}],[\"leading\",{\"1\":{\"119\":1,\"143\":1,\"223\":1,\"227\":1}}],[\"leads\",{\"1\":{\"24\":1,\"194\":1,\"203\":1,\"207\":1,\"223\":1}}],[\"leakyrelu\",{\"1\":{\"96\":1}}],[\"learned\",{\"1\":{\"89\":2}}],[\"learnable\",{\"1\":{\"59\":1}}],[\"learns\",{\"1\":{\"48\":1}}],[\"learn\",{\"1\":{\"21\":1,\"23\":1,\"29\":1,\"49\":1,\"223\":1}}],[\"learning\",{\"0\":{\"7\":1,\"43\":1,\"80\":1,\"83\":1,\"203\":1},\"1\":{\"5\":1,\"7\":1,\"8\":2,\"21\":1,\"46\":1,\"48\":1,\"49\":1,\"81\":7,\"83\":3,\"86\":1,\"99\":1,\"110\":4,\"111\":5,\"119\":1,\"120\":1,\"123\":1,\"124\":1,\"166\":3,\"203\":1}}],[\"leave\",{\"1\":{\"18\":2}}],[\"low\",{\"1\":{\"198\":1}}],[\"lowest\",{\"1\":{\"197\":1,\"223\":2}}],[\"lower\",{\"1\":{\"69\":1,\"111\":1,\"191\":2,\"203\":1}}],[\"locationvid​∈v\",{\"1\":{\"189\":1}}],[\"locationvio​∈v\",{\"1\":{\"189\":1}}],[\"locations\",{\"1\":{\"185\":1,\"188\":1,\"207\":1}}],[\"local\",{\"1\":{\"83\":1,\"91\":1,\"99\":1}}],[\"loss\",{\"1\":{\"81\":2,\"82\":6,\"115\":1,\"119\":1,\"203\":2,\"219\":1}}],[\"logically\",{\"1\":{\"226\":1}}],[\"logic\",{\"0\":{\"225\":1,\"228\":1},\"1\":{\"225\":1,\"226\":1,\"227\":1,\"228\":1}}],[\"logistic\",{\"1\":{\"110\":1,\"115\":2}}],[\"logits=wo​\",{\"1\":{\"59\":1}}],[\"logits\",{\"1\":{\"59\":4}}],[\"log\",{\"0\":{\"106\":1},\"1\":{\"71\":1,\"203\":2}}],[\"loops\",{\"1\":{\"92\":1}}],[\"loop\",{\"1\":{\"49\":1}}],[\"looking\",{\"1\":{\"23\":1}}],[\"longest\",{\"1\":{\"203\":1}}],[\"longer\",{\"1\":{\"122\":1,\"223\":2}}],[\"long\",{\"1\":{\"46\":1,\"49\":2,\"223\":2}}],[\"lol\",{\"1\":{\"3\":2}}],[\"li​\",{\"1\":{\"222\":4}}],[\"lij\",{\"1\":{\"161\":1}}],[\"li\",{\"1\":{\"158\":1,\"161\":1}}],[\"limited\",{\"1\":{\"193\":1}}],[\"limit\",{\"1\":{\"143\":1,\"144\":1,\"207\":1,\"223\":1}}],[\"lie\",{\"1\":{\"121\":1}}],[\"lightblue\",{\"1\":{\"104\":1}}],[\"list\",{\"1\":{\"87\":1,\"104\":1}}],[\"linearly\",{\"1\":{\"123\":1}}],[\"linear\",{\"1\":{\"49\":1,\"59\":1,\"61\":5,\"89\":2,\"110\":1,\"123\":1,\"227\":1}}],[\"line\",{\"1\":{\"49\":1,\"215\":3,\"224\":2}}],[\"links\",{\"1\":{\"229\":1}}],[\"link\",{\"1\":{\"5\":1,\"49\":1,\"91\":1,\"229\":2}}],[\"literals\",{\"1\":{\"226\":7,\"227\":2}}],[\"literal\",{\"1\":{\"23\":1,\"226\":6,\"227\":5,\"229\":2}}],[\"likewise\",{\"1\":{\"222\":1}}],[\"like\",{\"0\":{\"226\":1},\"1\":{\"22\":1,\"25\":1,\"30\":1,\"32\":1,\"33\":1,\"43\":1,\"44\":1,\"49\":3,\"81\":1,\"82\":2,\"84\":1,\"89\":1,\"91\":1,\"124\":1,\"144\":1,\"146\":1,\"199\":1,\"219\":1,\"221\":1,\"223\":1,\"225\":1,\"226\":4,\"227\":1}}],[\"f−1\",{\"1\":{\"223\":4}}],[\"f=k+1\",{\"1\":{\"219\":1}}],[\"f=1\",{\"1\":{\"161\":1}}],[\"f^\",{\"1\":{\"217\":2}}],[\"fd\",{\"1\":{\"148\":1}}],[\"ffn\",{\"1\":{\"49\":1}}],[\"feedback\",{\"1\":{\"207\":1}}],[\"feeding\",{\"1\":{\"59\":1}}],[\"feasiblity\",{\"1\":{\"207\":1}}],[\"feasible\",{\"0\":{\"211\":1,\"214\":1},\"1\":{\"191\":1}}],[\"features\",{\"1\":{\"52\":1,\"78\":2,\"87\":1,\"91\":1,\"92\":1,\"93\":3,\"98\":3,\"99\":1,\"100\":1,\"101\":2,\"104\":1}}],[\"feature\",{\"0\":{\"98\":1},\"1\":{\"46\":1,\"49\":1,\"87\":3,\"88\":1,\"89\":1,\"92\":2,\"94\":1,\"95\":2,\"96\":2,\"98\":2,\"100\":2,\"104\":24,\"110\":1,\"124\":3,\"224\":1}}],[\"fed\",{\"1\":{\"49\":1}}],[\"few\",{\"1\":{\"46\":1}}],[\"fluid\",{\"1\":{\"154\":1}}],[\"fluctuations\",{\"1\":{\"144\":2}}],[\"flaw\",{\"1\":{\"229\":1}}],[\"flashlight1\",{\"1\":{\"227\":1}}],[\"flashlights\",{\"1\":{\"226\":1}}],[\"flashlight\",{\"1\":{\"226\":21,\"227\":2,\"229\":2}}],[\"flatten\",{\"1\":{\"104\":2}}],[\"flame\",{\"1\":{\"3\":1}}],[\"flexible\",{\"1\":{\"94\":1}}],[\"floor\",{\"1\":{\"213\":1}}],[\"float\",{\"1\":{\"61\":4,\"104\":5}}],[\"flow\",{\"0\":{\"56\":1,\"145\":1},\"1\":{\"49\":2,\"141\":2,\"146\":1}}],[\"flipping\",{\"1\":{\"227\":1}}],[\"flip\",{\"1\":{\"43\":2}}],[\"family\",{\"1\":{\"224\":1}}],[\"failure\",{\"1\":{\"215\":1,\"217\":3,\"218\":1}}],[\"fail\",{\"1\":{\"200\":1}}],[\"fairness\",{\"1\":{\"197\":1}}],[\"fa\",{\"1\":{\"172\":2}}],[\"fast\",{\"1\":{\"144\":1}}],[\"fashion\",{\"1\":{\"89\":1}}],[\"false\",{\"0\":{\"64\":3},\"1\":{\"226\":2}}],[\"fall\",{\"1\":{\"49\":1}}],[\"falls\",{\"1\":{\"45\":1}}],[\"factor\",{\"1\":{\"53\":1,\"217\":1}}],[\"factors\",{\"0\":{\"35\":1}}],[\"fact\",{\"1\":{\"45\":1}}],[\"far\",{\"1\":{\"17\":2,\"229\":1}}],[\"f\",{\"1\":{\"32\":2,\"33\":1,\"49\":1,\"61\":3,\"69\":3,\"71\":2,\"72\":2,\"92\":1,\"95\":1,\"100\":1,\"104\":3,\"161\":6,\"213\":2,\"215\":1,\"217\":1,\"218\":1,\"219\":3,\"221\":1,\"223\":3,\"224\":1,\"226\":3,\"227\":1}}],[\"filled\",{\"1\":{\"229\":1}}],[\"filo\",{\"1\":{\"216\":1}}],[\"fifo\",{\"1\":{\"216\":1}}],[\"five\",{\"1\":{\"134\":1,\"221\":3}}],[\"fitting\",{\"1\":{\"111\":1}}],[\"fit\",{\"1\":{\"109\":1}}],[\"figure\",{\"1\":{\"168\":1,\"194\":1,\"207\":1,\"213\":1,\"215\":1,\"221\":1,\"222\":1,\"226\":1}}],[\"fig\",{\"1\":{\"104\":1}}],[\"firstly\",{\"1\":{\"221\":2}}],[\"first\",{\"1\":{\"45\":4,\"48\":1,\"82\":1,\"121\":1,\"138\":1,\"198\":1,\"199\":4,\"207\":1,\"216\":13,\"224\":1,\"226\":1,\"227\":1}}],[\"fixed\",{\"0\":{\"220\":1},\"1\":{\"32\":2,\"100\":1,\"219\":1,\"222\":2,\"223\":6}}],[\"fix\",{\"1\":{\"30\":2}}],[\"finalized\",{\"1\":{\"224\":1}}],[\"final\",{\"0\":{\"197\":1},\"1\":{\"102\":1,\"160\":4,\"161\":8,\"203\":1,\"207\":1,\"219\":2}}],[\"finally\",{\"1\":{\"7\":1,\"40\":1,\"197\":1,\"198\":1,\"207\":1}}],[\"fine\",{\"1\":{\"81\":1,\"109\":1,\"166\":1}}],[\"finnaly\",{\"1\":{\"40\":1}}],[\"finite\",{\"1\":{\"16\":1,\"17\":1,\"18\":1,\"188\":1,\"200\":2,\"207\":1,\"212\":1,\"219\":1,\"226\":3}}],[\"finds\",{\"1\":{\"154\":1}}],[\"finding\",{\"0\":{\"184\":1,\"197\":1},\"1\":{\"24\":1,\"39\":1,\"185\":2,\"192\":1,\"200\":1,\"223\":1,\"224\":1}}],[\"find\",{\"1\":{\"5\":1,\"7\":2,\"24\":1,\"27\":1,\"29\":1,\"83\":2,\"123\":1,\"185\":1,\"189\":1,\"191\":1,\"192\":1,\"199\":1,\"200\":2,\"213\":1,\"216\":2,\"221\":1,\"226\":2}}],[\"fuctional\",{\"1\":{\"221\":1}}],[\"fuction\",{\"1\":{\"212\":1,\"219\":1,\"221\":1}}],[\"fuels\",{\"1\":{\"140\":1}}],[\"further\",{\"0\":{\"118\":1},\"1\":{\"82\":1,\"83\":1,\"223\":2}}],[\"fully\",{\"0\":{\"192\":1},\"1\":{\"46\":1,\"53\":1,\"59\":1,\"78\":2,\"83\":1,\"192\":1,\"200\":1,\"214\":1}}],[\"full\",{\"1\":{\"40\":1,\"184\":1,\"201\":1}}],[\"fundemantal\",{\"1\":{\"148\":1}}],[\"fundamental\",{\"0\":{\"141\":1},\"1\":{\"24\":1,\"141\":1}}],[\"funciton\",{\"1\":{\"84\":1}}],[\"functions\",{\"1\":{\"100\":1,\"123\":1,\"220\":1,\"222\":1,\"226\":2}}],[\"functional\",{\"1\":{\"61\":1,\"104\":1,\"219\":2}}],[\"function\",{\"1\":{\"22\":1,\"33\":1,\"49\":2,\"53\":1,\"54\":1,\"59\":2,\"60\":1,\"69\":2,\"83\":1,\"87\":1,\"89\":2,\"93\":1,\"96\":1,\"97\":1,\"98\":1,\"101\":2,\"103\":2,\"114\":1,\"115\":1,\"119\":1,\"185\":1,\"190\":3,\"193\":1,\"203\":2,\"207\":2,\"216\":2,\"222\":1,\"223\":3,\"226\":3}}],[\"future\",{\"1\":{\"17\":3,\"23\":1,\"25\":1,\"198\":1,\"223\":1}}],[\"fomular\",{\"1\":{\"167\":1}}],[\"folds\",{\"1\":{\"120\":2}}],[\"fold\",{\"1\":{\"120\":2}}],[\"followed\",{\"1\":{\"59\":1}}],[\"follows\",{\"1\":{\"49\":1,\"134\":1,\"199\":1}}],[\"following\",{\"1\":{\"8\":1,\"18\":1,\"21\":2,\"24\":1,\"25\":2,\"30\":1,\"43\":1,\"49\":2,\"51\":1,\"59\":2,\"61\":3,\"197\":1,\"207\":1,\"215\":1,\"218\":1,\"221\":3,\"226\":2}}],[\"font\",{\"1\":{\"104\":2}}],[\"focusing\",{\"1\":{\"52\":1}}],[\"focuses\",{\"1\":{\"45\":1,\"215\":1}}],[\"focus\",{\"1\":{\"37\":1,\"94\":1,\"223\":1}}],[\"forbidden\",{\"1\":{\"198\":1}}],[\"force\",{\"1\":{\"154\":3}}],[\"forward\",{\"0\":{\"215\":1,\"216\":1,\"222\":1},\"1\":{\"61\":1,\"104\":1,\"144\":1,\"215\":2,\"216\":1,\"218\":1,\"220\":1,\"222\":2,\"223\":2,\"224\":5}}],[\"forwardfeed\",{\"1\":{\"49\":1}}],[\"forget\",{\"1\":{\"49\":3}}],[\"for\",{\"0\":{\"145\":1,\"214\":1},\"1\":{\"21\":1,\"22\":1,\"25\":1,\"37\":1,\"38\":1,\"41\":1,\"43\":2,\"44\":3,\"45\":2,\"46\":1,\"49\":7,\"52\":2,\"59\":1,\"67\":2,\"74\":1,\"82\":3,\"84\":1,\"86\":3,\"87\":2,\"89\":3,\"91\":1,\"93\":1,\"96\":3,\"98\":3,\"99\":1,\"100\":1,\"101\":1,\"102\":1,\"104\":5,\"120\":1,\"123\":2,\"124\":1,\"138\":2,\"144\":1,\"146\":1,\"160\":1,\"161\":11,\"167\":2,\"185\":3,\"188\":1,\"189\":1,\"191\":2,\"192\":1,\"193\":3,\"195\":6,\"196\":3,\"197\":2,\"198\":5,\"199\":3,\"203\":2,\"205\":1,\"207\":1,\"213\":4,\"215\":3,\"216\":4,\"217\":4,\"218\":4,\"219\":2,\"221\":4,\"222\":3,\"223\":9,\"224\":8,\"225\":1,\"226\":6,\"227\":7,\"229\":5}}],[\"former\",{\"1\":{\"222\":1}}],[\"formed\",{\"1\":{\"198\":1}}],[\"forms\",{\"1\":{\"185\":1,\"191\":1}}],[\"format\",{\"1\":{\"160\":1}}],[\"formulas\",{\"1\":{\"223\":2}}],[\"formulate\",{\"0\":{\"225\":1}}],[\"formulated\",{\"1\":{\"203\":1,\"224\":1}}],[\"formulations\",{\"1\":{\"223\":1}}],[\"formulation\",{\"0\":{\"69\":1,\"212\":1},\"1\":{\"212\":1,\"213\":1,\"219\":2,\"226\":1}}],[\"formula\",{\"1\":{\"20\":1,\"25\":1,\"39\":1,\"49\":1,\"221\":1,\"223\":2,\"227\":1}}],[\"form\",{\"1\":{\"11\":1,\"13\":1,\"23\":3,\"24\":3,\"29\":2,\"34\":2,\"46\":1,\"71\":1,\"198\":1,\"213\":2,\"226\":2,\"229\":1}}],[\"foudation\",{\"1\":{\"166\":2}}],[\"found\",{\"1\":{\"123\":1,\"197\":1,\"199\":1,\"216\":1,\"218\":1,\"231\":1}}],[\"foundation\",{\"1\":{\"24\":1}}],[\"foundations\",{\"1\":{\"5\":1}}],[\"fourth\",{\"1\":{\"207\":1}}],[\"four\",{\"1\":{\"21\":2,\"23\":1,\"88\":1,\"89\":1,\"168\":2,\"213\":1,\"221\":1,\"226\":1}}],[\"friend\",{\"1\":{\"196\":1}}],[\"front\",{\"0\":{\"197\":1},\"1\":{\"144\":1,\"185\":1,\"191\":1,\"196\":1}}],[\"from\",{\"0\":{\"8\":1},\"1\":{\"5\":1,\"7\":2,\"16\":1,\"18\":1,\"22\":2,\"23\":3,\"24\":1,\"25\":4,\"30\":2,\"45\":3,\"46\":5,\"49\":11,\"58\":1,\"59\":2,\"60\":1,\"67\":1,\"82\":1,\"91\":1,\"99\":1,\"100\":1,\"124\":1,\"185\":2,\"189\":1,\"195\":1,\"207\":1,\"213\":2,\"217\":5,\"218\":4,\"219\":1,\"221\":3,\"222\":4,\"223\":7,\"224\":1,\"226\":1,\"227\":2,\"229\":1}}],[\"free\",{\"1\":{\"43\":4,\"44\":3,\"67\":1,\"141\":1,\"185\":2,\"189\":1,\"207\":1}}],[\"frequently\",{\"1\":{\"25\":1}}],[\"framework\",{\"1\":{\"8\":1,\"49\":2,\"99\":1}}],[\"o2\",{\"1\":{\"229\":3}}],[\"o2∈oσ​\",{\"1\":{\"229\":1}}],[\"o1≺σ​o2\",{\"1\":{\"229\":1}}],[\"o1\",{\"1\":{\"229\":4}}],[\"oσ​=\",{\"1\":{\"229\":1}}],[\"oσ​\",{\"1\":{\"229\":3}}],[\"o=obstacle\",{\"1\":{\"161\":1}}],[\"o\",{\"1\":{\"160\":1,\"216\":2,\"226\":2}}],[\"oi\",{\"1\":{\"158\":1,\"161\":1}}],[\"occupying\",{\"1\":{\"189\":1}}],[\"occupied\",{\"1\":{\"143\":1}}],[\"occurs\",{\"1\":{\"217\":1}}],[\"occur\",{\"1\":{\"144\":1}}],[\"oscillate\",{\"1\":{\"109\":1,\"110\":1}}],[\"objects\",{\"1\":{\"226\":1}}],[\"object\",{\"1\":{\"226\":1}}],[\"objectives\",{\"0\":{\"190\":1},\"1\":{\"185\":1,\"190\":1,\"191\":1,\"193\":2,\"194\":1,\"196\":1,\"199\":1,\"200\":1}}],[\"objective\",{\"1\":{\"185\":2,\"190\":3,\"191\":2,\"192\":3,\"193\":4,\"194\":1,\"195\":4,\"196\":2,\"199\":11,\"200\":2,\"203\":1}}],[\"obviously\",{\"1\":{\"221\":1,\"223\":1,\"226\":1,\"227\":1}}],[\"obstacle\",{\"1\":{\"160\":7,\"198\":1}}],[\"observes\",{\"1\":{\"81\":1}}],[\"obtaining\",{\"1\":{\"38\":1,\"120\":1}}],[\"obtained\",{\"1\":{\"17\":2,\"22\":2,\"216\":1,\"219\":1,\"227\":1}}],[\"obtain\",{\"1\":{\"10\":1,\"23\":1,\"27\":1,\"28\":1,\"39\":1,\"40\":2,\"43\":2,\"44\":1,\"49\":2,\"198\":1,\"218\":1,\"219\":1,\"221\":2,\"223\":1,\"229\":1}}],[\"opposite\",{\"1\":{\"189\":1}}],[\"operator\",{\"1\":{\"226\":2,\"227\":1,\"229\":3}}],[\"operators\",{\"1\":{\"226\":5,\"227\":2,\"229\":3}}],[\"operations\",{\"1\":{\"91\":1}}],[\"operation\",{\"1\":{\"78\":1,\"96\":1,\"101\":1,\"216\":1}}],[\"open\",{\"1\":{\"89\":1}}],[\"optima\",{\"1\":{\"83\":1}}],[\"optimalitly\",{\"1\":{\"46\":1}}],[\"optimality\",{\"0\":{\"27\":1,\"29\":1,\"31\":1,\"191\":1,\"193\":1,\"194\":1},\"1\":{\"27\":1,\"28\":1,\"29\":2,\"37\":1,\"185\":1,\"207\":1}}],[\"optimal\",{\"0\":{\"27\":1,\"28\":1,\"35\":1,\"197\":1,\"219\":1,\"220\":1,\"223\":1},\"1\":{\"27\":2,\"28\":5,\"30\":1,\"37\":1,\"39\":1,\"43\":1,\"46\":4,\"123\":1,\"185\":2,\"191\":4,\"192\":1,\"194\":2,\"195\":1,\"196\":2,\"197\":1,\"199\":1,\"200\":3,\"203\":1,\"219\":2,\"220\":1,\"221\":1,\"222\":2,\"223\":6,\"224\":3,\"228\":1}}],[\"optim\",{\"0\":{\"81\":1},\"1\":{\"81\":2}}],[\"optimizing\",{\"1\":{\"219\":1}}],[\"optimizable\",{\"1\":{\"87\":1}}],[\"optimization\",{\"1\":{\"69\":1,\"82\":1,\"185\":1,\"203\":2}}],[\"optimizes\",{\"1\":{\"203\":1}}],[\"optimizer\",{\"1\":{\"83\":2}}],[\"optimize\",{\"0\":{\"84\":1},\"1\":{\"69\":3,\"185\":1,\"193\":2}}],[\"options\",{\"1\":{\"221\":1,\"222\":1,\"227\":2}}],[\"optional\",{\"1\":{\"87\":1}}],[\"optionally\",{\"1\":{\"53\":1}}],[\"option\",{\"1\":{\"18\":3,\"87\":1}}],[\"omits\",{\"1\":{\"168\":1}}],[\"omit\",{\"1\":{\"60\":1,\"168\":1}}],[\"omitted\",{\"1\":{\"23\":1,\"221\":1}}],[\"overwhelming\",{\"1\":{\"226\":1}}],[\"overreations\",{\"1\":{\"140\":1,\"144\":1}}],[\"overlapping\",{\"1\":{\"122\":1}}],[\"overfitting\",{\"1\":{\"119\":1}}],[\"overall\",{\"1\":{\"82\":1,\"223\":1}}],[\"over\",{\"1\":{\"59\":1,\"81\":3,\"83\":1,\"207\":1,\"220\":1,\"223\":1,\"224\":2}}],[\"overview\",{\"1\":{\"49\":1}}],[\"own\",{\"1\":{\"49\":1,\"92\":1,\"185\":2,\"192\":1,\"193\":1,\"196\":1}}],[\"okay\",{\"1\":{\"30\":1}}],[\"outside\",{\"1\":{\"219\":1,\"226\":1}}],[\"outcome\",{\"1\":{\"124\":1}}],[\"outcomes\",{\"1\":{\"43\":1}}],[\"output=attention\",{\"1\":{\"55\":1}}],[\"outputs=torch\",{\"1\":{\"82\":1,\"84\":1}}],[\"outputs=none\",{\"1\":{\"82\":1}}],[\"outputs\",{\"1\":{\"49\":1,\"59\":1,\"82\":7}}],[\"output\",{\"0\":{\"102\":1},\"1\":{\"49\":6,\"52\":1,\"55\":1,\"58\":2,\"59\":6,\"61\":5,\"67\":1,\"75\":1,\"82\":4,\"87\":1,\"93\":2,\"98\":2,\"104\":4}}],[\"out\",{\"1\":{\"24\":1,\"34\":1,\"49\":1,\"104\":3,\"194\":1,\"216\":2,\"221\":1}}],[\"our\",{\"1\":{\"12\":1}}],[\"otherwise\",{\"1\":{\"89\":1,\"144\":1,\"219\":1}}],[\"others\",{\"1\":{\"18\":1,\"54\":1,\"196\":1,\"200\":1,\"227\":1}}],[\"other\",{\"0\":{\"15\":1,\"217\":1},\"1\":{\"25\":1,\"29\":1,\"41\":1,\"45\":2,\"67\":1,\"82\":1,\"89\":3,\"103\":1,\"121\":1,\"154\":1,\"185\":2,\"191\":1,\"196\":2,\"197\":1,\"200\":1,\"217\":1,\"222\":1,\"226\":2,\"229\":2}}],[\"onward\",{\"1\":{\"168\":1}}],[\"only\",{\"1\":{\"25\":1,\"45\":1,\"46\":2,\"49\":1,\"82\":1,\"143\":1,\"144\":1,\"199\":1,\"215\":1,\"216\":1,\"222\":1,\"224\":1,\"226\":3,\"227\":1}}],[\"once\",{\"1\":{\"18\":1,\"40\":1,\"88\":1,\"207\":1,\"223\":2}}],[\"on\",{\"1\":{\"12\":1,\"14\":1,\"22\":1,\"25\":3,\"30\":1,\"37\":1,\"43\":2,\"45\":1,\"49\":1,\"52\":1,\"53\":1,\"59\":1,\"81\":4,\"82\":1,\"87\":2,\"94\":1,\"99\":1,\"111\":1,\"119\":1,\"120\":1,\"121\":1,\"124\":3,\"134\":1,\"166\":1,\"184\":1,\"190\":1,\"196\":1,\"199\":1,\"200\":1,\"203\":2,\"207\":1,\"213\":3,\"216\":1,\"221\":1,\"223\":7,\"224\":1,\"226\":13,\"227\":2,\"229\":4}}],[\"ones\",{\"1\":{\"82\":1,\"84\":1,\"160\":2}}],[\"one\",{\"1\":{\"7\":3,\"8\":4,\"10\":1,\"11\":2,\"20\":1,\"22\":1,\"23\":2,\"30\":3,\"39\":1,\"40\":2,\"41\":2,\"45\":4,\"49\":2,\"82\":1,\"89\":2,\"143\":1,\"144\":1,\"168\":6,\"185\":2,\"191\":2,\"193\":2,\"194\":1,\"195\":2,\"196\":4,\"197\":1,\"198\":1,\"207\":2,\"213\":2,\"215\":1,\"217\":1,\"224\":1,\"226\":4,\"229\":6}}],[\"ordinary\",{\"1\":{\"229\":1}}],[\"ordering\",{\"1\":{\"199\":8,\"227\":1,\"229\":2}}],[\"order\",{\"1\":{\"82\":3,\"87\":2,\"89\":1,\"109\":1,\"203\":1,\"227\":1}}],[\"organization\",{\"0\":{\"208\":1}}],[\"originating\",{\"1\":{\"221\":1}}],[\"originally\",{\"1\":{\"223\":1}}],[\"original\",{\"1\":{\"200\":1,\"203\":1,\"207\":1}}],[\"orientation\",{\"1\":{\"154\":1,\"207\":1}}],[\"orthogonal\",{\"1\":{\"121\":1}}],[\"or\",{\"0\":{\"197\":1},\"1\":{\"12\":1,\"18\":1,\"28\":2,\"35\":1,\"39\":1,\"41\":1,\"43\":2,\"44\":1,\"46\":1,\"49\":1,\"52\":2,\"81\":4,\"83\":2,\"86\":1,\"87\":3,\"88\":2,\"89\":4,\"100\":1,\"110\":1,\"119\":1,\"124\":2,\"134\":2,\"140\":1,\"143\":1,\"154\":1,\"168\":2,\"188\":2,\"190\":1,\"191\":1,\"197\":2,\"198\":2,\"199\":1,\"206\":1,\"207\":10,\"212\":1,\"213\":1,\"218\":5,\"219\":1,\"220\":1,\"223\":1,\"226\":7,\"227\":5,\"229\":1}}],[\"often\",{\"1\":{\"59\":1,\"83\":1,\"193\":1}}],[\"offs\",{\"0\":{\"194\":1},\"1\":{\"193\":1,\"196\":3,\"197\":1}}],[\"offer\",{\"1\":{\"167\":2,\"196\":1,\"229\":1}}],[\"off\",{\"1\":{\"41\":1,\"197\":1,\"226\":1}}],[\"of\",{\"0\":{\"7\":1,\"8\":1,\"33\":1,\"51\":1,\"72\":1,\"80\":1,\"195\":1,\"207\":1,\"208\":1,\"213\":1,\"218\":1,\"223\":1,\"229\":1},\"1\":{\"5\":2,\"7\":3,\"8\":1,\"9\":1,\"10\":1,\"11\":1,\"16\":2,\"17\":4,\"21\":1,\"22\":4,\"23\":4,\"24\":1,\"25\":1,\"30\":1,\"32\":1,\"33\":1,\"34\":1,\"39\":2,\"41\":2,\"43\":3,\"44\":8,\"45\":4,\"46\":6,\"48\":3,\"49\":12,\"52\":2,\"53\":2,\"54\":1,\"55\":1,\"57\":1,\"58\":2,\"59\":5,\"60\":2,\"61\":3,\"65\":1,\"67\":1,\"78\":6,\"81\":2,\"82\":9,\"83\":1,\"86\":2,\"87\":8,\"88\":9,\"89\":4,\"92\":2,\"93\":2,\"94\":1,\"95\":1,\"96\":1,\"97\":2,\"98\":1,\"99\":1,\"100\":2,\"101\":1,\"103\":1,\"104\":2,\"109\":1,\"110\":1,\"111\":6,\"119\":1,\"120\":3,\"121\":4,\"122\":1,\"123\":1,\"124\":2,\"127\":1,\"134\":2,\"140\":1,\"143\":1,\"144\":8,\"146\":1,\"148\":1,\"154\":4,\"166\":2,\"167\":2,\"168\":4,\"185\":5,\"188\":3,\"189\":6,\"190\":5,\"191\":2,\"192\":1,\"193\":2,\"194\":1,\"195\":3,\"196\":3,\"197\":6,\"198\":10,\"199\":4,\"200\":6,\"203\":7,\"205\":1,\"206\":2,\"207\":15,\"213\":8,\"214\":3,\"215\":5,\"216\":8,\"219\":9,\"220\":1,\"221\":7,\"222\":2,\"223\":13,\"224\":4,\"225\":2,\"226\":23,\"227\":16,\"229\":11}}],[\"oyh的帮助\",{\"1\":{\"1\":1}}],[\"away\",{\"1\":{\"224\":1}}],[\"ak​<bk​\",{\"1\":{\"199\":1}}],[\"ak−1​=bk−1​\",{\"1\":{\"199\":1}}],[\"aka\",{\"1\":{\"196\":1}}],[\"a≺b\",{\"1\":{\"199\":2}}],[\"aj​<bj​\",{\"1\":{\"195\":1}}],[\"aj​≤bj​\",{\"1\":{\"195\":1}}],[\"aj​\",{\"1\":{\"195\":1}}],[\"a2​<b2​\",{\"1\":{\"199\":1}}],[\"a2​=10\",{\"1\":{\"199\":1}}],[\"a2​=b2​\",{\"1\":{\"199\":1}}],[\"a2​\",{\"1\":{\"195\":1,\"199\":1}}],[\"am​\",{\"1\":{\"195\":1,\"199\":1}}],[\"among\",{\"1\":{\"185\":1,\"193\":1,\"203\":1}}],[\"amount\",{\"1\":{\"144\":1}}],[\"amax​\",{\"1\":{\"146\":1}}],[\"amplified\",{\"1\":{\"140\":2}}],[\"autonomous\",{\"1\":{\"185\":1}}],[\"automaton\",{\"1\":{\"134\":2}}],[\"automata\",{\"0\":{\"126\":1},\"1\":{\"127\":2,\"134\":1,\"138\":1,\"154\":1}}],[\"autograd\",{\"0\":{\"82\":1},\"1\":{\"82\":2,\"84\":1}}],[\"ax=ax\",{\"1\":{\"104\":1}}],[\"ax\",{\"1\":{\"104\":2}}],[\"axe\",{\"1\":{\"104\":4}}],[\"a=\",{\"1\":{\"98\":1,\"195\":1,\"199\":2}}],[\"a=​011​100​100​​\",{\"1\":{\"93\":1,\"98\":1}}],[\"a∈r2f\",{\"1\":{\"96\":1}}],[\"a∈rn×n\",{\"1\":{\"92\":1,\"95\":1,\"100\":1}}],[\"a^xw\",{\"1\":{\"93\":1}}],[\"a^\",{\"1\":{\"93\":1}}],[\"a^h\",{\"1\":{\"93\":1}}],[\"a^=d~−21​a~d~−21​≈​0\",{\"1\":{\"93\":1}}],[\"a^=d~−21​a~d~−21​\",{\"1\":{\"92\":1}}],[\"a~\",{\"1\":{\"92\":1}}],[\"a~=a+i=​111​110​101​​\",{\"1\":{\"93\":1}}],[\"a~=a+i\",{\"1\":{\"92\":1}}],[\"again\",{\"1\":{\"215\":1}}],[\"against\",{\"1\":{\"52\":1}}],[\"aggregators\",{\"1\":{\"100\":1}}],[\"aggregation\",{\"0\":{\"98\":1},\"1\":{\"94\":1,\"95\":1,\"100\":1,\"101\":1,\"103\":1}}],[\"aggregating\",{\"1\":{\"91\":1,\"99\":1}}],[\"aggregate\",{\"1\":{\"89\":3,\"98\":1,\"101\":2}}],[\"agents\",{\"0\":{\"188\":1},\"1\":{\"185\":3,\"188\":2,\"189\":3,\"190\":4,\"192\":1,\"193\":3,\"198\":1,\"203\":2}}],[\"agent\",{\"0\":{\"184\":1,\"189\":1},\"1\":{\"7\":5,\"8\":2,\"9\":1,\"10\":2,\"11\":1,\"12\":3,\"18\":3,\"23\":3,\"25\":2,\"45\":1,\"69\":2,\"185\":6,\"188\":1,\"189\":2,\"190\":2,\"192\":1,\"198\":4,\"203\":2,\"206\":1}}],[\"advances\",{\"1\":{\"219\":1}}],[\"advance\",{\"1\":{\"214\":1,\"223\":1,\"227\":1}}],[\"advantage\",{\"1\":{\"46\":1}}],[\"adverb\",{\"1\":{\"88\":1}}],[\"adjacent\",{\"1\":{\"104\":1}}],[\"adjacency\",{\"1\":{\"87\":2,\"92\":2,\"93\":2,\"95\":1,\"98\":1,\"100\":1}}],[\"adj\",{\"1\":{\"104\":8}}],[\"adjusted\",{\"1\":{\"203\":1}}],[\"adjust\",{\"1\":{\"81\":2}}],[\"added\",{\"1\":{\"198\":2,\"226\":1}}],[\"addresses\",{\"1\":{\"200\":1}}],[\"address\",{\"1\":{\"194\":1,\"203\":1,\"223\":1}}],[\"additive\",{\"1\":{\"219\":1}}],[\"additional\",{\"1\":{\"119\":1,\"223\":1}}],[\"additionally\",{\"1\":{\"87\":1}}],[\"addition\",{\"1\":{\"78\":1,\"213\":1}}],[\"adding\",{\"1\":{\"119\":1,\"200\":1,\"226\":1}}],[\"add\",{\"1\":{\"61\":3,\"89\":2,\"92\":1,\"104\":2}}],[\"adapt\",{\"1\":{\"228\":1}}],[\"adaptive\",{\"1\":{\"167\":1}}],[\"adapting\",{\"1\":{\"49\":1,\"166\":1}}],[\"adapted\",{\"1\":{\"49\":1,\"224\":1}}],[\"adopted\",{\"1\":{\"45\":1}}],[\"adopt\",{\"1\":{\"30\":1}}],[\"a1​=b1​=5\",{\"1\":{\"199\":1}}],[\"a1​=b1​\",{\"1\":{\"199\":1}}],[\"a1​\",{\"1\":{\"45\":1,\"195\":1,\"199\":1}}],[\"a4​\",{\"1\":{\"45\":1}}],[\"avoided\",{\"1\":{\"185\":1}}],[\"avoid\",{\"1\":{\"87\":1,\"144\":1,\"189\":1}}],[\"available\",{\"1\":{\"44\":1,\"205\":1}}],[\"average\",{\"1\":{\"25\":2,\"43\":1,\"44\":1,\"45\":4}}],[\"aims\",{\"1\":{\"123\":1,\"199\":1}}],[\"aim\",{\"1\":{\"43\":1,\"193\":2,\"223\":1}}],[\"a|s\",{\"1\":{\"35\":1}}],[\"affected\",{\"1\":{\"198\":1}}],[\"affect\",{\"1\":{\"46\":1,\"87\":1}}],[\"affecting\",{\"1\":{\"35\":1}}],[\"afterward\",{\"1\":{\"82\":1}}],[\"after\",{\"1\":{\"10\":1,\"21\":1,\"23\":1,\"25\":1,\"45\":1,\"46\":1,\"82\":2,\"88\":1,\"93\":1,\"102\":1,\"219\":2}}],[\"a0​\",{\"1\":{\"34\":1}}],[\"applies\",{\"1\":{\"223\":1}}],[\"applied\",{\"1\":{\"55\":1,\"203\":1,\"207\":2,\"212\":1,\"219\":2,\"221\":1,\"223\":2,\"226\":2,\"227\":1,\"229\":1}}],[\"application\",{\"1\":{\"214\":1,\"219\":1,\"226\":1,\"229\":1}}],[\"applications\",{\"1\":{\"86\":1,\"134\":1,\"154\":1}}],[\"apply\",{\"1\":{\"61\":2,\"88\":1,\"93\":1,\"218\":1,\"226\":2,\"227\":1}}],[\"applying\",{\"1\":{\"33\":1,\"49\":1,\"219\":1,\"223\":1,\"226\":1}}],[\"appear\",{\"1\":{\"45\":3,\"219\":1,\"226\":1,\"227\":2,\"229\":2}}],[\"appears\",{\"1\":{\"45\":1,\"226\":1,\"229\":1}}],[\"approximate\",{\"1\":{\"44\":1,\"45\":2}}],[\"approximated\",{\"1\":{\"43\":1}}],[\"approach\",{\"0\":{\"67\":1},\"1\":{\"29\":1,\"37\":1,\"43\":3,\"44\":1,\"53\":1,\"203\":2,\"207\":1,\"216\":1,\"223\":1,\"224\":2}}],[\"a3​\",{\"1\":{\"25\":1,\"45\":1}}],[\"a∣s\",{\"1\":{\"22\":1,\"23\":4,\"25\":1,\"29\":2,\"30\":2,\"34\":2,\"38\":1,\"39\":1}}],[\"abstract\",{\"0\":{\"185\":1},\"1\":{\"78\":1,\"185\":1}}],[\"absorbing\",{\"1\":{\"18\":2}}],[\"ability\",{\"1\":{\"46\":1,\"109\":1,\"110\":1,\"111\":1}}],[\"above\",{\"1\":{\"8\":1,\"25\":1,\"30\":1,\"215\":1,\"227\":1}}],[\"about\",{\"1\":{\"1\":1,\"5\":1,\"21\":1,\"45\":1,\"199\":1,\"224\":1}}],[\"arise\",{\"1\":{\"229\":1}}],[\"argmin\",{\"1\":{\"223\":1}}],[\"arguments\",{\"1\":{\"226\":2,\"227\":2}}],[\"argument\",{\"1\":{\"82\":1,\"226\":8,\"227\":2}}],[\"arriving\",{\"1\":{\"207\":1}}],[\"arrive=0\",{\"1\":{\"160\":1}}],[\"array\",{\"1\":{\"11\":1}}],[\"architectural\",{\"1\":{\"154\":1}}],[\"architecture\",{\"1\":{\"91\":1,\"94\":1}}],[\"architectures\",{\"0\":{\"90\":1},\"1\":{\"166\":1}}],[\"around\",{\"1\":{\"89\":1}}],[\"artificial\",{\"1\":{\"49\":1}}],[\"article\",{\"1\":{\"48\":1,\"49\":1}}],[\"arbitrary\",{\"1\":{\"38\":1}}],[\"ar​\",{\"1\":{\"34\":1}}],[\"area\",{\"1\":{\"34\":1,\"89\":1}}],[\"are\",{\"1\":{\"18\":1,\"21\":2,\"23\":1,\"24\":1,\"29\":2,\"35\":1,\"41\":1,\"44\":2,\"45\":3,\"46\":2,\"49\":4,\"53\":2,\"54\":1,\"55\":1,\"59\":3,\"60\":1,\"82\":1,\"86\":1,\"88\":3,\"89\":1,\"91\":1,\"93\":1,\"97\":1,\"121\":1,\"123\":1,\"124\":1,\"166\":2,\"185\":2,\"190\":2,\"193\":1,\"195\":1,\"196\":4,\"198\":1,\"199\":3,\"206\":1,\"207\":3,\"213\":3,\"214\":1,\"215\":3,\"216\":3,\"218\":2,\"220\":1,\"221\":5,\"223\":4,\"224\":1,\"226\":18,\"227\":5,\"229\":3}}],[\"although\",{\"1\":{\"229\":1}}],[\"alternative\",{\"1\":{\"66\":1,\"224\":1,\"229\":1}}],[\"alternatively\",{\"1\":{\"59\":1}}],[\"alphabet\",{\"1\":{\"227\":1}}],[\"always\",{\"1\":{\"144\":2,\"191\":1,\"200\":1,\"226\":1}}],[\"alive\",{\"1\":{\"134\":1,\"215\":4,\"224\":3}}],[\"al​\",{\"1\":{\"34\":1}}],[\"algorithms\",{\"0\":{\"168\":1},\"1\":{\"53\":1,\"167\":1,\"205\":1,\"206\":1,\"214\":2,\"215\":1,\"216\":2,\"224\":1,\"226\":1}}],[\"algorithm\",{\"0\":{\"34\":1,\"41\":1,\"44\":1,\"45\":1,\"46\":1,\"200\":1},\"1\":{\"24\":1,\"29\":1,\"37\":1,\"38\":3,\"39\":2,\"41\":1,\"44\":1,\"45\":2,\"46\":2,\"110\":2,\"111\":2,\"122\":1,\"123\":1,\"124\":1,\"198\":1,\"200\":2,\"214\":1,\"215\":1,\"216\":5,\"218\":2,\"220\":3,\"223\":1,\"224\":6,\"225\":1,\"226\":1,\"227\":2,\"229\":1}}],[\"already\",{\"1\":{\"23\":1,\"214\":1,\"217\":2}}],[\"alongside\",{\"1\":{\"104\":1}}],[\"along\",{\"1\":{\"16\":1,\"49\":1,\"189\":1,\"207\":1,\"224\":1}}],[\"allow\",{\"1\":{\"82\":1,\"84\":1,\"226\":1}}],[\"allows\",{\"1\":{\"48\":1,\"81\":1,\"82\":2,\"83\":1,\"94\":1,\"199\":1,\"223\":1}}],[\"allocation\",{\"1\":{\"69\":2,\"71\":1,\"74\":1,\"75\":1,\"203\":5}}],[\"all\",{\"1\":{\"16\":1,\"21\":1,\"22\":1,\"23\":1,\"25\":2,\"30\":1,\"44\":1,\"45\":5,\"46\":2,\"87\":1,\"89\":4,\"185\":1,\"190\":2,\"191\":2,\"192\":2,\"193\":1,\"195\":3,\"196\":1,\"199\":3,\"200\":1,\"203\":1,\"207\":1,\"213\":2,\"215\":1,\"216\":1,\"217\":3,\"218\":1,\"219\":1,\"221\":1,\"222\":1,\"224\":7,\"226\":1,\"227\":8,\"229\":1}}],[\"also\",{\"1\":{\"8\":1,\"25\":2,\"44\":1,\"45\":4,\"49\":1,\"89\":1,\"215\":1,\"216\":1,\"222\":1,\"226\":1,\"229\":1}}],[\"achieving\",{\"1\":{\"226\":1}}],[\"achieved\",{\"1\":{\"223\":1,\"229\":1}}],[\"achieves\",{\"1\":{\"121\":1,\"229\":2}}],[\"achieve\",{\"1\":{\"111\":1,\"229\":1}}],[\"across\",{\"1\":{\"49\":1,\"81\":1,\"229\":1}}],[\"acquire\",{\"1\":{\"44\":1}}],[\"acquiring\",{\"1\":{\"39\":2}}],[\"acceptable\",{\"1\":{\"227\":1}}],[\"acceleration\",{\"1\":{\"144\":1,\"146\":2}}],[\"accelerates\",{\"1\":{\"144\":1,\"146\":1}}],[\"accelerate\",{\"1\":{\"140\":1}}],[\"accumulated\",{\"1\":{\"221\":1}}],[\"accuracy\",{\"1\":{\"45\":1,\"203\":1}}],[\"accurate\",{\"1\":{\"43\":1,\"45\":1,\"49\":1}}],[\"accomplish\",{\"1\":{\"223\":1}}],[\"account\",{\"1\":{\"207\":1}}],[\"according\",{\"1\":{\"7\":1,\"25\":1,\"40\":2}}],[\"aciton\",{\"1\":{\"10\":1,\"23\":1}}],[\"acitons\",{\"1\":{\"7\":1}}],[\"acts\",{\"1\":{\"203\":1}}],[\"act\",{\"1\":{\"89\":1}}],[\"actual\",{\"1\":{\"52\":1,\"111\":1,\"203\":1}}],[\"actually\",{\"1\":{\"8\":1,\"23\":1,\"41\":1}}],[\"activation\",{\"1\":{\"49\":1,\"59\":1,\"93\":2,\"96\":1,\"98\":1,\"101\":1,\"114\":1}}],[\"actionvalueqπ​\",{\"1\":{\"25\":1}}],[\"actiona\",{\"1\":{\"21\":1}}],[\"actions\",{\"1\":{\"11\":1,\"23\":1,\"25\":1,\"34\":1,\"188\":1,\"207\":5,\"213\":1,\"214\":1,\"219\":2,\"223\":2}}],[\"action\",{\"0\":{\"10\":1,\"25\":1},\"1\":{\"7\":1,\"8\":1,\"10\":2,\"11\":1,\"12\":1,\"13\":2,\"14\":1,\"16\":1,\"21\":2,\"23\":1,\"25\":19,\"30\":3,\"38\":3,\"39\":3,\"43\":1,\"44\":7,\"45\":11,\"46\":6,\"207\":2,\"212\":3,\"218\":1,\"223\":15}}],[\"asymptotic\",{\"1\":{\"168\":1,\"216\":1}}],[\"aspects\",{\"1\":{\"154\":1,\"207\":1}}],[\"assessing\",{\"1\":{\"120\":1}}],[\"associate\",{\"1\":{\"88\":1}}],[\"associated\",{\"1\":{\"87\":1,\"199\":1}}],[\"associating\",{\"1\":{\"87\":1}}],[\"assigning\",{\"1\":{\"122\":1}}],[\"assignments\",{\"1\":{\"203\":1}}],[\"assignment\",{\"1\":{\"69\":1,\"203\":2}}],[\"assign\",{\"1\":{\"48\":1}}],[\"assigned\",{\"1\":{\"30\":1,\"122\":1}}],[\"assume\",{\"1\":{\"23\":1,\"61\":1,\"93\":1,\"98\":1,\"221\":1}}],[\"assumed\",{\"1\":{\"18\":1,\"226\":2}}],[\"as\",{\"1\":{\"7\":1,\"10\":1,\"11\":2,\"18\":4,\"22\":2,\"25\":1,\"30\":1,\"33\":1,\"38\":1,\"39\":1,\"43\":2,\"44\":3,\"45\":2,\"46\":1,\"49\":3,\"59\":2,\"60\":1,\"61\":1,\"67\":1,\"69\":1,\"72\":1,\"81\":1,\"82\":1,\"89\":2,\"93\":1,\"96\":1,\"98\":1,\"101\":1,\"104\":3,\"119\":1,\"120\":2,\"140\":1,\"144\":2,\"166\":1,\"188\":1,\"189\":1,\"190\":1,\"193\":2,\"197\":1,\"198\":1,\"199\":2,\"203\":2,\"206\":1,\"207\":6,\"213\":2,\"215\":1,\"216\":2,\"217\":3,\"219\":2,\"221\":6,\"222\":1,\"223\":1,\"224\":1,\"226\":8,\"227\":3,\"229\":2}}],[\"analyze\",{\"1\":{\"185\":1}}],[\"analysis\",{\"1\":{\"88\":1,\"110\":1,\"121\":1,\"197\":1}}],[\"analogous\",{\"1\":{\"88\":2,\"223\":1}}],[\"anything\",{\"1\":{\"196\":1}}],[\"anymore\",{\"1\":{\"43\":1}}],[\"any\",{\"1\":{\"38\":1,\"46\":1,\"67\":1,\"78\":1,\"111\":1,\"121\":1,\"196\":1,\"207\":2,\"215\":1,\"222\":1,\"223\":2,\"226\":4,\"227\":4}}],[\"answers\",{\"1\":{\"28\":1}}],[\"anther\",{\"1\":{\"23\":1}}],[\"another\",{\"1\":{\"7\":1,\"8\":1,\"23\":1,\"49\":1,\"140\":1,\"191\":3,\"193\":2,\"194\":1,\"196\":2,\"199\":1,\"223\":1,\"229\":1}}],[\"an\",{\"1\":{\"7\":1,\"18\":3,\"24\":1,\"25\":1,\"29\":1,\"38\":1,\"39\":1,\"44\":2,\"45\":1,\"49\":2,\"59\":2,\"66\":1,\"67\":1,\"78\":1,\"82\":1,\"87\":2,\"88\":5,\"89\":3,\"94\":1,\"188\":1,\"190\":3,\"193\":1,\"195\":1,\"198\":1,\"199\":2,\"200\":1,\"203\":2,\"207\":1,\"213\":1,\"214\":1,\"216\":1,\"218\":4,\"219\":1,\"221\":1,\"226\":3,\"229\":1}}],[\"andb=\",{\"1\":{\"195\":1,\"199\":1}}],[\"and\",{\"0\":{\"16\":1,\"27\":1,\"37\":1,\"40\":1,\"52\":1,\"67\":1,\"184\":1,\"188\":1,\"189\":1,\"190\":1,\"192\":1,\"193\":1,\"194\":1,\"201\":1},\"1\":{\"5\":1,\"7\":1,\"8\":2,\"12\":1,\"14\":1,\"17\":1,\"18\":3,\"20\":1,\"21\":2,\"22\":1,\"23\":4,\"25\":8,\"29\":2,\"30\":1,\"33\":1,\"34\":1,\"37\":2,\"40\":8,\"41\":1,\"43\":2,\"44\":4,\"45\":4,\"46\":4,\"49\":12,\"53\":5,\"58\":2,\"59\":6,\"60\":2,\"71\":1,\"78\":1,\"82\":2,\"83\":2,\"86\":2,\"87\":3,\"88\":3,\"89\":10,\"91\":1,\"93\":2,\"94\":1,\"96\":2,\"98\":3,\"99\":2,\"103\":1,\"110\":1,\"111\":1,\"120\":5,\"121\":2,\"122\":2,\"123\":2,\"124\":2,\"134\":4,\"138\":2,\"140\":5,\"143\":3,\"146\":1,\"154\":3,\"166\":4,\"167\":2,\"168\":1,\"184\":1,\"185\":7,\"190\":1,\"191\":1,\"192\":1,\"193\":4,\"195\":3,\"196\":3,\"198\":6,\"199\":5,\"203\":5,\"207\":4,\"213\":3,\"215\":3,\"216\":4,\"217\":6,\"218\":1,\"219\":3,\"220\":1,\"221\":2,\"222\":3,\"223\":7,\"224\":5,\"225\":1,\"226\":18,\"227\":6,\"229\":5}}],[\"attributes\",{\"1\":{\"87\":3,\"89\":1}}],[\"attention\",{\"0\":{\"48\":1,\"51\":1,\"53\":1,\"58\":1,\"96\":1,\"97\":1},\"1\":{\"48\":2,\"49\":1,\"51\":1,\"52\":1,\"53\":6,\"54\":2,\"55\":2,\"58\":3,\"59\":2,\"60\":3,\"61\":9,\"94\":2,\"95\":2,\"96\":2,\"97\":1,\"98\":1,\"203\":1}}],[\"ateention\",{\"1\":{\"48\":1}}],[\"at​=a\",{\"1\":{\"23\":2,\"25\":1}}],[\"at​→at​rt+1​\",{\"1\":{\"21\":1}}],[\"at​→at​st+1​\",{\"1\":{\"21\":1}}],[\"at​\",{\"1\":{\"21\":2}}],[\"at​∣st​\",{\"1\":{\"11\":1,\"21\":1}}],[\"at\",{\"1\":{\"5\":1,\"7\":1,\"10\":1,\"11\":2,\"13\":2,\"18\":1,\"40\":1,\"44\":1,\"49\":2,\"59\":3,\"78\":1,\"88\":1,\"96\":1,\"101\":1,\"134\":1,\"144\":1,\"184\":2,\"185\":1,\"189\":2,\"191\":1,\"193\":1,\"195\":2,\"196\":1,\"198\":1,\"201\":1,\"205\":1,\"207\":3,\"215\":1,\"216\":1,\"217\":1,\"219\":1,\"223\":3,\"224\":3}}],[\"a\",{\"0\":{\"145\":1,\"218\":1,\"226\":1,\"229\":1},\"1\":{\"5\":1,\"7\":5,\"8\":1,\"10\":5,\"11\":3,\"13\":4,\"16\":3,\"18\":8,\"21\":2,\"22\":8,\"23\":8,\"24\":3,\"25\":9,\"28\":1,\"29\":5,\"30\":4,\"32\":2,\"33\":1,\"34\":4,\"35\":2,\"38\":4,\"39\":4,\"40\":2,\"41\":1,\"43\":5,\"44\":7,\"45\":6,\"46\":5,\"48\":1,\"49\":8,\"53\":4,\"54\":1,\"55\":1,\"59\":4,\"61\":2,\"65\":1,\"67\":2,\"69\":1,\"78\":5,\"81\":2,\"82\":3,\"83\":3,\"84\":1,\"86\":3,\"87\":6,\"88\":6,\"89\":9,\"91\":3,\"92\":2,\"94\":2,\"96\":1,\"98\":1,\"99\":2,\"100\":2,\"104\":6,\"115\":1,\"119\":2,\"120\":4,\"121\":2,\"122\":2,\"123\":3,\"124\":9,\"134\":4,\"138\":1,\"140\":1,\"143\":1,\"144\":1,\"185\":5,\"188\":3,\"189\":5,\"190\":1,\"191\":4,\"192\":4,\"193\":2,\"195\":6,\"196\":3,\"197\":10,\"198\":8,\"199\":11,\"200\":6,\"203\":9,\"206\":1,\"207\":20,\"212\":1,\"213\":4,\"215\":6,\"216\":7,\"217\":1,\"218\":5,\"219\":3,\"221\":15,\"222\":6,\"223\":13,\"224\":10,\"225\":1,\"226\":24,\"227\":14,\"229\":14}}],[\"illinois\",{\"1\":{\"205\":1}}],[\"illustration\",{\"1\":{\"59\":2,\"61\":2}}],[\"illustrative\",{\"1\":{\"23\":2}}],[\"illustrate\",{\"1\":{\"43\":1,\"59\":1}}],[\"i∈i\",{\"1\":{\"189\":1}}],[\"irrelevant\",{\"1\":{\"226\":1}}],[\"irregular\",{\"1\":{\"91\":1}}],[\"iros\",{\"1\":{\"184\":1}}],[\"ieee\",{\"1\":{\"184\":1}}],[\"i+neigh\",{\"1\":{\"161\":1}}],[\"i=\",{\"1\":{\"188\":1,\"226\":1}}],[\"i=2\",{\"1\":{\"161\":1}}],[\"i=1\",{\"1\":{\"58\":1,\"161\":2,\"216\":1}}],[\"ica\",{\"1\":{\"110\":1}}],[\"identity\",{\"1\":{\"104\":1}}],[\"identify\",{\"1\":{\"88\":1,\"196\":1}}],[\"ideas\",{\"1\":{\"222\":1}}],[\"idea\",{\"1\":{\"43\":1,\"229\":1}}],[\"i−γpπ​\",{\"1\":{\"24\":2}}],[\"ij​=pπ​\",{\"1\":{\"23\":1}}],[\"imagine\",{\"1\":{\"226\":1}}],[\"imagined\",{\"1\":{\"207\":1,\"213\":1}}],[\"images\",{\"1\":{\"154\":1}}],[\"image\",{\"1\":{\"78\":4,\"88\":4}}],[\"imtsp\",{\"0\":{\"68\":1,\"203\":1}}],[\"immediate\",{\"1\":{\"23\":1,\"25\":1,\"223\":2}}],[\"immediately\",{\"1\":{\"23\":1,\"222\":1}}],[\"imperative\",{\"0\":{\"203\":1}}],[\"imperfect\",{\"1\":{\"144\":2}}],[\"impurity\",{\"1\":{\"124\":2}}],[\"impact\",{\"1\":{\"111\":1}}],[\"improving\",{\"1\":{\"81\":2,\"83\":1,\"193\":1,\"224\":1}}],[\"improves\",{\"1\":{\"203\":1,\"224\":1}}],[\"improve\",{\"1\":{\"44\":1,\"45\":2,\"83\":1,\"191\":1,\"194\":1,\"203\":1}}],[\"improved\",{\"1\":{\"39\":1,\"191\":1,\"199\":1}}],[\"improvement\",{\"1\":{\"39\":1,\"41\":1,\"44\":3,\"46\":1}}],[\"implied\",{\"1\":{\"226\":1}}],[\"implicit\",{\"1\":{\"225\":1}}],[\"implicitly\",{\"1\":{\"207\":2,\"229\":1}}],[\"implictly\",{\"1\":{\"8\":1}}],[\"implement\",{\"1\":{\"71\":1}}],[\"implementation\",{\"1\":{\"53\":1}}],[\"imposing\",{\"1\":{\"227\":1}}],[\"impossible\",{\"1\":{\"43\":1,\"191\":1,\"194\":1}}],[\"import\",{\"1\":{\"61\":2,\"104\":4}}],[\"importance\",{\"1\":{\"44\":1,\"48\":1,\"54\":1,\"94\":1}}],[\"important\",{\"1\":{\"20\":1,\"24\":1,\"32\":1,\"49\":1,\"60\":1,\"82\":1,\"214\":1,\"224\":1,\"226\":1}}],[\"if\",{\"1\":{\"17\":2,\"22\":2,\"28\":3,\"32\":2,\"43\":2,\"45\":2,\"46\":2,\"61\":6,\"81\":3,\"82\":2,\"83\":1,\"104\":3,\"110\":2,\"144\":1,\"161\":3,\"190\":1,\"191\":2,\"195\":2,\"196\":2,\"197\":2,\"198\":1,\"199\":1,\"213\":1,\"215\":2,\"216\":2,\"217\":7,\"218\":4,\"219\":1,\"224\":4,\"227\":1,\"229\":1}}],[\"i\",{\"1\":{\"9\":1,\"10\":2,\"21\":2,\"23\":1,\"37\":2,\"81\":2,\"82\":1,\"96\":2,\"97\":1,\"98\":1,\"104\":8,\"144\":8,\"158\":5,\"160\":7,\"161\":17,\"192\":1,\"195\":2,\"198\":7,\"213\":3,\"215\":2,\"216\":5,\"217\":7,\"226\":6,\"227\":2}}],[\"iterate\",{\"1\":{\"218\":1}}],[\"iterates\",{\"1\":{\"122\":1,\"224\":1}}],[\"iterations\",{\"1\":{\"81\":1,\"88\":1,\"89\":1,\"221\":1}}],[\"iteration\",{\"0\":{\"37\":2,\"38\":1,\"39\":1,\"40\":2,\"41\":1,\"221\":1,\"222\":1},\"1\":{\"37\":3,\"38\":4,\"39\":9,\"40\":7,\"41\":7,\"44\":6,\"45\":1,\"46\":1,\"220\":3,\"222\":1,\"223\":6,\"224\":7,\"228\":1,\"229\":2}}],[\"iteratively\",{\"1\":{\"198\":1,\"220\":1,\"221\":1,\"223\":1,\"229\":1}}],[\"iterative\",{\"0\":{\"34\":1},\"1\":{\"24\":2,\"37\":1,\"38\":2,\"39\":1,\"40\":3,\"216\":1}}],[\"itmes\",{\"1\":{\"61\":3}}],[\"it\",{\"1\":{\"8\":2,\"11\":1,\"16\":1,\"18\":2,\"22\":3,\"23\":4,\"24\":4,\"25\":2,\"29\":1,\"33\":1,\"38\":1,\"39\":2,\"43\":2,\"45\":2,\"46\":4,\"48\":1,\"49\":6,\"53\":1,\"78\":1,\"81\":3,\"82\":2,\"83\":3,\"87\":1,\"91\":1,\"94\":1,\"99\":1,\"111\":2,\"119\":1,\"120\":1,\"121\":1,\"122\":1,\"124\":1,\"134\":1,\"144\":5,\"154\":1,\"167\":1,\"191\":2,\"194\":1,\"195\":1,\"196\":1,\"197\":3,\"199\":3,\"200\":2,\"207\":4,\"213\":1,\"217\":2,\"218\":1,\"219\":1,\"220\":1,\"221\":2,\"223\":4,\"224\":4,\"226\":7,\"227\":1,\"229\":1}}],[\"itself\",{\"1\":{\"111\":2}}],[\"its\",{\"1\":{\"8\":1,\"9\":1,\"10\":1,\"25\":1,\"46\":1,\"49\":3,\"58\":1,\"83\":1,\"87\":2,\"88\":1,\"96\":1,\"98\":2,\"121\":1,\"143\":1,\"144\":1,\"185\":1,\"189\":1,\"192\":1,\"193\":3,\"196\":1,\"203\":1,\"221\":1,\"224\":1,\"226\":2}}],[\"issue\",{\"1\":{\"223\":1}}],[\"issues\",{\"1\":{\"167\":1}}],[\"isinf\",{\"1\":{\"104\":1}}],[\"is\",{\"0\":{\"119\":1,\"120\":1,\"196\":1,\"206\":1},\"1\":{\"5\":1,\"7\":2,\"8\":1,\"9\":2,\"10\":2,\"12\":2,\"13\":2,\"16\":2,\"17\":4,\"18\":2,\"21\":5,\"22\":9,\"23\":6,\"24\":7,\"25\":7,\"27\":1,\"28\":5,\"30\":3,\"32\":2,\"33\":1,\"35\":1,\"38\":4,\"39\":8,\"40\":1,\"41\":2,\"43\":9,\"44\":8,\"45\":6,\"46\":10,\"48\":1,\"49\":10,\"52\":2,\"53\":2,\"57\":2,\"58\":2,\"59\":8,\"60\":2,\"61\":2,\"67\":4,\"78\":2,\"81\":4,\"82\":5,\"83\":3,\"87\":5,\"88\":2,\"89\":7,\"91\":1,\"92\":3,\"93\":1,\"94\":1,\"95\":2,\"96\":1,\"98\":1,\"99\":1,\"100\":3,\"101\":1,\"102\":1,\"104\":4,\"110\":2,\"119\":1,\"120\":5,\"121\":1,\"122\":2,\"123\":4,\"124\":2,\"134\":1,\"138\":1,\"143\":3,\"144\":1,\"168\":2,\"184\":1,\"185\":2,\"188\":4,\"189\":2,\"190\":2,\"191\":3,\"192\":1,\"193\":1,\"195\":1,\"196\":4,\"197\":2,\"198\":9,\"199\":7,\"200\":3,\"203\":6,\"206\":1,\"207\":4,\"213\":3,\"214\":3,\"215\":6,\"216\":4,\"217\":4,\"218\":4,\"219\":10,\"220\":1,\"221\":6,\"222\":4,\"223\":17,\"224\":7,\"225\":1,\"226\":22,\"227\":8,\"229\":15}}],[\"inappropriate\",{\"1\":{\"223\":1}}],[\"inserted\",{\"1\":{\"218\":1,\"226\":2}}],[\"insert\",{\"1\":{\"215\":2,\"217\":6,\"218\":1,\"224\":2,\"226\":3,\"229\":4}}],[\"institute\",{\"1\":{\"226\":1}}],[\"instance\",{\"1\":{\"221\":1,\"222\":1,\"226\":3,\"227\":2}}],[\"instances\",{\"1\":{\"203\":1,\"226\":10,\"227\":3}}],[\"instead\",{\"1\":{\"17\":1,\"43\":1,\"44\":2,\"191\":1,\"199\":2,\"214\":1,\"227\":1}}],[\"ingredients\",{\"0\":{\"207\":1}}],[\"inherited\",{\"1\":{\"219\":1}}],[\"inherent\",{\"1\":{\"67\":1}}],[\"inhomogeneous\",{\"1\":{\"146\":2}}],[\"inhomogenous\",{\"0\":{\"145\":1}}],[\"independent\",{\"1\":{\"223\":1}}],[\"indexed\",{\"1\":{\"188\":1}}],[\"index\",{\"1\":{\"104\":1,\"199\":1,\"223\":2}}],[\"indicate\",{\"1\":{\"226\":2}}],[\"indicates\",{\"1\":{\"198\":1,\"226\":1,\"229\":4}}],[\"indicating\",{\"1\":{\"54\":1}}],[\"individual\",{\"1\":{\"190\":4,\"198\":1}}],[\"inductive\",{\"1\":{\"99\":1}}],[\"inefficient\",{\"1\":{\"87\":1,\"226\":1,\"227\":1}}],[\"incurred\",{\"1\":{\"223\":3}}],[\"inclusion\",{\"1\":{\"207\":3}}],[\"including\",{\"1\":{\"166\":1,\"167\":1,\"224\":1}}],[\"included\",{\"1\":{\"226\":1}}],[\"include\",{\"1\":{\"92\":1,\"100\":1,\"190\":1,\"218\":1,\"224\":1,\"227\":1}}],[\"includes\",{\"1\":{\"8\":1,\"134\":1,\"143\":1}}],[\"incorporate\",{\"1\":{\"216\":1}}],[\"incorporated\",{\"1\":{\"207\":2}}],[\"incorporates\",{\"1\":{\"207\":1}}],[\"incorporating\",{\"1\":{\"119\":1}}],[\"incompleteness\",{\"1\":{\"200\":1}}],[\"increase\",{\"1\":{\"190\":1,\"216\":1,\"223\":1}}],[\"increases\",{\"1\":{\"190\":1,\"197\":1}}],[\"increasing\",{\"1\":{\"81\":1,\"119\":1,\"191\":1}}],[\"incrementally\",{\"1\":{\"214\":1,\"229\":1}}],[\"increments\",{\"1\":{\"168\":2,\"213\":1}}],[\"increment\",{\"1\":{\"168\":2}}],[\"init\",{\"1\":{\"61\":2,\"104\":2}}],[\"initialization\",{\"1\":{\"218\":1}}],[\"initializing\",{\"1\":{\"122\":1,\"198\":1}}],[\"initially\",{\"1\":{\"215\":2,\"216\":1,\"226\":2}}],[\"initial\",{\"1\":{\"38\":1,\"44\":1,\"207\":2,\"212\":1,\"213\":1,\"216\":1,\"217\":2,\"218\":1,\"222\":1,\"226\":4,\"227\":1,\"229\":1}}],[\"inner\",{\"1\":{\"49\":1}}],[\"inputs=true\",{\"1\":{\"82\":1}}],[\"inputs\",{\"1\":{\"82\":2,\"93\":1,\"98\":1}}],[\"input\",{\"1\":{\"48\":1,\"49\":9,\"52\":2,\"54\":1,\"57\":2,\"59\":3,\"60\":1,\"61\":5,\"75\":1,\"87\":1,\"93\":1,\"104\":18,\"124\":1,\"221\":1}}],[\"invested\",{\"1\":{\"225\":1}}],[\"investigated\",{\"1\":{\"216\":1}}],[\"investigates\",{\"1\":{\"185\":1}}],[\"inversely\",{\"1\":{\"227\":1}}],[\"inverse\",{\"1\":{\"24\":1}}],[\"invariant\",{\"1\":{\"87\":1}}],[\"invariances\",{\"1\":{\"87\":1}}],[\"invariance\",{\"1\":{\"87\":2}}],[\"involving\",{\"1\":{\"86\":1,\"219\":1}}],[\"involved\",{\"1\":{\"199\":1}}],[\"involve\",{\"1\":{\"13\":1,\"207\":2}}],[\"involves\",{\"1\":{\"8\":1,\"44\":1,\"51\":1,\"120\":1,\"191\":1,\"200\":1,\"218\":1,\"226\":3}}],[\"inferior\",{\"1\":{\"196\":2}}],[\"inf\",{\"1\":{\"161\":1}}],[\"influenced\",{\"1\":{\"200\":1}}],[\"influence\",{\"1\":{\"89\":1}}],[\"information\",{\"0\":{\"143\":1},\"1\":{\"49\":5,\"52\":1,\"58\":1,\"60\":2,\"86\":1,\"88\":2,\"89\":3,\"103\":1,\"124\":1,\"154\":2}}],[\"infinite\",{\"1\":{\"17\":1,\"44\":1,\"200\":1,\"207\":2,\"212\":1,\"213\":1,\"222\":1}}],[\"infj\",{\"1\":{\"3\":1}}],[\"intro\",{\"1\":{\"208\":1}}],[\"introducing\",{\"1\":{\"29\":1}}],[\"introduces\",{\"1\":{\"94\":1}}],[\"introduced\",{\"1\":{\"45\":1,\"49\":1,\"203\":1,\"227\":1}}],[\"introduce\",{\"1\":{\"8\":1,\"20\":1,\"21\":1,\"24\":1,\"32\":1,\"37\":2,\"43\":1,\"48\":1,\"49\":1,\"57\":1,\"74\":1,\"146\":1,\"219\":1,\"221\":1,\"222\":1,\"223\":2}}],[\"introduction\",{\"0\":{\"0\":1,\"205\":1,\"211\":1},\"1\":{\"86\":1,\"194\":1}}],[\"integers\",{\"1\":{\"213\":1}}],[\"integer\",{\"1\":{\"213\":2}}],[\"integrating\",{\"1\":{\"203\":1}}],[\"integrates\",{\"1\":{\"167\":1,\"216\":1}}],[\"integrate\",{\"1\":{\"46\":1}}],[\"intelligent\",{\"1\":{\"184\":1}}],[\"interpreted\",{\"1\":{\"226\":1}}],[\"international\",{\"1\":{\"184\":1}}],[\"internal\",{\"1\":{\"124\":1}}],[\"intersection\",{\"0\":{\"184\":1},\"1\":{\"185\":4,\"193\":1}}],[\"interested\",{\"1\":{\"82\":1}}],[\"interesting\",{\"1\":{\"29\":1,\"213\":1}}],[\"interchangeable\",{\"1\":{\"206\":1}}],[\"interchange\",{\"0\":{\"72\":1}}],[\"intermediate\",{\"1\":{\"49\":2}}],[\"interimmediate\",{\"1\":{\"38\":1,\"40\":1}}],[\"interacting\",{\"1\":{\"18\":1}}],[\"interactions\",{\"1\":{\"49\":1,\"87\":1,\"144\":1}}],[\"interaction\",{\"1\":{\"8\":1,\"144\":1}}],[\"interacts\",{\"1\":{\"8\":1}}],[\"into\",{\"1\":{\"21\":1,\"23\":2,\"45\":2,\"46\":2,\"48\":1,\"49\":2,\"57\":1,\"59\":1,\"78\":2,\"89\":1,\"120\":2,\"121\":1,\"122\":1,\"123\":2,\"140\":2,\"143\":1,\"185\":1,\"190\":1,\"198\":1,\"200\":1,\"207\":3,\"218\":2,\"223\":2,\"226\":2,\"227\":1}}],[\"in\",{\"0\":{\"68\":1,\"118\":1,\"145\":1,\"168\":1,\"186\":1,\"229\":1},\"1\":{\"7\":1,\"8\":1,\"11\":2,\"12\":1,\"17\":2,\"18\":1,\"21\":2,\"23\":3,\"24\":3,\"25\":1,\"27\":1,\"30\":1,\"37\":2,\"38\":1,\"39\":5,\"41\":1,\"43\":1,\"44\":3,\"45\":10,\"46\":3,\"48\":1,\"49\":4,\"57\":1,\"59\":1,\"61\":1,\"81\":1,\"82\":2,\"83\":1,\"84\":2,\"88\":2,\"89\":8,\"103\":2,\"104\":7,\"111\":2,\"119\":2,\"120\":1,\"122\":1,\"124\":1,\"134\":2,\"140\":1,\"143\":1,\"144\":4,\"154\":2,\"167\":1,\"168\":2,\"185\":3,\"189\":1,\"190\":2,\"191\":1,\"192\":1,\"193\":1,\"196\":2,\"198\":1,\"199\":4,\"200\":2,\"203\":3,\"207\":10,\"213\":5,\"214\":2,\"215\":6,\"216\":2,\"217\":5,\"218\":4,\"219\":2,\"220\":1,\"221\":4,\"222\":4,\"223\":11,\"224\":5,\"225\":2,\"226\":22,\"227\":10,\"229\":14}}],[\"感谢你看到这里\",{\"1\":{\"4\":1}}],[\"❤️\",{\"1\":{\"4\":2}}],[\"博主的自我介绍\",{\"1\":{\"4\":1}}],[\"g₋₄\",{\"1\":{\"223\":1}}],[\"g₋₃\",{\"1\":{\"223\":1}}],[\"g₋₂\",{\"1\":{\"223\":1}}],[\"g₋₁\",{\"1\":{\"223\":1}}],[\"g₀\",{\"1\":{\"223\":1}}],[\"g∗\",{\"1\":{\"223\":2}}],[\"g₁\",{\"1\":{\"221\":1}}],[\"g₂\",{\"1\":{\"221\":1}}],[\"g₃\",{\"1\":{\"221\":1}}],[\"g₄\",{\"1\":{\"221\":1}}],[\"g₅\",{\"1\":{\"221\":1}}],[\"g5∗​\",{\"1\":{\"221\":2}}],[\"g3∗​\",{\"1\":{\"221\":5}}],[\"g4∗​\",{\"1\":{\"221\":5}}],[\"gf∗​\",{\"1\":{\"221\":1}}],[\"gf​\",{\"1\":{\"200\":1}}],[\"gk+1∗​\",{\"1\":{\"221\":1}}],[\"gk∗​\",{\"1\":{\"221\":4}}],[\"gn\",{\"1\":{\"199\":1}}],[\"gnn\",{\"0\":{\"90\":1},\"1\":{\"88\":1,\"103\":1}}],[\"gnns\",{\"1\":{\"86\":2,\"87\":1,\"88\":1,\"89\":1,\"103\":1}}],[\"g2​\",{\"1\":{\"223\":1}}],[\"g2∗​\",{\"1\":{\"221\":2}}],[\"g2\",{\"1\":{\"199\":1}}],[\"g1​\",{\"1\":{\"223\":1}}],[\"g1∗​\",{\"1\":{\"221\":2}}],[\"g1\",{\"1\":{\"199\":1}}],[\"gpt\",{\"1\":{\"167\":1}}],[\"gird\",{\"1\":{\"213\":1}}],[\"gini\",{\"1\":{\"124\":1}}],[\"giving\",{\"1\":{\"60\":1}}],[\"give\",{\"1\":{\"28\":1}}],[\"given\",{\"1\":{\"24\":1,\"38\":2,\"44\":2,\"45\":1,\"195\":1,\"223\":1,\"227\":1}}],[\"gives\",{\"1\":{\"11\":1,\"197\":1,\"215\":1}}],[\"gcns\",{\"1\":{\"91\":1,\"94\":2,\"99\":1,\"104\":3}}],[\"gcn\",{\"0\":{\"91\":1},\"1\":{\"91\":1,\"93\":2}}],[\"globally\",{\"1\":{\"224\":1}}],[\"global\",{\"1\":{\"87\":2,\"88\":1,\"89\":3,\"134\":1,\"154\":1,\"224\":1}}],[\"g=on\",{\"1\":{\"226\":1}}],[\"g=g\",{\"1\":{\"198\":1}}],[\"g=\",{\"1\":{\"87\":1}}],[\"game\",{\"1\":{\"196\":1,\"224\":1}}],[\"gat\",{\"0\":{\"94\":1},\"1\":{\"94\":2}}],[\"gather\",{\"1\":{\"89\":1}}],[\"gate\",{\"1\":{\"49\":8}}],[\"gates\",{\"1\":{\"49\":1}}],[\"gained\",{\"1\":{\"23\":1}}],[\"gain\",{\"1\":{\"18\":1,\"30\":2,\"39\":1,\"44\":1,\"46\":1,\"124\":1}}],[\"german\",{\"1\":{\"138\":1}}],[\"gentle\",{\"1\":{\"86\":1}}],[\"generating\",{\"1\":{\"167\":1}}],[\"generates\",{\"1\":{\"99\":1,\"207\":1}}],[\"generated\",{\"1\":{\"49\":1,\"59\":3,\"67\":1,\"198\":2}}],[\"generate\",{\"1\":{\"49\":3,\"58\":1,\"59\":2,\"66\":1,\"198\":1,\"200\":1,\"218\":1,\"227\":1}}],[\"generalization\",{\"1\":{\"109\":1,\"110\":1,\"111\":1,\"119\":1,\"223\":1,\"229\":1}}],[\"generalizing\",{\"1\":{\"99\":1}}],[\"generalized\",{\"1\":{\"45\":1,\"223\":1}}],[\"general\",{\"0\":{\"192\":1,\"215\":1,\"217\":1},\"1\":{\"46\":1,\"185\":1,\"192\":1,\"207\":1,\"215\":2,\"224\":1,\"226\":1,\"229\":1}}],[\"getfirst\",{\"1\":{\"215\":1,\"217\":3,\"224\":1}}],[\"getting\",{\"1\":{\"140\":1}}],[\"get\",{\"1\":{\"13\":1,\"23\":1,\"24\":1,\"25\":2,\"43\":1,\"89\":1,\"196\":1,\"222\":1}}],[\"guitar\",{\"0\":{\"236\":1}}],[\"guides\",{\"1\":{\"12\":1}}],[\"guide\",{\"1\":{\"7\":1,\"167\":1}}],[\"guarantees\",{\"1\":{\"200\":1,\"216\":1}}],[\"guarantee\",{\"1\":{\"46\":1}}],[\"guaranteed\",{\"1\":{\"44\":1}}],[\"gtj​​=i∈tj​max​g\",{\"1\":{\"190\":1}}],[\"gtj​​=i∈tj​∑​g\",{\"1\":{\"190\":1}}],[\"gtj​​\",{\"1\":{\"190\":1}}],[\"gt\",{\"2\":{\"170\":1,\"175\":1,\"179\":1,\"183\":1}}],[\"gt+1​∣st​=s\",{\"1\":{\"23\":1}}],[\"gt​∣st​=s\",{\"1\":{\"22\":1,\"25\":1}}],[\"grown\",{\"1\":{\"217\":2}}],[\"grouped\",{\"1\":{\"185\":1,\"190\":1}}],[\"gray\",{\"1\":{\"104\":1}}],[\"graphg=\",{\"1\":{\"188\":1}}],[\"graphsage\",{\"0\":{\"99\":1},\"1\":{\"99\":2,\"102\":1}}],[\"graphs\",{\"1\":{\"87\":1,\"88\":1,\"91\":1,\"99\":2}}],[\"graph\",{\"0\":{\"86\":1,\"87\":1},\"1\":{\"82\":4,\"86\":4,\"87\":14,\"88\":2,\"89\":7,\"91\":3,\"92\":1,\"94\":2,\"95\":1,\"99\":1,\"100\":1,\"104\":7,\"213\":1,\"214\":2,\"216\":1,\"218\":2,\"229\":2}}],[\"graph=true\",{\"1\":{\"82\":4,\"84\":1}}],[\"graph=false\",{\"1\":{\"82\":1}}],[\"graph=none\",{\"1\":{\"82\":1}}],[\"gradien\",{\"1\":{\"203\":1}}],[\"gradients\",{\"1\":{\"82\":8,\"203\":1}}],[\"gradient\",{\"0\":{\"70\":1,\"72\":1},\"1\":{\"49\":1,\"71\":1,\"74\":1,\"82\":6,\"83\":1,\"110\":2,\"203\":4}}],[\"gradually\",{\"1\":{\"83\":1}}],[\"grads=false\",{\"1\":{\"82\":1}}],[\"grads\",{\"1\":{\"82\":1}}],[\"grad\",{\"0\":{\"82\":1},\"1\":{\"82\":8,\"84\":11,\"104\":1}}],[\"grus\",{\"1\":{\"49\":1,\"59\":1}}],[\"greatest\",{\"1\":{\"121\":1}}],[\"greater\",{\"1\":{\"22\":2}}],[\"great\",{\"1\":{\"87\":1}}],[\"greedy\",{\"0\":{\"46\":1},\"1\":{\"46\":10,\"224\":1}}],[\"grids\",{\"1\":{\"86\":1}}],[\"grid\",{\"1\":{\"7\":3,\"213\":2}}],[\"going\",{\"1\":{\"216\":1,\"223\":1}}],[\"go=0\",{\"1\":{\"160\":1}}],[\"goes\",{\"1\":{\"23\":1,\"30\":1,\"34\":1,\"38\":1,\"45\":1,\"221\":1}}],[\"go\",{\"1\":{\"23\":1,\"34\":3,\"140\":4,\"216\":1,\"220\":1,\"221\":1,\"223\":5,\"224\":1}}],[\"governed\",{\"1\":{\"21\":4}}],[\"good\",{\"1\":{\"7\":2,\"28\":1}}],[\"goals\",{\"1\":{\"198\":1}}],[\"goal\",{\"1\":{\"7\":2,\"27\":1,\"122\":1,\"185\":1,\"189\":3,\"191\":1,\"192\":1,\"207\":2,\"212\":1,\"213\":1,\"216\":1,\"217\":2,\"223\":5,\"224\":5,\"226\":6,\"227\":2}}],[\"g\",{\"1\":{\"4\":1,\"16\":1,\"18\":1,\"52\":1,\"59\":1,\"69\":3,\"71\":2,\"72\":2,\"86\":1,\"87\":4,\"88\":1,\"93\":1,\"98\":1,\"101\":2,\"189\":2,\"198\":3,\"199\":1,\"215\":1,\"216\":2,\"217\":9,\"218\":2,\"223\":1,\"225\":1}}],[\"effects\",{\"1\":{\"226\":4,\"227\":1}}],[\"effectively\",{\"1\":{\"223\":1}}],[\"effort\",{\"1\":{\"225\":1}}],[\"efficiency\",{\"1\":{\"185\":1,\"224\":1}}],[\"efficient\",{\"1\":{\"45\":2,\"196\":1,\"207\":1,\"215\":1,\"217\":1,\"224\":3}}],[\"e⊆v×v\",{\"1\":{\"188\":1}}],[\"else\",{\"1\":{\"161\":1,\"215\":1,\"217\":3}}],[\"element\",{\"1\":{\"49\":1,\"54\":1,\"58\":1,\"199\":6,\"227\":1}}],[\"elements\",{\"1\":{\"29\":1,\"48\":1,\"51\":1,\"52\":2,\"78\":1,\"87\":1,\"199\":3}}],[\"elementwise\",{\"1\":{\"29\":1,\"34\":1}}],[\"ego\",{\"1\":{\"154\":1}}],[\"error\",{\"1\":{\"111\":1}}],[\"eye\",{\"1\":{\"104\":2}}],[\"e13​\",{\"1\":{\"98\":3}}],[\"e13​=leakyrelu\",{\"1\":{\"98\":1}}],[\"e12​\",{\"1\":{\"98\":3}}],[\"e12​=leakyrelu\",{\"1\":{\"98\":1}}],[\"ei\",{\"1\":{\"158\":2}}],[\"eigenvalues\",{\"1\":{\"121\":1}}],[\"eigenvectors\",{\"1\":{\"121\":1}}],[\"eik​\",{\"1\":{\"97\":1}}],[\"eij​=leakyrelu\",{\"1\":{\"96\":1}}],[\"eij​\",{\"1\":{\"96\":1,\"97\":2}}],[\"either\",{\"1\":{\"43\":1,\"143\":1,\"190\":1,\"207\":1,\"217\":1,\"226\":1}}],[\"employ\",{\"1\":{\"198\":1}}],[\"empty\",{\"1\":{\"143\":1,\"144\":2,\"215\":1,\"217\":2,\"218\":1,\"224\":1}}],[\"emmisions\",{\"1\":{\"140\":1}}],[\"embed\",{\"1\":{\"104\":1}}],[\"embedding\",{\"0\":{\"102\":1},\"1\":{\"101\":1,\"102\":1}}],[\"embeddings\",{\"1\":{\"89\":4,\"99\":1}}],[\"emotion\",{\"1\":{\"88\":1}}],[\"etc\",{\"1\":{\"88\":1}}],[\"epoch\",{\"1\":{\"110\":2}}],[\"epochs\",{\"1\":{\"81\":1}}],[\"episodic\",{\"1\":{\"18\":3}}],[\"episodes\",{\"1\":{\"18\":1,\"44\":2,\"45\":3,\"46\":2}}],[\"episode\",{\"0\":{\"18\":1},\"1\":{\"18\":2,\"45\":10,\"46\":1}}],[\"equilibria\",{\"1\":{\"224\":1}}],[\"equiped\",{\"1\":{\"154\":1}}],[\"equivalent\",{\"1\":{\"78\":1,\"222\":1}}],[\"equals\",{\"1\":{\"222\":1}}],[\"equally\",{\"1\":{\"48\":1,\"120\":1,\"197\":1}}],[\"equal\",{\"1\":{\"38\":1,\"191\":1,\"199\":4,\"216\":1}}],[\"equations\",{\"1\":{\"23\":1}}],[\"equation\",{\"0\":{\"20\":1,\"23\":1,\"27\":1,\"29\":1,\"31\":1},\"1\":{\"21\":1,\"23\":2,\"24\":1,\"27\":1,\"28\":1,\"29\":2,\"30\":4,\"33\":1,\"37\":1,\"38\":1,\"39\":2,\"43\":1,\"207\":1,\"213\":1,\"221\":4,\"222\":1,\"227\":1}}],[\"euclidean\",{\"1\":{\"69\":1}}],[\"essential\",{\"1\":{\"86\":1}}],[\"essentially\",{\"1\":{\"78\":1,\"82\":1,\"200\":1}}],[\"essence\",{\"1\":{\"60\":1}}],[\"estimates\",{\"1\":{\"203\":2,\"223\":1}}],[\"estimate\",{\"1\":{\"44\":4,\"45\":3,\"120\":1,\"203\":1,\"216\":1}}],[\"estimation\",{\"0\":{\"70\":1},\"1\":{\"43\":2,\"45\":1}}],[\"earlier\",{\"1\":{\"223\":1}}],[\"early\",{\"1\":{\"111\":1,\"218\":1,\"229\":1}}],[\"easily\",{\"1\":{\"221\":1,\"224\":1,\"226\":1,\"227\":1}}],[\"easier\",{\"1\":{\"200\":1}}],[\"easy\",{\"1\":{\"49\":1,\"213\":1}}],[\"each\",{\"1\":{\"7\":1,\"11\":1,\"25\":1,\"30\":1,\"44\":1,\"45\":1,\"49\":4,\"53\":1,\"54\":1,\"58\":1,\"82\":2,\"87\":1,\"88\":3,\"89\":2,\"100\":1,\"101\":1,\"103\":1,\"120\":1,\"121\":1,\"122\":1,\"124\":3,\"134\":2,\"143\":3,\"168\":2,\"185\":3,\"188\":1,\"189\":2,\"190\":2,\"192\":1,\"193\":1,\"195\":2,\"196\":2,\"198\":3,\"199\":2,\"203\":1,\"207\":1,\"212\":2,\"213\":2,\"216\":2,\"221\":3,\"222\":1,\"223\":1,\"224\":1,\"226\":3,\"227\":7,\"229\":5}}],[\"exsits\",{\"1\":{\"222\":1}}],[\"exhausted\",{\"1\":{\"216\":1,\"217\":1}}],[\"executed\",{\"1\":{\"207\":1}}],[\"execute\",{\"1\":{\"207\":1}}],[\"execution\",{\"1\":{\"207\":1}}],[\"exits\",{\"1\":{\"198\":1}}],[\"existing\",{\"1\":{\"226\":1}}],[\"exists\",{\"1\":{\"46\":1,\"195\":1,\"198\":1,\"199\":1,\"215\":1}}],[\"exist\",{\"1\":{\"28\":1}}],[\"existence\",{\"1\":{\"24\":1}}],[\"extending\",{\"1\":{\"227\":1,\"229\":1}}],[\"extended\",{\"1\":{\"223\":1}}],[\"extends\",{\"1\":{\"91\":1,\"224\":1}}],[\"extensive\",{\"1\":{\"154\":1}}],[\"external\",{\"1\":{\"144\":1}}],[\"extract\",{\"1\":{\"78\":1}}],[\"extraction\",{\"1\":{\"49\":1}}],[\"extreme\",{\"1\":{\"41\":1}}],[\"exchange\",{\"1\":{\"89\":1,\"103\":1}}],[\"except\",{\"1\":{\"44\":1,\"215\":1,\"224\":1}}],[\"exact\",{\"1\":{\"219\":1,\"223\":1}}],[\"exactly\",{\"1\":{\"44\":1}}],[\"exapmle\",{\"1\":{\"30\":2}}],[\"exapmles\",{\"1\":{\"29\":1}}],[\"examples\",{\"0\":{\"30\":1,\"213\":1},\"1\":{\"21\":1,\"23\":1,\"24\":1,\"32\":1,\"46\":1,\"224\":1}}],[\"example\",{\"0\":{\"61\":1},\"1\":{\"7\":1,\"8\":1,\"23\":2,\"25\":1,\"30\":3,\"32\":1,\"34\":4,\"41\":1,\"43\":2,\"46\":2,\"49\":1,\"82\":1,\"87\":1,\"89\":1,\"93\":1,\"98\":1,\"193\":1,\"197\":1,\"199\":1,\"213\":2,\"216\":1,\"221\":3,\"222\":1,\"223\":3,\"226\":5,\"227\":1,\"229\":3}}],[\"expansion\",{\"1\":{\"198\":2}}],[\"expand\",{\"1\":{\"199\":1,\"224\":1}}],[\"expanding\",{\"1\":{\"198\":1}}],[\"expands\",{\"1\":{\"78\":1,\"224\":1}}],[\"expensive\",{\"1\":{\"218\":1,\"224\":1}}],[\"expense\",{\"1\":{\"193\":1}}],[\"experimental\",{\"0\":{\"201\":1},\"1\":{\"203\":1}}],[\"expect\",{\"1\":{\"226\":1}}],[\"expected\",{\"1\":{\"111\":2}}],[\"expects\",{\"1\":{\"81\":1}}],[\"expectation\",{\"1\":{\"21\":1,\"22\":2,\"23\":2,\"43\":1,\"44\":1}}],[\"exp\",{\"1\":{\"97\":1,\"98\":2,\"161\":1}}],[\"explicit\",{\"1\":{\"229\":1}}],[\"explicitly\",{\"1\":{\"86\":1,\"207\":2,\"227\":1}}],[\"explanation\",{\"1\":{\"168\":1}}],[\"explores\",{\"1\":{\"224\":2}}],[\"explored\",{\"1\":{\"83\":1,\"199\":1,\"225\":1}}],[\"exploration\",{\"1\":{\"46\":1,\"199\":1,\"224\":1}}],[\"exploring\",{\"0\":{\"45\":1},\"1\":{\"45\":2,\"46\":4,\"83\":1}}],[\"expresses\",{\"1\":{\"111\":1}}],[\"expressed\",{\"1\":{\"59\":1,\"213\":1,\"226\":2,\"227\":1}}],[\"expressive\",{\"1\":{\"94\":1}}],[\"expression\",{\"1\":{\"29\":1,\"44\":1,\"219\":1}}],[\"express\",{\"1\":{\"49\":1,\"227\":1}}],[\"enough\",{\"1\":{\"229\":1}}],[\"enormous\",{\"1\":{\"225\":1,\"227\":1}}],[\"enables\",{\"1\":{\"223\":1}}],[\"enabling\",{\"1\":{\"166\":1,\"167\":1}}],[\"energy\",{\"1\":{\"219\":1}}],[\"ensuring\",{\"1\":{\"199\":1,\"229\":1}}],[\"ensure\",{\"1\":{\"120\":1,\"200\":1,\"207\":1}}],[\"english\",{\"0\":{\"118\":1,\"234\":1}}],[\"end\",{\"1\":{\"88\":2,\"160\":11,\"161\":14}}],[\"encountered\",{\"1\":{\"215\":1,\"217\":1,\"227\":1}}],[\"encourages\",{\"1\":{\"83\":1}}],[\"encodings\",{\"1\":{\"225\":1}}],[\"encoding\",{\"0\":{\"57\":1}}],[\"encoded\",{\"1\":{\"227\":3}}],[\"encode\",{\"1\":{\"154\":1,\"203\":1,\"227\":1}}],[\"encodes\",{\"1\":{\"49\":1,\"144\":1,\"218\":1,\"229\":1}}],[\"encoder\",{\"1\":{\"48\":1,\"49\":5,\"57\":1,\"58\":2}}],[\"enter\",{\"1\":{\"198\":1}}],[\"entering\",{\"1\":{\"18\":1,\"34\":1}}],[\"entropy\",{\"1\":{\"124\":1}}],[\"entities\",{\"1\":{\"86\":1,\"87\":1}}],[\"entire\",{\"1\":{\"49\":1,\"87\":1,\"88\":2,\"99\":1,\"192\":1,\"224\":1,\"225\":1}}],[\"environment\",{\"1\":{\"7\":1,\"8\":2,\"9\":1,\"18\":1,\"188\":1,\"193\":2}}],[\"evacuation\",{\"1\":{\"154\":1}}],[\"evaluation\",{\"1\":{\"24\":1,\"25\":1,\"39\":1,\"43\":1,\"44\":3,\"45\":1}}],[\"evaluated\",{\"1\":{\"120\":1}}],[\"evaluate\",{\"1\":{\"16\":1,\"24\":1,\"28\":1,\"110\":1,\"193\":1,\"226\":1}}],[\"even\",{\"1\":{\"45\":1,\"197\":1,\"199\":1}}],[\"everything\",{\"1\":{\"22\":1}}],[\"every\",{\"1\":{\"7\":1,\"44\":2,\"45\":3,\"46\":4,\"104\":1,\"134\":1,\"144\":1,\"198\":1,\"200\":1,\"215\":2,\"219\":1,\"223\":1,\"227\":2}}],[\"e\",{\"1\":{\"4\":1,\"16\":1,\"18\":1,\"22\":1,\"23\":1,\"43\":3,\"52\":1,\"53\":3,\"59\":1,\"66\":1,\"67\":2,\"75\":1,\"81\":2,\"82\":1,\"86\":1,\"87\":5,\"88\":1,\"93\":1,\"98\":1,\"101\":2,\"188\":1,\"192\":1,\"195\":2,\"198\":3,\"218\":1,\"221\":7,\"222\":3,\"223\":1,\"225\":1}}],[\"edge\",{\"0\":{\"165\":1},\"1\":{\"87\":2,\"88\":4,\"89\":13,\"104\":1,\"189\":2,\"198\":1,\"218\":3,\"221\":3,\"229\":1}}],[\"edges\",{\"1\":{\"86\":1,\"87\":4,\"88\":1,\"89\":2,\"188\":1,\"189\":1,\"221\":1}}],[\"edg\",{\"1\":{\"3\":1}}],[\"日常\",{\"1\":{\"4\":1}}],[\"纪录读研阶段学习内容\",{\"1\":{\"4\":1}}],[\"路径规划\",{\"1\":{\"4\":1}}],[\"笔记及代码\",{\"1\":{\"4\":1}}],[\"机器学习三个主要分类是什么\",{\"1\":{\"110\":1}}],[\"机器学习流程\",{\"1\":{\"110\":1}}],[\"机器学习是研究计算机怎样模拟或实现人类的学习行为\",{\"1\":{\"110\":1}}],[\"机器学习是一种实现人工智能的方法\",{\"1\":{\"110\":1}}],[\"机器学习\",{\"0\":{\"108\":1},\"1\":{\"4\":1}}],[\"乐评\",{\"1\":{\"4\":1}}],[\"有人\",{\"1\":{\"161\":1}}],[\"有时候很难找到一个合适的核函数\",{\"1\":{\"115\":1}}],[\"有时间也会加入书评\",{\"1\":{\"4\":1}}],[\"有导师的学习\",{\"1\":{\"114\":1}}],[\"有欠拟合风险\",{\"1\":{\"113\":1}}],[\"有些分支当前划分虽然不能提升泛化性能\",{\"1\":{\"113\":1}}],[\"有和环境交互的能力\",{\"1\":{\"110\":1}}],[\"有点变化\",{\"1\":{\"3\":1}}],[\"自动驾驶系统所需具良好的泛化能力\",{\"1\":{\"167\":1}}],[\"自主车辆\",{\"1\":{\"154\":1}}],[\"自底向上对所有非叶节点逐一考察\",{\"1\":{\"113\":1}}],[\"自助法主要面向数据集同规模的划分问题\",{\"1\":{\"111\":1}}],[\"自助法等\",{\"1\":{\"111\":1}}],[\"自编码器\",{\"1\":{\"110\":1}}],[\"自学内容的整理等\",{\"1\":{\"4\":1}}],[\"自我感觉是一个矛盾体\",{\"1\":{\"3\":1}}],[\"自我评价\",{\"1\":{\"3\":1}}],[\"国足进世界杯\",{\"1\":{\"3\":1}}],[\"国家队比赛\",{\"1\":{\"3\":1}}],[\"统一\",{\"1\":{\"3\":1}}],[\"tc\",{\"0\":{\"199\":1,\"200\":1},\"1\":{\"197\":1,\"199\":6,\"200\":6}}],[\"tcpf\",{\"0\":{\"192\":1},\"1\":{\"185\":3,\"191\":1,\"192\":3,\"193\":1}}],[\"tj​​g\",{\"1\":{\"200\":1}}],[\"tj​=i\",{\"1\":{\"192\":1}}],[\"tj​⊆i\",{\"1\":{\"190\":1}}],[\"tj​\",{\"1\":{\"190\":2,\"195\":1}}],[\"tile\",{\"1\":{\"213\":3}}],[\"tiles\",{\"1\":{\"207\":1}}],[\"ties\",{\"1\":{\"199\":1}}],[\"tight\",{\"1\":{\"104\":1}}],[\"title\",{\"1\":{\"104\":1}}],[\"times\",{\"1\":{\"43\":1,\"45\":1,\"46\":1,\"49\":1,\"120\":2,\"185\":1,\"207\":1,\"229\":1}}],[\"time\",{\"1\":{\"25\":1,\"49\":4,\"59\":6,\"81\":1,\"83\":1,\"120\":1,\"134\":1,\"140\":1,\"143\":1,\"148\":1,\"160\":1,\"185\":2,\"189\":2,\"191\":1,\"193\":3,\"198\":1,\"200\":1,\"207\":4,\"216\":2,\"219\":1,\"224\":2,\"227\":1}}],[\"t1\",{\"1\":{\"104\":8}}],[\"tsp\",{\"1\":{\"69\":3,\"71\":1,\"203\":2}}],[\"types\",{\"1\":{\"88\":1,\"103\":1,\"189\":1}}],[\"type\",{\"1\":{\"61\":1,\"104\":1}}],[\"typically\",{\"1\":{\"51\":1,\"59\":1}}],[\"tuple\",{\"1\":{\"198\":1}}],[\"tune\",{\"1\":{\"109\":1}}],[\"tuning\",{\"1\":{\"81\":1,\"110\":1,\"166\":1}}],[\"turn\",{\"1\":{\"46\":1}}],[\"turned\",{\"1\":{\"46\":1}}],[\"tutorial\",{\"1\":{\"5\":1,\"18\":1,\"21\":1,\"23\":1,\"37\":1}}],[\"t\",{\"0\":{\"200\":1},\"1\":{\"21\":1,\"57\":1,\"58\":1,\"82\":2,\"87\":1,\"98\":2,\"111\":1,\"161\":4,\"196\":1,\"198\":5,\"200\":3,\"223\":1,\"227\":1}}],[\"trees\",{\"1\":{\"217\":1,\"218\":1,\"226\":1}}],[\"tree\",{\"0\":{\"124\":1},\"1\":{\"110\":1,\"124\":3,\"198\":2,\"207\":3,\"216\":1,\"217\":1,\"218\":1}}],[\"treated\",{\"1\":{\"213\":1}}],[\"treating\",{\"1\":{\"48\":1}}],[\"treat\",{\"1\":{\"18\":4}}],[\"trucks\",{\"1\":{\"148\":1}}],[\"true\",{\"1\":{\"82\":1,\"83\":1,\"223\":1,\"226\":2}}],[\"truncated\",{\"0\":{\"41\":1},\"1\":{\"37\":1,\"41\":5}}],[\"trivial\",{\"1\":{\"168\":1,\"218\":1}}],[\"triggering\",{\"1\":{\"134\":1}}],[\"triu\",{\"1\":{\"104\":1}}],[\"triangular\",{\"1\":{\"104\":1}}],[\"trial\",{\"1\":{\"18\":1}}],[\"trick\",{\"0\":{\"106\":1},\"1\":{\"71\":1,\"203\":1}}],[\"trying\",{\"1\":{\"88\":1}}],[\"try\",{\"1\":{\"34\":1,\"222\":1}}],[\"traffic\",{\"0\":{\"140\":1,\"145\":1,\"184\":1,\"233\":1},\"1\":{\"138\":1,\"140\":3,\"154\":1,\"167\":1},\"2\":{\"152\":1,\"164\":1}}],[\"traveling\",{\"0\":{\"203\":1},\"1\":{\"203\":1}}],[\"travelling\",{\"1\":{\"7\":1}}],[\"traversal\",{\"1\":{\"185\":3,\"191\":1,\"193\":2}}],[\"traversing\",{\"1\":{\"124\":1}}],[\"trained\",{\"1\":{\"120\":1,\"166\":1}}],[\"train\",{\"1\":{\"110\":2}}],[\"trainable\",{\"1\":{\"93\":1,\"101\":1}}],[\"training\",{\"1\":{\"53\":1,\"81\":1,\"99\":1,\"111\":1,\"120\":2,\"166\":1,\"167\":1,\"203\":1}}],[\"traditional\",{\"1\":{\"86\":1,\"203\":2}}],[\"tradeoffs\",{\"1\":{\"185\":1}}],[\"trade\",{\"0\":{\"194\":1},\"1\":{\"41\":1,\"185\":1,\"193\":1,\"196\":3,\"197\":2}}],[\"track\",{\"1\":{\"82\":1,\"214\":1}}],[\"transpose\",{\"1\":{\"61\":1,\"104\":1}}],[\"translation\",{\"1\":{\"59\":1}}],[\"transformer\",{\"1\":{\"166\":2}}],[\"transformed\",{\"1\":{\"93\":1,\"200\":1}}],[\"transform\",{\"1\":{\"123\":1}}],[\"transforming\",{\"1\":{\"49\":1,\"57\":1}}],[\"transforms\",{\"1\":{\"49\":1,\"121\":1,\"207\":1,\"213\":1}}],[\"transformations\",{\"1\":{\"87\":1}}],[\"transformation\",{\"1\":{\"49\":1,\"59\":1,\"61\":2,\"78\":1,\"96\":1,\"200\":2}}],[\"transited\",{\"1\":{\"21\":1}}],[\"transiting\",{\"1\":{\"14\":1}}],[\"transition\",{\"1\":{\"8\":2,\"13\":1,\"23\":1,\"25\":1,\"158\":1,\"212\":1,\"213\":2,\"214\":1,\"219\":1,\"221\":1,\"227\":1}}],[\"transit\",{\"1\":{\"13\":1,\"16\":1}}],[\"trajectory\",{\"0\":{\"16\":1},\"1\":{\"16\":3,\"18\":2,\"154\":1}}],[\"teams\",{\"0\":{\"190\":1},\"1\":{\"185\":5,\"190\":3,\"191\":2,\"193\":2,\"195\":3,\"196\":1,\"197\":2,\"199\":1,\"200\":1}}],[\"team\",{\"1\":{\"185\":4,\"190\":6,\"191\":5,\"192\":2,\"193\":5,\"194\":2,\"195\":3,\"196\":1,\"197\":9,\"199\":6,\"200\":1}}],[\"teamwise\",{\"0\":{\"184\":1,\"199\":1},\"1\":{\"185\":1}}],[\"technology\",{\"0\":{\"165\":1}}],[\"techniques\",{\"1\":{\"166\":1,\"185\":1,\"227\":1}}],[\"technique\",{\"1\":{\"48\":1,\"119\":1,\"120\":1,\"121\":1}}],[\"tests\",{\"1\":{\"218\":1}}],[\"tested\",{\"1\":{\"185\":1}}],[\"test\",{\"1\":{\"110\":1,\"167\":1,\"215\":1}}],[\"template\",{\"1\":{\"215\":2,\"216\":1,\"229\":1}}],[\"temp\",{\"1\":{\"84\":1}}],[\"tell\",{\"1\":{\"215\":1}}],[\"telling\",{\"1\":{\"82\":2}}],[\"tells\",{\"1\":{\"11\":1,\"223\":1}}],[\"tensor\",{\"1\":{\"61\":3,\"81\":1,\"82\":1}}],[\"termination\",{\"1\":{\"218\":1,\"223\":4}}],[\"terminate\",{\"1\":{\"200\":1}}],[\"terminates\",{\"1\":{\"199\":1,\"200\":1,\"217\":1}}],[\"terminal\",{\"1\":{\"18\":1}}],[\"terms\",{\"1\":{\"119\":1}}],[\"term\",{\"1\":{\"49\":1,\"219\":3}}],[\"termdependencies\",{\"1\":{\"49\":1}}],[\"text\",{\"1\":{\"48\":1,\"49\":1,\"88\":2,\"167\":1}}],[\"two\",{\"1\":{\"13\":1,\"20\":1,\"23\":2,\"24\":1,\"29\":1,\"30\":2,\"38\":1,\"40\":1,\"41\":1,\"44\":2,\"45\":2,\"46\":1,\"188\":1,\"189\":3,\"195\":1,\"196\":1,\"197\":1,\"198\":2,\"199\":2,\"213\":1,\"215\":1,\"217\":1,\"218\":1,\"220\":1,\"221\":1,\"223\":2,\"226\":3,\"227\":1}}],[\"twin\",{\"1\":{\"3\":1}}],[\"table\",{\"1\":{\"221\":1,\"226\":3}}],[\"tabular\",{\"1\":{\"11\":2}}],[\"task\",{\"0\":{\"88\":1},\"1\":{\"49\":1,\"88\":2,\"111\":1}}],[\"tasks\",{\"1\":{\"18\":5,\"86\":1,\"88\":2,\"91\":1,\"123\":1,\"124\":1,\"166\":1}}],[\"tail\",{\"1\":{\"43\":2}}],[\"taking\",{\"1\":{\"7\":1,\"10\":1,\"13\":2,\"21\":1,\"23\":2,\"25\":2,\"223\":1}}],[\"takes\",{\"1\":{\"168\":2,\"213\":1,\"223\":1}}],[\"taken\",{\"1\":{\"11\":1,\"23\":1,\"207\":1}}],[\"take\",{\"1\":{\"7\":1,\"11\":1,\"21\":1,\"46\":2,\"83\":1,\"188\":1,\"207\":1,\"221\":2,\"226\":1,\"229\":1}}],[\"target=\",{\"1\":{\"49\":1}}],[\"target\",{\"0\":{\"65\":1},\"1\":{\"7\":2,\"12\":1,\"16\":2,\"18\":5,\"34\":1,\"49\":2,\"124\":1}}],[\"talk\",{\"1\":{\"1\":1}}],[\"toss\",{\"1\":{\"196\":1}}],[\"together\",{\"1\":{\"192\":1}}],[\"total=960\",{\"1\":{\"160\":1}}],[\"total\",{\"1\":{\"81\":1,\"160\":1,\"193\":1,\"223\":6,\"227\":1}}],[\"tours\",{\"1\":{\"203\":1}}],[\"tour\",{\"1\":{\"75\":1,\"203\":3}}],[\"torch\",{\"0\":{\"81\":1,\"82\":1},\"1\":{\"61\":14,\"81\":3,\"82\":2,\"84\":2,\"104\":19}}],[\"token\",{\"1\":{\"49\":1,\"52\":1}}],[\"topological\",{\"1\":{\"86\":1,\"203\":1}}],[\"top\",{\"1\":{\"49\":1,\"121\":1}}],[\"tools\",{\"1\":{\"86\":1}}],[\"too\",{\"1\":{\"46\":1,\"83\":1,\"110\":2,\"140\":1,\"224\":1}}],[\"to\",{\"0\":{\"24\":1,\"197\":1,\"211\":1,\"225\":1,\"227\":1},\"1\":{\"7\":6,\"8\":1,\"9\":1,\"11\":4,\"12\":1,\"13\":3,\"14\":1,\"16\":2,\"17\":2,\"18\":3,\"21\":1,\"23\":1,\"24\":6,\"25\":4,\"27\":1,\"28\":2,\"29\":3,\"30\":8,\"33\":2,\"34\":1,\"35\":1,\"38\":1,\"39\":1,\"40\":3,\"43\":6,\"44\":5,\"45\":12,\"46\":7,\"48\":5,\"49\":12,\"53\":2,\"54\":3,\"55\":2,\"58\":2,\"59\":3,\"60\":2,\"61\":6,\"74\":1,\"78\":3,\"81\":3,\"82\":14,\"83\":6,\"86\":2,\"87\":4,\"88\":12,\"89\":11,\"91\":2,\"92\":2,\"94\":4,\"95\":1,\"99\":2,\"100\":1,\"104\":5,\"119\":4,\"120\":1,\"121\":2,\"122\":4,\"123\":4,\"124\":3,\"140\":1,\"143\":1,\"144\":3,\"146\":2,\"166\":1,\"167\":2,\"185\":8,\"189\":2,\"190\":3,\"191\":2,\"192\":4,\"193\":4,\"194\":4,\"196\":1,\"197\":1,\"198\":3,\"199\":7,\"200\":4,\"203\":11,\"207\":8,\"213\":3,\"215\":2,\"216\":5,\"217\":1,\"218\":4,\"219\":3,\"220\":4,\"221\":11,\"222\":8,\"223\":27,\"224\":20,\"225\":3,\"226\":22,\"227\":14,\"228\":2,\"229\":16}}],[\"things\",{\"1\":{\"226\":1}}],[\"third\",{\"1\":{\"199\":2,\"207\":1}}],[\"this\",{\"0\":{\"186\":1,\"208\":1},\"1\":{\"5\":2,\"16\":1,\"18\":1,\"20\":1,\"21\":2,\"24\":1,\"25\":1,\"27\":1,\"30\":1,\"37\":2,\"38\":2,\"39\":1,\"40\":1,\"43\":2,\"44\":1,\"45\":4,\"48\":1,\"49\":2,\"57\":1,\"59\":2,\"81\":3,\"82\":3,\"83\":1,\"87\":1,\"89\":5,\"94\":1,\"99\":1,\"119\":1,\"120\":1,\"121\":1,\"122\":1,\"123\":1,\"144\":1,\"184\":1,\"185\":1,\"192\":1,\"194\":1,\"196\":1,\"200\":1,\"203\":4,\"207\":1,\"213\":1,\"214\":1,\"215\":1,\"216\":1,\"217\":1,\"218\":2,\"219\":3,\"220\":2,\"221\":3,\"223\":8,\"224\":1,\"226\":2,\"227\":6,\"228\":1,\"229\":1}}],[\"th\",{\"1\":{\"93\":2,\"101\":1}}],[\"though\",{\"1\":{\"197\":1,\"199\":1}}],[\"thought\",{\"1\":{\"25\":1}}],[\"those\",{\"1\":{\"195\":1}}],[\"thorough\",{\"1\":{\"44\":1}}],[\"through\",{\"1\":{\"27\":1,\"29\":1,\"30\":1,\"39\":3,\"40\":1,\"43\":1,\"49\":1,\"54\":1,\"59\":3,\"71\":1,\"82\":1,\"86\":1,\"89\":1,\"185\":1,\"189\":1,\"207\":2,\"214\":1,\"221\":1}}],[\"three\",{\"1\":{\"8\":1,\"21\":1,\"37\":1,\"49\":2,\"59\":1,\"61\":3,\"88\":1,\"89\":1,\"197\":1,\"207\":1,\"215\":1}}],[\"thus\",{\"1\":{\"21\":1,\"24\":1,\"46\":1,\"185\":2,\"198\":1,\"199\":1,\"222\":2,\"224\":1,\"226\":1}}],[\"that\",{\"1\":{\"21\":1,\"22\":2,\"23\":4,\"24\":2,\"25\":2,\"27\":1,\"30\":4,\"32\":1,\"38\":2,\"39\":2,\"43\":2,\"44\":2,\"45\":3,\"46\":4,\"49\":1,\"52\":2,\"60\":1,\"66\":1,\"67\":1,\"78\":1,\"81\":1,\"82\":1,\"83\":1,\"87\":1,\"88\":2,\"89\":1,\"104\":1,\"111\":1,\"120\":1,\"121\":2,\"122\":1,\"123\":1,\"124\":1,\"144\":1,\"167\":1,\"168\":3,\"185\":1,\"190\":1,\"191\":1,\"193\":1,\"196\":2,\"197\":1,\"198\":1,\"199\":3,\"200\":2,\"203\":1,\"207\":6,\"213\":4,\"214\":3,\"215\":5,\"216\":3,\"217\":1,\"219\":1,\"221\":3,\"222\":4,\"223\":7,\"224\":3,\"225\":2,\"226\":14,\"227\":9,\"229\":13}}],[\"than\",{\"1\":{\"14\":1,\"28\":1,\"48\":1,\"49\":1,\"82\":1,\"89\":1,\"94\":1,\"144\":1,\"185\":1,\"195\":1,\"199\":4,\"222\":1,\"224\":1}}],[\"theoretic\",{\"1\":{\"208\":1}}],[\"theoretical\",{\"1\":{\"138\":1}}],[\"theorem\",{\"1\":{\"32\":2,\"33\":2,\"38\":1}}],[\"theory\",{\"1\":{\"127\":1,\"134\":1}}],[\"they\",{\"1\":{\"86\":1,\"196\":1,\"197\":1,\"199\":1,\"215\":1,\"221\":1,\"226\":1,\"229\":1}}],[\"theme\",{\"1\":{\"168\":2}}],[\"them\",{\"1\":{\"46\":1,\"48\":1,\"49\":1,\"57\":1,\"60\":1,\"89\":3,\"122\":1,\"166\":1,\"199\":1,\"221\":3}}],[\"therefore\",{\"1\":{\"216\":1,\"225\":1}}],[\"thereby\",{\"1\":{\"134\":1}}],[\"there\",{\"1\":{\"23\":1,\"44\":1,\"45\":2,\"46\":2,\"144\":1,\"185\":1,\"188\":1,\"193\":1,\"195\":1,\"196\":1,\"197\":1,\"198\":2,\"199\":1,\"207\":2,\"215\":3,\"218\":3,\"220\":1,\"221\":2,\"222\":1,\"223\":1,\"224\":2,\"226\":1,\"227\":2,\"229\":1}}],[\"then\",{\"1\":{\"22\":2,\"39\":1,\"40\":2,\"43\":4,\"45\":1,\"46\":1,\"82\":2,\"122\":1,\"185\":1,\"195\":1,\"198\":2,\"199\":1,\"213\":1,\"216\":1,\"218\":3,\"224\":1,\"227\":1}}],[\"their\",{\"1\":{\"21\":1,\"44\":2,\"95\":1,\"104\":1,\"134\":1,\"185\":2,\"221\":1}}],[\"these\",{\"1\":{\"21\":1,\"45\":1,\"54\":1,\"88\":1,\"166\":1,\"193\":1,\"194\":1,\"215\":2,\"216\":1,\"229\":1}}],[\"the\",{\"0\":{\"8\":1,\"31\":1,\"35\":1,\"197\":2,\"218\":1,\"227\":1},\"1\":{\"5\":1,\"7\":9,\"8\":9,\"9\":4,\"10\":3,\"11\":2,\"12\":8,\"13\":2,\"14\":2,\"16\":4,\"17\":11,\"18\":12,\"21\":9,\"22\":12,\"23\":16,\"24\":6,\"25\":12,\"27\":1,\"28\":8,\"29\":3,\"30\":7,\"32\":1,\"33\":4,\"34\":2,\"35\":1,\"37\":5,\"38\":10,\"39\":10,\"40\":6,\"41\":4,\"43\":18,\"44\":14,\"45\":29,\"46\":21,\"48\":5,\"49\":42,\"51\":2,\"52\":10,\"53\":11,\"54\":3,\"55\":4,\"57\":8,\"58\":11,\"59\":34,\"60\":6,\"61\":11,\"65\":1,\"67\":2,\"78\":11,\"81\":20,\"82\":27,\"83\":16,\"84\":2,\"87\":14,\"88\":4,\"89\":10,\"92\":3,\"93\":4,\"94\":2,\"95\":2,\"96\":2,\"97\":2,\"98\":2,\"99\":1,\"100\":2,\"101\":2,\"102\":1,\"103\":2,\"104\":6,\"110\":5,\"111\":14,\"119\":6,\"120\":9,\"121\":10,\"122\":7,\"123\":10,\"124\":10,\"134\":3,\"143\":4,\"144\":8,\"146\":4,\"154\":2,\"167\":2,\"168\":3,\"185\":16,\"188\":4,\"189\":11,\"190\":11,\"191\":4,\"192\":5,\"193\":6,\"194\":2,\"195\":4,\"196\":7,\"197\":8,\"198\":13,\"199\":13,\"200\":11,\"203\":26,\"206\":2,\"207\":30,\"213\":10,\"214\":6,\"215\":6,\"216\":10,\"217\":9,\"218\":5,\"219\":17,\"220\":2,\"221\":24,\"222\":12,\"223\":62,\"224\":11,\"225\":4,\"226\":47,\"227\":34,\"228\":2,\"229\":26}}],[\"冰岛\",{\"1\":{\"3\":1}}],[\"欧洲游\",{\"1\":{\"3\":1}}],[\"欧洲杯\",{\"1\":{\"3\":1}}],[\"欧冠\",{\"1\":{\"3\":1}}],[\"亚冠\",{\"1\":{\"3\":1}}],[\"现场看一次球赛\",{\"1\":{\"3\":1}}],[\"现场看一场\",{\"1\":{\"3\":1}}],[\"看一次霉妈的演唱会\",{\"1\":{\"3\":1}}],[\"看待问题十分的现实\",{\"1\":{\"3\":1}}],[\"m和chowdhury\",{\"1\":{\"150\":1}}],[\"m\",{\"1\":{\"111\":1,\"143\":1,\"150\":1,\"190\":1,\"195\":1,\"205\":2}}],[\"mnist\",{\"1\":{\"88\":1}}],[\"mi\",{\"1\":{\"172\":1}}],[\"microscopic\",{\"1\":{\"154\":1}}],[\"michael\",{\"1\":{\"138\":2}}],[\"might\",{\"1\":{\"83\":2,\"196\":1,\"197\":2,\"217\":2,\"226\":2,\"227\":1}}],[\"min​\",{\"1\":{\"223\":2}}],[\"min\",{\"0\":{\"203\":1},\"1\":{\"190\":2,\"193\":2,\"203\":1,\"224\":1}}],[\"minimizing\",{\"1\":{\"223\":1}}],[\"minimizes\",{\"1\":{\"223\":1,\"224\":1}}],[\"minimize\",{\"1\":{\"12\":1,\"74\":1,\"122\":1,\"185\":1,\"190\":3,\"192\":1,\"193\":2,\"203\":1,\"223\":3}}],[\"minimum\",{\"1\":{\"83\":2,\"198\":1,\"224\":1}}],[\"minl=maxd\",{\"1\":{\"69\":1,\"71\":1}}],[\"minor\",{\"1\":{\"49\":1}}],[\"mtsp\",{\"1\":{\"69\":1,\"203\":2}}],[\"multi\",{\"0\":{\"184\":1},\"1\":{\"185\":3,\"192\":1}}],[\"multiplication\",{\"1\":{\"78\":1,\"93\":1}}],[\"multiple\",{\"0\":{\"203\":1},\"1\":{\"45\":1,\"49\":1,\"120\":1,\"185\":2,\"190\":1,\"191\":1,\"193\":1,\"196\":1,\"199\":1,\"203\":1,\"224\":1,\"229\":2}}],[\"must\",{\"1\":{\"67\":1,\"185\":1,\"189\":1,\"207\":1,\"214\":2,\"222\":2,\"226\":3,\"227\":1,\"229\":2}}],[\"much\",{\"1\":{\"21\":1,\"82\":1,\"196\":1,\"224\":1}}],[\"mc\",{\"0\":{\"44\":1,\"45\":1,\"46\":1},\"1\":{\"44\":3,\"45\":6,\"46\":1}}],[\"meet\",{\"1\":{\"217\":1}}],[\"mechanical\",{\"1\":{\"207\":1}}],[\"mechanisms\",{\"1\":{\"49\":1,\"53\":1}}],[\"mechanism\",{\"0\":{\"48\":1,\"96\":1},\"1\":{\"48\":2,\"51\":1,\"55\":1,\"58\":1,\"59\":2,\"60\":2,\"94\":1,\"95\":1,\"96\":1,\"203\":1,\"223\":1}}],[\"mesoscopic\",{\"1\":{\"154\":1}}],[\"messages\",{\"1\":{\"89\":3}}],[\"message\",{\"0\":{\"89\":1},\"1\":{\"89\":6,\"203\":1}}],[\"metric\",{\"1\":{\"224\":1}}],[\"metrics\",{\"1\":{\"124\":1}}],[\"method\",{\"0\":{\"66\":1},\"1\":{\"45\":4,\"48\":1,\"111\":1,\"120\":1,\"216\":1,\"222\":1,\"223\":1,\"224\":1,\"228\":1}}],[\"methods\",{\"0\":{\"216\":1,\"218\":1,\"228\":1},\"1\":{\"43\":1,\"45\":2,\"59\":1,\"81\":1,\"86\":1,\"214\":1,\"218\":1,\"229\":2}}],[\"measurements\",{\"1\":{\"81\":1}}],[\"measured\",{\"1\":{\"60\":1,\"124\":1,\"219\":1}}],[\"measures\",{\"1\":{\"53\":2,\"111\":2}}],[\"meaningless\",{\"1\":{\"226\":1}}],[\"meaning\",{\"1\":{\"190\":1,\"192\":1,\"223\":1}}],[\"mean\",{\"1\":{\"22\":1,\"43\":2,\"84\":1,\"100\":1,\"101\":1,\"122\":1}}],[\"means\",{\"0\":{\"122\":1},\"1\":{\"12\":1,\"45\":1,\"82\":1,\"83\":1,\"116\":7,\"122\":1,\"196\":1,\"216\":1,\"219\":1,\"223\":1,\"226\":1,\"227\":1}}],[\"memory\",{\"1\":{\"49\":1,\"82\":1}}],[\"memoryless\",{\"1\":{\"14\":1}}],[\"mentioned\",{\"1\":{\"21\":1,\"30\":1,\"44\":1,\"227\":1}}],[\"mdp\",{\"1\":{\"8\":1}}],[\"momentum\",{\"1\":{\"207\":1}}],[\"monotonicity\",{\"1\":{\"168\":1}}],[\"monte\",{\"0\":{\"43\":1,\"44\":1,\"45\":1,\"46\":1},\"1\":{\"43\":1}}],[\"motivation\",{\"1\":{\"224\":1}}],[\"motivating\",{\"0\":{\"30\":1},\"1\":{\"21\":1}}],[\"motions\",{\"1\":{\"207\":1,\"213\":1}}],[\"motion\",{\"1\":{\"154\":1,\"185\":1,\"208\":1}}],[\"moore邻居选择示意图\",{\"1\":{\"158\":1}}],[\"moore\",{\"1\":{\"130\":2,\"133\":2,\"157\":1}}],[\"mood\",{\"1\":{\"88\":1}}],[\"molecules\",{\"1\":{\"87\":1}}],[\"moves\",{\"1\":{\"188\":1,\"213\":1}}],[\"movement\",{\"1\":{\"154\":1}}],[\"move\",{\"1\":{\"140\":1,\"207\":1}}],[\"movable\",{\"1\":{\"78\":1}}],[\"moving\",{\"1\":{\"7\":1,\"189\":1,\"213\":1}}],[\"modifications\",{\"1\":{\"224\":1}}],[\"modified\",{\"1\":{\"200\":1}}],[\"module\",{\"1\":{\"61\":1,\"104\":1}}],[\"modeled\",{\"1\":{\"207\":1}}],[\"modeling\",{\"1\":{\"154\":1}}],[\"modelling\",{\"1\":{\"49\":1}}],[\"models\",{\"1\":{\"44\":1,\"86\":1,\"166\":3}}],[\"model\",{\"0\":{\"138\":1,\"142\":1,\"143\":1,\"144\":1,\"145\":1},\"1\":{\"8\":1,\"35\":2,\"37\":1,\"43\":7,\"44\":5,\"48\":2,\"52\":3,\"53\":1,\"81\":1,\"83\":4,\"86\":1,\"88\":1,\"94\":1,\"119\":2,\"120\":4,\"124\":1,\"134\":2,\"138\":5,\"144\":1,\"154\":2,\"166\":1,\"203\":1,\"207\":1,\"226\":1,\"229\":1}}],[\"most\",{\"1\":{\"48\":1,\"120\":1,\"121\":1,\"216\":1,\"226\":1}}],[\"more\",{\"1\":{\"5\":1,\"12\":1,\"48\":1,\"49\":1,\"60\":1,\"82\":1,\"83\":1,\"89\":1,\"94\":2,\"120\":1,\"140\":2,\"167\":1,\"168\":1,\"185\":1,\"196\":1,\"207\":2,\"215\":1,\"217\":1,\"218\":1,\"224\":3,\"226\":5,\"227\":1,\"229\":1}}],[\"massive\",{\"1\":{\"166\":1}}],[\"master\",{\"1\":{\"87\":1,\"89\":1,\"207\":1}}],[\"macroscopic\",{\"1\":{\"154\":1}}],[\"machine\",{\"0\":{\"123\":1},\"1\":{\"48\":1,\"59\":1,\"110\":1,\"119\":1,\"120\":1,\"123\":1,\"166\":1}}],[\"mark\",{\"1\":{\"215\":2,\"217\":6}}],[\"markov\",{\"0\":{\"8\":1,\"14\":1},\"1\":{\"8\":1}}],[\"margin\",{\"1\":{\"123\":1}}],[\"magnitude\",{\"1\":{\"109\":2,\"110\":1,\"119\":1}}],[\"mapf\",{\"1\":{\"192\":1}}],[\"map=ones\",{\"1\":{\"160\":1}}],[\"map\",{\"1\":{\"104\":12,\"154\":1,\"160\":6,\"161\":8}}],[\"mapping\",{\"1\":{\"32\":3,\"33\":3,\"38\":1}}],[\"manipulate\",{\"1\":{\"207\":1,\"229\":1}}],[\"management\",{\"1\":{\"154\":1}}],[\"manually\",{\"0\":{\"84\":1},\"1\":{\"82\":1,\"84\":1}}],[\"many\",{\"1\":{\"43\":1,\"44\":1,\"45\":3,\"46\":1,\"49\":1,\"134\":1,\"213\":1,\"217\":1,\"224\":1,\"225\":1,\"226\":3}}],[\"made\",{\"1\":{\"49\":1,\"226\":2}}],[\"making\",{\"1\":{\"49\":1,\"83\":2,\"94\":1,\"191\":1,\"194\":1,\"196\":1,\"203\":1,\"223\":1,\"224\":1,\"227\":1}}],[\"maker\",{\"1\":{\"197\":1,\"206\":1}}],[\"makes\",{\"1\":{\"99\":1}}],[\"make\",{\"1\":{\"45\":1,\"61\":1,\"88\":1,\"104\":1}}],[\"maintain\",{\"1\":{\"224\":1,\"229\":1}}],[\"maintaining\",{\"1\":{\"224\":1}}],[\"maintains\",{\"1\":{\"224\":1}}],[\"main\",{\"1\":{\"48\":1,\"49\":1,\"59\":1}}],[\"mainly\",{\"1\":{\"5\":1,\"37\":1,\"49\":1,\"220\":2}}],[\"maximize\",{\"1\":{\"123\":1,\"124\":1}}],[\"maximum\",{\"1\":{\"12\":1,\"30\":1,\"39\":3,\"44\":1,\"75\":1,\"141\":1,\"146\":3,\"190\":1,\"193\":1,\"203\":2}}],[\"max\",{\"0\":{\"203\":1},\"1\":{\"30\":1,\"101\":1,\"161\":3,\"190\":1,\"193\":1,\"199\":1,\"203\":1,\"216\":1}}],[\"maxπ​∑a​=1\",{\"1\":{\"30\":1}}],[\"mamely\",{\"1\":{\"21\":1}}],[\"may\",{\"1\":{\"18\":1,\"22\":1,\"43\":1,\"193\":4,\"200\":2,\"207\":2,\"219\":1,\"221\":1,\"222\":1,\"224\":1,\"226\":2,\"229\":2}}],[\"matmul\",{\"1\":{\"104\":1}}],[\"matplotlib\",{\"1\":{\"104\":1}}],[\"match\",{\"1\":{\"82\":1}}],[\"matching\",{\"1\":{\"82\":1}}],[\"materialize\",{\"1\":{\"82\":1}}],[\"matter\",{\"1\":{\"45\":1}}],[\"matrixd~\",{\"1\":{\"93\":1}}],[\"matrix\",{\"1\":{\"11\":1,\"23\":4,\"24\":2,\"29\":1,\"34\":1,\"61\":1,\"75\":1,\"87\":2,\"92\":5,\"93\":4,\"95\":2,\"96\":1,\"98\":1,\"100\":2,\"101\":1,\"104\":11,\"121\":1}}],[\"mathematical\",{\"1\":{\"5\":1,\"11\":1,\"18\":1,\"43\":1}}],[\"ml\",{\"0\":{\"232\":1},\"1\":{\"4\":1,\"49\":1},\"2\":{\"62\":1,\"76\":1,\"79\":1,\"85\":1,\"105\":1,\"107\":1,\"125\":1}}],[\"msi\",{\"1\":{\"3\":1}}],[\"myself\",{\"1\":{\"1\":1,\"4\":1}}],[\"赛\",{\"1\":{\"3\":1}}],[\"✓\",{\"1\":{\"3\":1}}],[\"探究生命的意义\",{\"1\":{\"3\":1}}],[\"却也逐渐看清了生活的本质\",{\"1\":{\"3\":1}}],[\"梦想很多\",{\"1\":{\"3\":1}}],[\"⚽\",{\"1\":{\"3\":1}}],[\"足球\",{\"1\":{\"3\":1}}],[\"吉他学习\",{\"1\":{\"4\":1}}],[\"吉他\",{\"1\":{\"3\":1}}],[\"唱歌\",{\"1\":{\"3\":1}}],[\"听歌\",{\"1\":{\"3\":1}}],[\"爱好\",{\"1\":{\"3\":1}}],[\"绿老头一枚\",{\"1\":{\"3\":1}}],[\"🎸\",{\"1\":{\"3\":1}}],[\"🎤\",{\"1\":{\"3\":1}}],[\"🎧\",{\"1\":{\"3\":1}}],[\"🍓\",{\"1\":{\"3\":1}}],[\"🏫\",{\"1\":{\"3\":1}}],[\"s=\",{\"1\":{\"226\":1}}],[\"s=randsrc\",{\"1\":{\"161\":1}}],[\"sm\",{\"1\":{\"160\":6}}],[\"sm=ones\",{\"1\":{\"160\":1}}],[\"smallest\",{\"1\":{\"199\":1,\"216\":1,\"221\":1,\"224\":1,\"226\":1}}],[\"smaller\",{\"1\":{\"83\":1,\"199\":4,\"222\":1}}],[\"small\",{\"1\":{\"81\":1,\"83\":1,\"110\":1,\"140\":2,\"200\":1,\"224\":1,\"227\":1}}],[\"svm\",{\"0\":{\"115\":1},\"1\":{\"115\":20,\"123\":4}}],[\"slow\",{\"1\":{\"146\":1}}],[\"slowdown\",{\"1\":{\"144\":1}}],[\"slowly\",{\"1\":{\"110\":1,\"140\":1}}],[\"slightly\",{\"1\":{\"81\":1,\"200\":1}}],[\"slide\",{\"0\":{\"64\":1}}],[\"sqrt\",{\"1\":{\"104\":5}}],[\"square\",{\"1\":{\"84\":1}}],[\"symbol\",{\"1\":{\"227\":1}}],[\"symmetrically\",{\"1\":{\"222\":2}}],[\"symmetries\",{\"1\":{\"87\":1}}],[\"systematic\",{\"1\":{\"214\":1}}],[\"systems\",{\"1\":{\"184\":1}}],[\"system\",{\"1\":{\"35\":1,\"121\":1,\"192\":1,\"197\":2,\"199\":1,\"223\":3,\"226\":1}}],[\"sk​\",{\"1\":{\"57\":1}}],[\"skip\",{\"1\":{\"21\":1}}],[\"sj​\",{\"1\":{\"53\":1}}],[\"sj​∣si​\",{\"1\":{\"23\":1}}],[\"scnarios\",{\"1\":{\"154\":1,\"167\":1}}],[\"scenarios\",{\"1\":{\"167\":1}}],[\"scenario\",{\"1\":{\"144\":1}}],[\"schemes\",{\"0\":{\"217\":1}}],[\"scheduler\",{\"0\":{\"81\":1},\"1\":{\"81\":7}}],[\"schreckenberg\",{\"0\":{\"138\":1},\"1\":{\"138\":4}}],[\"scalability\",{\"1\":{\"203\":1}}],[\"scalable\",{\"1\":{\"99\":1}}],[\"scalar\",{\"1\":{\"81\":2,\"198\":1,\"199\":1}}],[\"scale\",{\"1\":{\"87\":2}}],[\"scaled\",{\"1\":{\"53\":1}}],[\"scores\",{\"1\":{\"54\":1,\"59\":1}}],[\"score\",{\"0\":{\"53\":1},\"1\":{\"53\":2,\"54\":1,\"58\":2,\"61\":4}}],[\"scope\",{\"1\":{\"45\":1}}],[\"s5​\",{\"1\":{\"45\":1}}],[\"s2​\",{\"1\":{\"45\":2,\"57\":1}}],[\"si\",{\"1\":{\"158\":2,\"172\":1}}],[\"sideback\",{\"1\":{\"150\":1}}],[\"sidebar\",{\"0\":{\"64\":1}}],[\"sidefront\",{\"1\":{\"150\":1}}],[\"situations\",{\"1\":{\"83\":1}}],[\"sized\",{\"1\":{\"120\":1}}],[\"size=12\",{\"1\":{\"104\":1}}],[\"size\",{\"1\":{\"78\":1,\"100\":1,\"104\":4,\"111\":1,\"160\":3,\"161\":5,\"227\":1}}],[\"si​\",{\"1\":{\"58\":2}}],[\"signal\",{\"1\":{\"154\":2,\"185\":1}}],[\"significantly\",{\"1\":{\"122\":1,\"203\":1}}],[\"sigmoid\",{\"1\":{\"49\":1,\"78\":3}}],[\"sighted\",{\"1\":{\"35\":1}}],[\"single\",{\"0\":{\"145\":1},\"1\":{\"45\":1,\"49\":1,\"69\":1,\"74\":1,\"147\":1,\"191\":1,\"192\":1,\"193\":1,\"199\":1,\"218\":1,\"224\":2,\"227\":1}}],[\"since\",{\"1\":{\"38\":1,\"39\":2,\"44\":1,\"46\":1,\"185\":1,\"191\":1,\"199\":1,\"223\":2}}],[\"simultaneously\",{\"1\":{\"193\":1,\"224\":2}}],[\"simulating\",{\"1\":{\"154\":1,\"167\":1}}],[\"simulation\",{\"0\":{\"153\":1},\"1\":{\"67\":1,\"134\":1,\"138\":1,\"154\":1,\"207\":1}}],[\"simply\",{\"1\":{\"207\":1,\"223\":1,\"226\":1}}],[\"simplicity\",{\"1\":{\"93\":1}}],[\"simple\",{\"1\":{\"8\":1,\"144\":1,\"226\":1,\"229\":1}}],[\"similarity\",{\"1\":{\"53\":2,\"58\":1,\"60\":1,\"223\":1}}],[\"similar\",{\"1\":{\"30\":1,\"88\":2,\"94\":1,\"199\":1,\"223\":1}}],[\"similarly\",{\"1\":{\"10\":1}}],[\"suffice\",{\"1\":{\"227\":1}}],[\"sufficient\",{\"1\":{\"44\":1}}],[\"sufficiently\",{\"1\":{\"44\":1,\"45\":2,\"46\":2}}],[\"sustained\",{\"1\":{\"140\":1}}],[\"subgoals\",{\"1\":{\"229\":1}}],[\"subgroups\",{\"1\":{\"122\":1}}],[\"subroutine\",{\"1\":{\"207\":1}}],[\"substitute\",{\"1\":{\"227\":1}}],[\"substantial\",{\"1\":{\"225\":1,\"229\":1}}],[\"substantially\",{\"1\":{\"119\":1}}],[\"subsets\",{\"1\":{\"120\":1}}],[\"subset\",{\"1\":{\"100\":1,\"190\":1,\"200\":1}}],[\"subplots\",{\"1\":{\"104\":1}}],[\"suboptimal\",{\"1\":{\"83\":1}}],[\"subepisodes\",{\"1\":{\"45\":2}}],[\"success\",{\"1\":{\"74\":1,\"215\":1,\"217\":4}}],[\"such\",{\"1\":{\"45\":1,\"66\":1,\"67\":1,\"83\":1,\"119\":1,\"121\":1,\"140\":1,\"166\":1,\"193\":1,\"199\":1,\"215\":1,\"226\":2}}],[\"surrounding\",{\"1\":{\"154\":1}}],[\"surrogate\",{\"0\":{\"75\":1},\"1\":{\"69\":1,\"74\":1,\"82\":1,\"84\":2,\"203\":3}}],[\"sure\",{\"1\":{\"45\":1,\"61\":1,\"104\":1}}],[\"superficial\",{\"1\":{\"168\":1}}],[\"supervised\",{\"1\":{\"123\":1,\"124\":1}}],[\"super\",{\"1\":{\"61\":1,\"104\":1}}],[\"support\",{\"0\":{\"123\":1},\"1\":{\"110\":1,\"123\":3}}],[\"supported\",{\"1\":{\"43\":1}}],[\"suppose\",{\"1\":{\"30\":1,\"32\":1,\"43\":1,\"197\":1,\"199\":1,\"213\":3,\"221\":1,\"222\":1,\"227\":1,\"229\":1}}],[\"suggests\",{\"1\":{\"49\":1}}],[\"suggest\",{\"1\":{\"38\":1}}],[\"summation\",{\"1\":{\"87\":1}}],[\"summary\",{\"0\":{\"60\":1,\"103\":1},\"1\":{\"103\":1,\"226\":2}}],[\"summarized\",{\"1\":{\"38\":1}}],[\"summed\",{\"1\":{\"81\":2}}],[\"sum\",{\"0\":{\"55\":1},\"1\":{\"16\":1,\"17\":1,\"54\":1,\"55\":1,\"58\":1,\"60\":1,\"81\":2,\"82\":1,\"89\":1,\"98\":1,\"104\":1,\"161\":2,\"189\":1,\"190\":2,\"193\":1,\"197\":1,\"199\":1,\"219\":1,\"223\":1}}],[\"s0​\",{\"1\":{\"29\":1}}],[\"s0​∣s\",{\"1\":{\"22\":1,\"29\":1,\"35\":1}}],[\"s∈s\",{\"1\":{\"29\":1,\"30\":1}}],[\"satisfied\",{\"1\":{\"218\":1}}],[\"satisfies\",{\"1\":{\"207\":1}}],[\"satisfying\",{\"1\":{\"207\":1,\"229\":1}}],[\"salesman\",{\"0\":{\"203\":1},\"1\":{\"203\":1}}],[\"safe\",{\"1\":{\"143\":1,\"144\":1}}],[\"save\",{\"1\":{\"82\":1}}],[\"sampled\",{\"1\":{\"100\":1,\"101\":1}}],[\"sample\",{\"1\":{\"43\":1,\"74\":1}}],[\"samplings\",{\"1\":{\"45\":1}}],[\"sampling\",{\"1\":{\"43\":1,\"59\":1,\"69\":1,\"99\":1,\"100\":1}}],[\"same\",{\"1\":{\"22\":1,\"39\":1,\"44\":1,\"46\":1,\"82\":1,\"111\":1,\"189\":4,\"190\":1,\"192\":1,\"216\":1,\"222\":2,\"223\":1,\"229\":2}}],[\"says\",{\"1\":{\"226\":1}}],[\"saying\",{\"1\":{\"223\":1}}],[\"say\",{\"1\":{\"28\":2,\"46\":1,\"199\":1}}],[\"shared\",{\"1\":{\"96\":1,\"185\":1,\"193\":1}}],[\"share\",{\"1\":{\"88\":1}}],[\"shape\",{\"1\":{\"61\":4,\"78\":1,\"82\":1,\"104\":3}}],[\"should\",{\"1\":{\"44\":1,\"45\":1,\"48\":1,\"67\":1,\"81\":1,\"82\":2,\"196\":1,\"203\":1}}],[\"shortest\",{\"1\":{\"224\":1}}],[\"shorter\",{\"1\":{\"203\":1}}],[\"short\",{\"1\":{\"35\":1,\"49\":4,\"138\":1,\"146\":1,\"160\":1,\"216\":2}}],[\"shown\",{\"1\":{\"213\":1,\"221\":1,\"224\":2,\"226\":1}}],[\"showcase\",{\"1\":{\"185\":1}}],[\"showing\",{\"1\":{\"24\":1}}],[\"show\",{\"1\":{\"24\":1,\"27\":1,\"104\":1}}],[\"shiyu\",{\"1\":{\"5\":1}}],[\"sn​\",{\"1\":{\"23\":2}}],[\"s1​\",{\"1\":{\"23\":2,\"25\":1,\"57\":1}}],[\"s1​→s2​→s3​→s5​\",{\"1\":{\"18\":1}}],[\"s1​→s2​→s3​→s5​→s5​→s5​\",{\"1\":{\"16\":1}}],[\"sorted\",{\"1\":{\"218\":1,\"224\":1}}],[\"sort\",{\"1\":{\"216\":2}}],[\"sorting\",{\"1\":{\"216\":1}}],[\"sora有助于提高交通管理系统的准备性和响应能力\",{\"1\":{\"167\":1}}],[\"sora可以用来模拟自然灾害\",{\"1\":{\"167\":1}}],[\"sora可以帮助预测和分析交通模式\",{\"1\":{\"167\":1}}],[\"sora生成的视频可以模拟各种决策场景\",{\"1\":{\"167\":1}}],[\"sora结合了gpt技术\",{\"1\":{\"167\":1}}],[\"sora\",{\"0\":{\"167\":1},\"1\":{\"167\":1}}],[\"social\",{\"1\":{\"154\":1}}],[\"source=\",{\"1\":{\"49\":1}}],[\"source\",{\"1\":{\"49\":3,\"57\":1,\"60\":1,\"82\":1}}],[\"softmax\",{\"0\":{\"54\":1},\"1\":{\"49\":1,\"53\":1,\"54\":1,\"59\":2,\"61\":1,\"97\":1,\"98\":1}}],[\"soft\",{\"1\":{\"46\":2,\"53\":1}}],[\"sol\",{\"1\":{\"172\":1}}],[\"solutions\",{\"1\":{\"89\":1,\"185\":1,\"191\":2,\"192\":1,\"193\":1,\"194\":3,\"195\":2,\"196\":8,\"197\":1,\"199\":5,\"200\":4}}],[\"solution\",{\"0\":{\"197\":1},\"1\":{\"24\":4,\"71\":1,\"74\":1,\"83\":2,\"89\":1,\"191\":3,\"193\":1,\"195\":4,\"196\":2,\"197\":7,\"199\":3,\"203\":1,\"216\":2,\"218\":2,\"223\":1,\"226\":1,\"229\":1}}],[\"solver\",{\"1\":{\"69\":3,\"71\":1,\"203\":2,\"226\":1}}],[\"solve\",{\"0\":{\"24\":1,\"31\":1},\"1\":{\"24\":1,\"29\":1,\"30\":5,\"33\":2,\"38\":1,\"200\":1,\"225\":2,\"227\":1}}],[\"solving\",{\"0\":{\"203\":1},\"1\":{\"23\":1,\"29\":1,\"44\":1,\"203\":1,\"223\":1}}],[\"so\",{\"1\":{\"18\":1,\"23\":1,\"25\":1,\"29\":1,\"30\":2,\"32\":2,\"33\":2,\"41\":1,\"46\":2,\"49\":1,\"60\":1,\"87\":1,\"89\":1,\"199\":1,\"221\":3,\"223\":2,\"224\":1,\"226\":2,\"229\":1}}],[\"some\",{\"1\":{\"18\":1,\"29\":2,\"48\":1,\"49\":1,\"81\":1,\"196\":1,\"200\":1,\"215\":1,\"218\":2,\"220\":1,\"223\":1,\"224\":2,\"226\":1,\"229\":2}}],[\"spite\",{\"1\":{\"207\":1}}],[\"split\",{\"1\":{\"198\":1}}],[\"splits\",{\"1\":{\"124\":1,\"198\":1}}],[\"splitting\",{\"1\":{\"124\":1}}],[\"spatial\",{\"1\":{\"78\":1,\"154\":1}}],[\"spaces\",{\"1\":{\"224\":1,\"227\":1}}],[\"space\",{\"0\":{\"227\":1,\"229\":1},\"1\":{\"9\":1,\"10\":1,\"83\":2,\"123\":1,\"134\":1,\"140\":2,\"143\":1,\"148\":1,\"193\":1,\"199\":1,\"207\":1,\"212\":2,\"220\":1,\"224\":1,\"225\":2,\"227\":2,\"229\":1}}],[\"spent\",{\"1\":{\"224\":1}}],[\"speeds\",{\"1\":{\"207\":1}}],[\"speed逐渐降低\",{\"1\":{\"148\":1}}],[\"speed\",{\"1\":{\"141\":2,\"144\":6,\"203\":1}}],[\"speed以及波次间隔时间wave\",{\"1\":{\"140\":1}}],[\"speech\",{\"1\":{\"49\":1,\"88\":1,\"166\":1}}],[\"specified\",{\"1\":{\"214\":1,\"223\":1,\"227\":1}}],[\"specifically\",{\"1\":{\"44\":1,\"59\":1}}],[\"specific\",{\"1\":{\"22\":1,\"25\":1,\"43\":1,\"218\":1,\"223\":1,\"226\":1,\"229\":2}}],[\"specify\",{\"1\":{\"82\":1,\"207\":2,\"216\":2}}],[\"specialize\",{\"1\":{\"87\":1}}],[\"special\",{\"1\":{\"18\":1,\"192\":1,\"216\":1,\"223\":1}}],[\"steven\",{\"1\":{\"205\":2}}],[\"step=1\",{\"1\":{\"160\":1}}],[\"stepping\",{\"1\":{\"48\":1,\"213\":1}}],[\"steps\",{\"1\":{\"38\":1,\"44\":1,\"59\":1,\"83\":1,\"89\":1,\"168\":2,\"213\":1,\"216\":3,\"218\":1,\"223\":2,\"226\":1}}],[\"step\",{\"0\":{\"144\":1},\"1\":{\"21\":1,\"38\":2,\"39\":3,\"44\":3,\"45\":1,\"46\":2,\"49\":1,\"59\":6,\"78\":1,\"81\":1,\"89\":2,\"93\":3,\"98\":3,\"134\":1,\"144\":5,\"198\":5,\"216\":1,\"218\":2,\"219\":1,\"223\":1,\"227\":1}}],[\"studies\",{\"1\":{\"154\":1}}],[\"study\",{\"1\":{\"37\":1}}],[\"stca模型在应用过程中将一个时步分为两个相同的子时步\",{\"1\":{\"150\":1}}],[\"stca\",{\"1\":{\"150\":1}}],[\"strings\",{\"1\":{\"227\":2}}],[\"string\",{\"1\":{\"227\":6}}],[\"strips\",{\"0\":{\"226\":1},\"1\":{\"226\":3,\"227\":1}}],[\"strictly\",{\"1\":{\"191\":1,\"195\":1}}],[\"straightforward\",{\"1\":{\"229\":1}}],[\"straight\",{\"1\":{\"49\":1,\"196\":1}}],[\"strategy\",{\"1\":{\"45\":1,\"224\":1}}],[\"structured\",{\"1\":{\"86\":1,\"91\":1,\"94\":1,\"199\":1}}],[\"structure\",{\"1\":{\"49\":2,\"87\":1,\"92\":1,\"95\":1,\"100\":1,\"124\":1,\"154\":1}}],[\"stronger\",{\"1\":{\"46\":1}}],[\"storage\",{\"1\":{\"87\":1}}],[\"stochastic\",{\"1\":{\"28\":1,\"46\":2,\"224\":1}}],[\"stopping\",{\"1\":{\"111\":1}}],[\"stops\",{\"1\":{\"81\":1,\"83\":1}}],[\"stop\",{\"1\":{\"18\":1,\"140\":2,\"223\":2}}],[\"st​\",{\"1\":{\"21\":2,\"57\":1}}],[\"st​→at​at​\",{\"1\":{\"21\":1}}],[\"st​→at​rt+1​\",{\"1\":{\"21\":1}}],[\"st+1​∣st​\",{\"1\":{\"21\":1}}],[\"st+1​\",{\"1\":{\"21\":1}}],[\"still\",{\"1\":{\"18\":1,\"45\":1,\"46\":1,\"200\":1,\"216\":1,\"222\":1}}],[\"stanford\",{\"1\":{\"226\":1}}],[\"standard\",{\"1\":{\"192\":1}}],[\"stage\",{\"1\":{\"219\":3,\"221\":1,\"222\":2,\"223\":4}}],[\"stages\",{\"1\":{\"219\":1,\"223\":2}}],[\"stagnant\",{\"1\":{\"81\":1}}],[\"stack\",{\"1\":{\"216\":1}}],[\"stabilizing\",{\"1\":{\"203\":1}}],[\"stabilize\",{\"1\":{\"53\":1}}],[\"star\",{\"1\":{\"160\":3}}],[\"starting\",{\"1\":{\"22\":1,\"25\":2,\"45\":2,\"46\":1,\"207\":1,\"217\":1,\"218\":1}}],[\"starts\",{\"0\":{\"45\":1},\"1\":{\"22\":1,\"45\":2,\"46\":4,\"81\":1,\"198\":1}}],[\"start\",{\"1\":{\"7\":1,\"45\":2,\"46\":1,\"146\":2,\"185\":1,\"189\":2,\"217\":1,\"222\":1,\"224\":1}}],[\"stays\",{\"1\":{\"207\":1}}],[\"stay\",{\"1\":{\"34\":1,\"190\":1}}],[\"static\",{\"1\":{\"158\":1}}],[\"stationary\",{\"1\":{\"146\":1}}],[\"status\",{\"1\":{\"9\":1}}],[\"statements\",{\"1\":{\"226\":1}}],[\"statement\",{\"0\":{\"187\":1},\"1\":{\"44\":1}}],[\"states\",{\"1\":{\"18\":1,\"21\":1,\"23\":1,\"48\":1,\"49\":1,\"58\":1,\"188\":1,\"207\":3,\"214\":1,\"215\":6,\"218\":2,\"219\":1,\"224\":11,\"225\":1,\"227\":1,\"229\":3}}],[\"state\",{\"0\":{\"9\":1,\"22\":1,\"24\":1,\"227\":1},\"1\":{\"3\":1,\"7\":3,\"8\":2,\"9\":2,\"10\":3,\"11\":2,\"12\":2,\"13\":4,\"14\":2,\"16\":1,\"18\":9,\"21\":2,\"22\":10,\"23\":11,\"24\":2,\"25\":12,\"28\":1,\"29\":1,\"30\":1,\"38\":2,\"39\":2,\"40\":2,\"43\":2,\"44\":5,\"45\":7,\"46\":4,\"49\":8,\"57\":1,\"58\":2,\"59\":9,\"134\":4,\"154\":1,\"207\":8,\"212\":7,\"213\":4,\"214\":1,\"215\":6,\"216\":4,\"217\":4,\"218\":1,\"219\":1,\"220\":1,\"221\":7,\"222\":1,\"223\":16,\"224\":8,\"225\":2,\"226\":2,\"227\":10,\"229\":1}}],[\"serves\",{\"1\":{\"222\":1}}],[\"series\",{\"1\":{\"49\":1}}],[\"sections\",{\"1\":{\"224\":1}}],[\"section\",{\"1\":{\"214\":1,\"216\":1,\"219\":2,\"220\":1,\"222\":1,\"223\":1,\"224\":2,\"227\":1,\"228\":1}}],[\"second\",{\"1\":{\"45\":1,\"82\":1,\"199\":2,\"207\":1}}],[\"searching\",{\"0\":{\"214\":1,\"229\":1},\"1\":{\"229\":1}}],[\"search\",{\"0\":{\"198\":1,\"215\":1,\"216\":1,\"217\":1,\"218\":1},\"1\":{\"195\":1,\"196\":1,\"198\":4,\"199\":3,\"214\":2,\"215\":5,\"216\":8,\"217\":6,\"218\":5,\"224\":1,\"227\":2,\"229\":3}}],[\"separator\",{\"1\":{\"123\":1}}],[\"separates\",{\"1\":{\"123\":1}}],[\"separated\",{\"1\":{\"122\":1}}],[\"separable\",{\"1\":{\"123\":1}}],[\"segmentation\",{\"1\":{\"88\":1}}],[\"several\",{\"1\":{\"81\":2,\"88\":1,\"104\":1,\"168\":1,\"216\":1,\"226\":2}}],[\"selection\",{\"1\":{\"227\":1}}],[\"selecting\",{\"1\":{\"121\":1,\"223\":3}}],[\"selected\",{\"1\":{\"59\":1,\"198\":2,\"227\":1}}],[\"select\",{\"1\":{\"59\":1,\"60\":1,\"218\":2,\"223\":1}}],[\"self\",{\"1\":{\"45\":1,\"61\":9,\"92\":2,\"104\":6,\"140\":1}}],[\"semantic\",{\"1\":{\"49\":2,\"154\":1}}],[\"sensible\",{\"1\":{\"229\":1}}],[\"sensors\",{\"1\":{\"154\":1}}],[\"sense\",{\"1\":{\"23\":1,\"215\":1}}],[\"sentiment\",{\"1\":{\"88\":1}}],[\"sentence\",{\"1\":{\"49\":5,\"88\":2}}],[\"seq2seq\",{\"1\":{\"49\":2}}],[\"sequencial\",{\"1\":{\"49\":1}}],[\"sequences\",{\"1\":{\"86\":1}}],[\"sequence\",{\"1\":{\"7\":1,\"24\":1,\"43\":1,\"49\":4,\"82\":1,\"189\":1,\"207\":2,\"213\":1,\"219\":2}}],[\"seemed\",{\"1\":{\"229\":1}}],[\"seems\",{\"1\":{\"82\":1,\"224\":1}}],[\"seeks\",{\"1\":{\"185\":3,\"194\":1}}],[\"see\",{\"1\":{\"30\":1,\"46\":2,\"61\":1,\"201\":1,\"207\":1,\"221\":1,\"227\":1}}],[\"seen\",{\"1\":{\"7\":1}}],[\"setg\",{\"1\":{\"226\":1}}],[\"sets\",{\"1\":{\"120\":1,\"198\":1,\"213\":1,\"226\":5}}],[\"settings\",{\"1\":{\"185\":1}}],[\"setting\",{\"1\":{\"82\":2}}],[\"set\",{\"1\":{\"9\":1,\"10\":1,\"23\":1,\"46\":1,\"97\":1,\"104\":1,\"111\":1,\"120\":2,\"134\":1,\"185\":1,\"188\":4,\"191\":1,\"192\":1,\"195\":1,\"196\":1,\"198\":1,\"200\":1,\"207\":1,\"213\":2,\"223\":2,\"224\":4,\"226\":10,\"227\":4,\"229\":5}}],[\"s\",{\"1\":{\"3\":1,\"9\":2,\"10\":1,\"13\":4,\"22\":3,\"23\":12,\"24\":2,\"25\":8,\"27\":1,\"28\":4,\"29\":7,\"30\":5,\"34\":5,\"35\":2,\"38\":7,\"39\":4,\"44\":6,\"49\":5,\"53\":2,\"58\":3,\"69\":1,\"83\":1,\"84\":2,\"87\":1,\"89\":1,\"91\":1,\"92\":1,\"99\":1,\"100\":1,\"119\":1,\"120\":1,\"121\":1,\"146\":1,\"161\":6,\"190\":2,\"191\":4,\"193\":2,\"194\":3,\"195\":1,\"196\":2,\"197\":1,\"198\":1,\"199\":6,\"200\":1,\"203\":1,\"213\":1,\"216\":1,\"221\":3,\"224\":3,\"226\":5}}],[\"swiftie\",{\"1\":{\"3\":1}}],[\"成分\",{\"1\":{\"3\":1}}],[\"水瓶座\",{\"1\":{\"3\":1}}],[\"星座\",{\"1\":{\"3\":1}}],[\"u−1\",{\"1\":{\"223\":5}}],[\"u∗=argu∈u\",{\"1\":{\"223\":1}}],[\"u∗\",{\"1\":{\"223\":3}}],[\"u2​\",{\"1\":{\"223\":2}}],[\"ut​\",{\"1\":{\"223\":4}}],[\"utilize\",{\"1\":{\"33\":1,\"45\":1}}],[\"utilizes\",{\"1\":{\"8\":1}}],[\"ud​b\",{\"1\":{\"221\":1}}],[\"ud​c\",{\"1\":{\"221\":1}}],[\"uc​d\",{\"1\":{\"221\":1}}],[\"ub​d\",{\"1\":{\"221\":1}}],[\"u4​\",{\"1\":{\"221\":3}}],[\"uk−1min​sumi=1k−1​l\",{\"1\":{\"222\":1}}],[\"uk\",{\"1\":{\"221\":1}}],[\"ukmin​sumi=kk​l\",{\"1\":{\"221\":1}}],[\"uk​\",{\"1\":{\"219\":8,\"221\":2,\"222\":2}}],[\"u1​\",{\"1\":{\"219\":1,\"223\":2}}],[\"u^\",{\"1\":{\"217\":4}}],[\"u∈u\",{\"1\":{\"213\":1,\"215\":1}}],[\"u=argu−1∈u−1\",{\"1\":{\"223\":1}}],[\"u=\",{\"1\":{\"213\":1}}],[\"urban\",{\"1\":{\"154\":1}}],[\"ulam和john\",{\"1\":{\"127\":1}}],[\"ultimate\",{\"1\":{\"27\":1}}],[\"u\",{\"1\":{\"53\":1,\"212\":4,\"213\":4,\"215\":4,\"217\":5,\"218\":1,\"219\":1,\"223\":9,\"224\":5,\"227\":3}}],[\"uht−1​\",{\"1\":{\"49\":1}}],[\"unnecessary\",{\"1\":{\"223\":1}}],[\"unnormalized\",{\"1\":{\"59\":1}}],[\"unreasonable\",{\"1\":{\"223\":1}}],[\"unrolled\",{\"1\":{\"49\":1}}],[\"unless\",{\"1\":{\"218\":1}}],[\"unlike\",{\"1\":{\"49\":1,\"86\":1,\"99\":1}}],[\"unvisited\",{\"1\":{\"215\":2}}],[\"unpredictable\",{\"1\":{\"207\":1}}],[\"uncountably\",{\"1\":{\"207\":1}}],[\"unchanged\",{\"1\":{\"34\":1,\"49\":1,\"87\":1}}],[\"unspecified\",{\"0\":{\"223\":1},\"1\":{\"223\":2}}],[\"unseen\",{\"1\":{\"99\":1,\"119\":1}}],[\"unsqueeze\",{\"1\":{\"61\":3,\"104\":2}}],[\"undirected\",{\"1\":{\"87\":1}}],[\"understood\",{\"1\":{\"221\":1}}],[\"understanding\",{\"1\":{\"49\":1,\"166\":1,\"214\":1}}],[\"understand\",{\"1\":{\"21\":1,\"24\":1,\"48\":1,\"61\":1,\"104\":1,\"167\":1}}],[\"under\",{\"1\":{\"21\":1,\"72\":1,\"207\":3,\"208\":1,\"226\":4,\"228\":1,\"229\":1}}],[\"unused=true\",{\"1\":{\"84\":1}}],[\"unused=none\",{\"1\":{\"82\":1}}],[\"until\",{\"1\":{\"45\":1,\"122\":1,\"198\":1,\"207\":1,\"216\":1,\"217\":1}}],[\"unknowns\",{\"1\":{\"30\":2,\"35\":1}}],[\"unknown\",{\"1\":{\"29\":2,\"30\":1,\"39\":1}}],[\"uniformly\",{\"1\":{\"223\":1}}],[\"unified\",{\"0\":{\"218\":1},\"1\":{\"18\":1}}],[\"unimportant\",{\"1\":{\"168\":1}}],[\"unit\",{\"1\":{\"49\":4,\"144\":2}}],[\"units\",{\"1\":{\"49\":1}}],[\"unique\",{\"1\":{\"28\":1}}],[\"university\",{\"1\":{\"3\":1,\"5\":1,\"205\":2}}],[\"us\",{\"1\":{\"223\":1}}],[\"usage\",{\"1\":{\"185\":1}}],[\"using\",{\"0\":{\"225\":1},\"1\":{\"53\":1,\"58\":2,\"59\":1,\"82\":1,\"97\":1,\"98\":1,\"120\":1,\"166\":1,\"197\":1,\"199\":1,\"203\":1,\"213\":1,\"227\":2,\"229\":1}}],[\"usually\",{\"1\":{\"18\":1,\"81\":1,\"82\":1,\"89\":1,\"218\":1}}],[\"user\",{\"1\":{\"206\":1}}],[\"users\",{\"1\":{\"87\":1,\"154\":1}}],[\"useful\",{\"1\":{\"82\":1}}],[\"uses\",{\"1\":{\"45\":1,\"49\":1,\"52\":1,\"123\":1,\"199\":1,\"200\":1,\"203\":1,\"224\":2}}],[\"used\",{\"1\":{\"16\":1,\"23\":1,\"24\":1,\"28\":1,\"44\":2,\"45\":3,\"49\":3,\"59\":1,\"81\":2,\"91\":1,\"119\":1,\"123\":1,\"124\":1,\"199\":1,\"207\":1,\"221\":1,\"223\":3,\"226\":1}}],[\"use\",{\"1\":{\"11\":2,\"25\":1,\"43\":2,\"44\":1,\"45\":1,\"49\":1,\"78\":1,\"87\":1,\"88\":1,\"104\":1,\"207\":1,\"216\":3,\"222\":1,\"224\":1,\"227\":1}}],[\"updating\",{\"1\":{\"122\":1,\"198\":1}}],[\"updataing\",{\"1\":{\"84\":1}}],[\"updates\",{\"1\":{\"81\":2,\"134\":1,\"224\":3}}],[\"updated\",{\"1\":{\"38\":1,\"89\":2,\"143\":1}}],[\"update\",{\"1\":{\"38\":2,\"39\":1,\"40\":1,\"45\":2,\"89\":6,\"103\":1,\"110\":1}}],[\"upper\",{\"1\":{\"69\":1,\"104\":1,\"203\":2,\"223\":1}}],[\"up\",{\"1\":{\"2\":1,\"23\":1,\"49\":1,\"88\":1,\"89\":1,\"146\":1,\"168\":2,\"185\":1,\"196\":1,\"197\":1,\"213\":1,\"226\":1,\"227\":1}}],[\"就无法选择\",{\"1\":{\"158\":1}}],[\"就会发生的\",{\"1\":{\"144\":1}}],[\"就是机器学习的其中一种方法\",{\"1\":{\"110\":1}}],[\"就是那个california\",{\"1\":{\"3\":1}}],[\"就显得非常合适\",{\"1\":{\"2\":1}}],[\"对交通规划\",{\"1\":{\"167\":1}}],[\"对元胞潜能进行归一化\",{\"1\":{\"158\":1}}],[\"对行人流仿真进一步了解可以移步此篇论文\",{\"1\":{\"154\":1}}],[\"对数据进行合适的编码\",{\"1\":{\"154\":1}}],[\"对数据集异常点不敏感\",{\"1\":{\"116\":1}}],[\"对簇形状和密度的适应性\",{\"1\":{\"116\":1}}],[\"对高维数据和不均匀密度数据的处理相对困难\",{\"1\":{\"116\":1}}],[\"对参数的选择敏感\",{\"1\":{\"116\":1}}],[\"对象总数\",{\"1\":{\"116\":1}}],[\"对不同形状的簇和噪声具有较好的鲁棒性\",{\"1\":{\"116\":1}}],[\"对偶问题将原始问题中的约束转为了对偶问题中的等式约束\",{\"1\":{\"115\":1}}],[\"对逻辑回归而言\",{\"1\":{\"115\":1}}],[\"对异常点非常敏感\",{\"1\":{\"115\":1}}],[\"对具有同一规律的学习集以外的数据\",{\"1\":{\"110\":1}}],[\"对于自动驾驶\",{\"1\":{\"167\":1}}],[\"对于未到达最大速度的车辆\",{\"1\":{\"144\":1}}],[\"对于\",{\"1\":{\"133\":2}}],[\"对于非球形簇或具有不同密度的簇效果较差\",{\"1\":{\"116\":1}}],[\"对于非线性特征\",{\"1\":{\"115\":1}}],[\"对于非线性可分的数据集\",{\"1\":{\"115\":2}}],[\"对于一个新的数据样本\",{\"1\":{\"110\":1}}],[\"对于深层网络\",{\"1\":{\"78\":1}}],[\"对伸缩的不变形\",{\"1\":{\"78\":1}}],[\"对\",{\"1\":{\"3\":1}}],[\"一拍的时间不是固定的\",{\"1\":{\"181\":1}}],[\"一个细线加一个粗线是结尾\",{\"1\":{\"174\":1}}],[\"一个完整的元胞自动机模型包含\",{\"1\":{\"127\":1}}],[\"一般常用为固定型和周期型边界条件\",{\"1\":{\"131\":1}}],[\"一般为二维\",{\"1\":{\"128\":1}}],[\"一\",{\"0\":{\"110\":1}}],[\"一种下采样方式\",{\"1\":{\"78\":1}}],[\"一步一步通俗理解lstm\",{\"1\":{\"49\":1}}],[\"一枚\",{\"1\":{\"3\":1}}],[\"一名在读\",{\"1\":{\"3\":1}}],[\"一时难以消化吸收\",{\"1\":{\"1\":1}}],[\"职业\",{\"1\":{\"3\":1}}],[\"我们使用的样本通常是多维数据\",{\"1\":{\"110\":1}}],[\"我们通常得等到测试集才可以知道我们模型真正得实力\",{\"1\":{\"110\":1}}],[\"我\",{\"0\":{\"3\":1}}],[\"我会逐步去完善\",{\"1\":{\"1\":1}}],[\"尝试新的记录生活的方式\",{\"1\":{\"2\":1}}],[\"走出舒适圈\",{\"1\":{\"2\":1}}],[\"写过的很多代码\",{\"1\":{\"2\":1}}],[\"写在前面\",{\"0\":{\"1\":1}}],[\"准备夏令营过程中\",{\"1\":{\"2\":1}}],[\"主成分\",{\"1\":{\"121\":1}}],[\"主成分分析\",{\"1\":{\"110\":1,\"121\":1}}],[\"主要见下图\",{\"1\":{\"173\":1}}],[\"主要用来训练神经网络中的参数\",{\"1\":{\"110\":1}}],[\"主要有三个功效\",{\"1\":{\"78\":1}}],[\"主要为自学\",{\"1\":{\"4\":1}}],[\"主要记录本科专业一些相关学习内容\",{\"1\":{\"4\":1}}],[\"主\",{\"1\":{\"2\":1}}],[\"想当一个知识区博主\",{\"1\":{\"2\":1}}],[\"缘由\",{\"0\":{\"2\":1}}],[\"布局\",{\"1\":{\"1\":1}}],[\"🛣\",{\"1\":{\"195\":1}}],[\"📝\",{\"1\":{\"3\":1}}],[\"💭\",{\"1\":{\"3\":1}}],[\"👐\",{\"1\":{\"2\":1}}],[\"💖\",{\"1\":{\"1\":1}}],[\"👋\",{\"1\":{\"0\":1}}],[\"bσ​\",{\"1\":{\"229\":1}}],[\"b=\",{\"1\":{\"199\":1}}],[\"bj​\",{\"1\":{\"195\":1}}],[\"bm​\",{\"1\":{\"195\":1,\"199\":1}}],[\"bmm\",{\"1\":{\"61\":2,\"104\":3}}],[\"b2​=12\",{\"1\":{\"199\":1}}],[\"b2​\",{\"1\":{\"195\":1,\"199\":1}}],[\"b1​\",{\"1\":{\"195\":1,\"199\":1}}],[\"binding\",{\"1\":{\"229\":2}}],[\"binary\",{\"1\":{\"226\":1,\"227\":2}}],[\"bits\",{\"1\":{\"227\":1}}],[\"bit\",{\"1\":{\"227\":1}}],[\"bidirectional\",{\"1\":{\"217\":2,\"218\":1}}],[\"billions\",{\"1\":{\"166\":1}}],[\"bilevel\",{\"1\":{\"69\":1,\"203\":1}}],[\"bias\",{\"1\":{\"111\":1,\"119\":1}}],[\"black\",{\"1\":{\"104\":1}}],[\"blog\",{\"1\":{\"1\":1,\"2\":2,\"5\":1,\"37\":1,\"49\":1}}],[\"b∈r\",{\"1\":{\"67\":1}}],[\"b\",{\"1\":{\"49\":1,\"82\":2,\"114\":1,\"195\":5,\"196\":2,\"197\":1,\"199\":4,\"221\":10,\"222\":3,\"223\":1}}],[\"border\",{\"1\":{\"160\":5,\"161\":3}}],[\"border=ones\",{\"1\":{\"160\":1}}],[\"bound\",{\"1\":{\"111\":1}}],[\"boundary\",{\"1\":{\"34\":1,\"134\":1,\"147\":1,\"154\":1}}],[\"bo​\",{\"1\":{\"59\":1}}],[\"both\",{\"1\":{\"53\":1,\"124\":1,\"193\":1,\"196\":1,\"197\":1,\"207\":1,\"223\":1,\"226\":3,\"227\":1}}],[\"boe\",{\"0\":{\"29\":1,\"33\":1},\"1\":{\"28\":1,\"30\":1,\"33\":2,\"43\":1}}],[\"books\",{\"1\":{\"226\":1}}],[\"book\",{\"0\":{\"208\":1},\"1\":{\"5\":1,\"226\":3}}],[\"broader\",{\"1\":{\"224\":2}}],[\"brought\",{\"1\":{\"25\":1}}],[\"braking\",{\"1\":{\"140\":1,\"144\":1}}],[\"branching\",{\"1\":{\"217\":1}}],[\"branches\",{\"1\":{\"198\":1}}],[\"branch\",{\"1\":{\"124\":1,\"206\":1}}],[\"bring\",{\"1\":{\"223\":1}}],[\"bridge\",{\"1\":{\"89\":1}}],[\"briefer\",{\"1\":{\"23\":1}}],[\"brief\",{\"1\":{\"23\":1}}],[\"breadth\",{\"1\":{\"216\":2}}],[\"breadcrumb\",{\"0\":{\"64\":1}}],[\"breath\",{\"1\":{\"216\":2}}],[\"break\",{\"1\":{\"199\":1}}],[\"breakdown\",{\"1\":{\"69\":1}}],[\"back\",{\"1\":{\"226\":1}}],[\"backward\",{\"0\":{\"221\":1},\"1\":{\"82\":2,\"217\":3,\"218\":2,\"220\":1,\"222\":1,\"223\":1}}],[\"battery\",{\"1\":{\"226\":1}}],[\"battery2\",{\"1\":{\"226\":6,\"227\":1}}],[\"battery1\",{\"1\":{\"226\":7,\"227\":1,\"229\":5}}],[\"batteries\",{\"1\":{\"226\":6}}],[\"batched=false\",{\"1\":{\"82\":1}}],[\"batch\",{\"1\":{\"61\":2,\"81\":2,\"104\":6}}],[\"bahdanau\",{\"1\":{\"53\":2}}],[\"based\",{\"0\":{\"198\":1,\"228\":1},\"1\":{\"22\":1,\"25\":1,\"37\":1,\"43\":3,\"44\":2,\"49\":1,\"53\":1,\"59\":1,\"81\":3,\"124\":3,\"134\":1,\"138\":1,\"197\":1,\"203\":4,\"207\":1,\"223\":1,\"224\":1,\"225\":1,\"226\":3,\"227\":1,\"228\":1,\"229\":1}}],[\"basic\",{\"0\":{\"7\":1,\"8\":1,\"44\":1,\"50\":1,\"207\":1},\"1\":{\"8\":1,\"44\":3,\"45\":4,\"226\":1}}],[\"balances\",{\"1\":{\"197\":1}}],[\"balance\",{\"1\":{\"17\":1,\"197\":1}}],[\"begin\",{\"1\":{\"223\":1}}],[\"behavioral\",{\"1\":{\"154\":1}}],[\"behavior\",{\"1\":{\"144\":2,\"154\":1,\"207\":1}}],[\"best\",{\"0\":{\"197\":1},\"1\":{\"123\":1,\"196\":1,\"197\":1,\"216\":1,\"223\":1,\"224\":1}}],[\"being\",{\"1\":{\"52\":1,\"82\":1,\"196\":1,\"214\":1}}],[\"belongs\",{\"1\":{\"224\":1}}],[\"belong\",{\"1\":{\"190\":1,\"192\":1}}],[\"below\",{\"1\":{\"46\":1,\"49\":1,\"224\":1}}],[\"belt\",{\"1\":{\"49\":1}}],[\"bellman\",{\"0\":{\"20\":1,\"23\":1,\"27\":1,\"29\":1,\"31\":1},\"1\":{\"21\":1,\"23\":2,\"24\":1,\"27\":1,\"28\":1,\"29\":2,\"33\":1,\"37\":1}}],[\"been\",{\"1\":{\"45\":1,\"89\":1,\"207\":1,\"215\":5,\"217\":1,\"218\":1,\"225\":1,\"226\":1,\"227\":1}}],[\"benefits\",{\"1\":{\"229\":1}}],[\"benefit\",{\"1\":{\"23\":1,\"193\":1}}],[\"become\",{\"1\":{\"81\":1,\"86\":1,\"168\":1,\"215\":1,\"224\":1,\"226\":1}}],[\"becomes\",{\"1\":{\"17\":1,\"55\":1,\"229\":1}}],[\"because\",{\"1\":{\"22\":1,\"25\":1,\"44\":1,\"87\":1,\"168\":1,\"193\":1,\"195\":1,\"196\":1,\"197\":1,\"200\":2,\"215\":1,\"223\":1}}],[\"better\",{\"1\":{\"22\":1,\"24\":2,\"25\":1,\"28\":1,\"43\":1,\"61\":1,\"78\":1,\"83\":1,\"104\":2,\"119\":1,\"167\":1,\"195\":1,\"196\":4,\"207\":1}}],[\"between\",{\"0\":{\"40\":1},\"1\":{\"8\":1,\"22\":1,\"23\":1,\"25\":1,\"37\":1,\"40\":4,\"44\":1,\"46\":1,\"53\":2,\"58\":1,\"60\":2,\"86\":1,\"87\":1,\"89\":1,\"95\":1,\"111\":1,\"123\":1,\"144\":1,\"188\":1,\"193\":1,\"199\":1,\"203\":1,\"216\":2,\"223\":1,\"224\":1}}],[\"be\",{\"1\":{\"7\":2,\"11\":1,\"16\":2,\"18\":1,\"21\":1,\"22\":3,\"25\":3,\"28\":1,\"29\":2,\"33\":1,\"35\":1,\"38\":2,\"43\":3,\"44\":2,\"45\":5,\"46\":2,\"49\":2,\"59\":2,\"67\":2,\"78\":1,\"81\":1,\"82\":4,\"87\":2,\"88\":1,\"104\":1,\"122\":1,\"123\":1,\"134\":1,\"140\":2,\"143\":1,\"185\":1,\"191\":1,\"194\":1,\"195\":1,\"196\":2,\"197\":1,\"199\":1,\"206\":1,\"207\":9,\"213\":3,\"214\":1,\"215\":1,\"216\":2,\"217\":2,\"218\":3,\"219\":2,\"220\":1,\"221\":8,\"222\":3,\"223\":5,\"224\":4,\"225\":1,\"226\":13,\"227\":7,\"229\":4}}],[\"before\",{\"0\":{\"5\":1},\"1\":{\"21\":1,\"29\":1,\"39\":1,\"44\":1,\"48\":1,\"89\":1,\"216\":2,\"229\":2}}],[\"by\",{\"1\":{\"5\":1,\"7\":1,\"8\":1,\"17\":2,\"18\":1,\"21\":4,\"23\":1,\"25\":2,\"37\":1,\"43\":1,\"45\":3,\"49\":2,\"53\":1,\"58\":1,\"59\":3,\"60\":1,\"78\":1,\"82\":2,\"83\":1,\"86\":1,\"87\":1,\"91\":1,\"99\":1,\"111\":1,\"119\":2,\"121\":3,\"122\":1,\"123\":1,\"124\":3,\"138\":1,\"143\":2,\"144\":2,\"154\":1,\"185\":2,\"188\":1,\"195\":1,\"198\":1,\"199\":1,\"200\":1,\"203\":3,\"205\":1,\"216\":1,\"221\":1,\"223\":3,\"224\":1,\"225\":1,\"226\":1,\"227\":5,\"229\":3}}],[\"bunch\",{\"1\":{\"226\":1}}],[\"building\",{\"1\":{\"89\":1,\"207\":1}}],[\"but\",{\"1\":{\"29\":1,\"45\":1,\"46\":2,\"214\":1,\"215\":1,\"223\":2,\"224\":1,\"226\":2}}],[\"bushi\",{\"1\":{\"3\":1}}],[\"bug\",{\"1\":{\"1\":1}}],[\"很容易就会出现梯度消失的情况\",{\"1\":{\"78\":1}}],[\"很早就有的想法\",{\"1\":{\"2\":1}}],[\"很多新东西从未见过\",{\"1\":{\"1\":1}}],[\"很麻烦\",{\"1\":{\"1\":1}}],[\"刚开始的过程确实很难\",{\"1\":{\"1\":1}}],[\"觉得还是要尝试些新东西\",{\"1\":{\"1\":1}}],[\"但保持后面音符的时值\",{\"1\":{\"174\":1}}],[\"但在新环境中可能无法适应\",{\"1\":{\"167\":1}}],[\"但在其基础上进行的后续划分有可能使得性能显著提高\",{\"1\":{\"113\":1}}],[\"但训练集\",{\"1\":{\"111\":1}}],[\"但不能作为调参\",{\"1\":{\"110\":1}}],[\"但还是希望自己未来能成为一个有用之人\",{\"1\":{\"3\":1}}],[\"但还好有dream\",{\"1\":{\"1\":1}}],[\"但同时也是一个理想的完美主义者\",{\"1\":{\"3\":1}}],[\"但痛定思痛\",{\"1\":{\"1\":1}}],[\"但总感觉很难\",{\"1\":{\"1\":1}}],[\"欢迎来到我的博客\",{\"1\":{\"0\":1}}]],\"serializationVersion\":2}}")).map(([e,t])=>[e,zt(t,{fields:["h","t","c"],storeFields:["h","t","c"]})]));self.onmessage=({data:{type:e="all",query:t,locale:s,options:n,id:o}})=>{const u=bt[s];e==="suggest"?self.postMessage([e,o,tt(t,u,n)]):e==="search"?self.postMessage([e,o,Z(t,u,n)]):self.postMessage({suggestions:[e,o,tt(t,u,n)],results:[e,o,Z(t,u,n)]})};
//# sourceMappingURL=index.js.map
